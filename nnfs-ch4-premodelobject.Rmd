---
title: "nnfs ch 5"
author: "Hans Elliott"
date: "5/23/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
### Initialize Layer Parameters-----------------------------------------------
init_params = function(n_inputs = "# of features",
                       n_neurons = "desired # of neurons",
                       momentum = FALSE,
                       cache = FALSE){
    
    weights = matrix(data = (0.1 * rnorm(n = n_inputs*n_neurons)),
                             nrow = n_inputs, ncol = n_neurons)
           #Number of weights = the number of inputs*number of neurons,
           #since every connection between the previous neurons (from input) and
           #the current neurons have an associated weight
    biases = matrix(data = 0, nrow = 1, ncol = n_neurons)
           #Number of biases = the number
    
    #momentum initialization
    weight_momentums = matrix(data = 0, 
                            nrow = nrow(weights),
                            ncol = ncol(weights))
    bias_momentums = matrix(data = 0,
                            nrow = nrow(biases),
                            ncol = ncol(biases))
    #cache initialization
    weight_cache = matrix(data = 0,
                          nrow = nrow(weights),
                          ncol = ncol(weights))
    bias_cache = matrix(data = 0,
                        nrow = nrow(biases),
                        ncol = ncol(biases))

    #saving:
    if (momentum == TRUE & cache == FALSE){ ##momentums only
          list("weights"=weights,"biases"=biases,
               "weight_momentums"=weight_momentums,
               "bias_momentums"=bias_momentums,
               "n_neurons"=n_neurons)
    } else if (cache == TRUE & momentum == FALSE){ ##cache only
          list("weights"=weights,"biases"=biases,
               "weight_cache"=weight_cache,
               "bias_cache"=bias_cache,
               "n_neurons"=n_neurons)
    } else if (momentum == TRUE &  cache == TRUE){ ##momentums and cache
          list("weights"=weights,"biases"=biases,
               "weight_momentums"=weight_momentums,
               "bias_momentums"=bias_momentums,
               "weight_cache"=weight_cache,
               "bias_cache"=bias_cache,
               "n_neurons"=n_neurons)
    } else if (momentum == FALSE & cache == FALSE){ ##no momentums or cache
          list("weights"=weights,"biases"=biases,"n_neurons"=n_neurons)
    }
}



### Dense Layer ---------------------------------------------------------------
layer_dense = list(
## FORWARD PASS 
 forward = function(    
            inputs,      
            parameters, ## from initialize_parameters
            weight_L1 = 0, weight_L2 = 0,  ##regularization
            bias_L1 = 0, bias_L2 = 0
){
  
 if(is.matrix(inputs) == FALSE ) message("Convert inputs to matrix first")
 
 n_inputs = ncol(inputs)
 n_neurons = parameters$n_neurons
 weights = parameters$weights
 biases = parameters$biases
 #Forward Pass 
 output = inputs%*%weights + biases[col(inputs%*%weights)]
 
 #Regularization values:
 regularization = list("weight_L1" = weight_L1, "weight_L2" = weight_L2,
                       "bias_L1" = bias_L1, "bias_L2" = bias_L2)

 #SAVING:
 #then layer saves momentum only
 if (exists(x = "weight_momentums", where = parameters) & 
     !exists(x = "weight_cache", where = parameters)){
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "weight_momentums"=parameters$weight_momentums, ##for momentum
      "bias_momentums"=parameters$bias_momentums,
      "regularization" = regularization)  ##for regularization
 #if momentum==FALSE & cache==TRUE, saves cache only
 } else if (!exists(x = "weight_momentums", where = parameters) &
            exists(x = "weight_cache", where = parameters)){
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "weight_cache"=parameters$weight_cache, ##for cache
      "bias_cache"=parameters$bias_cache,
      "regularization" = regularization)

 #if momentum==TRUE & cache==TRUE, saves both
 } else if (exists(x = "weight_momentums", where = parameters) &
            exists(x = "weight_cache", where = parameters)){
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "weight_momentums"=parameters$weight_momentums, ##for momentum
      "bias_momentums"=parameters$bias_momentums,
      "weight_cache"=parameters$weight_cache, ##for cache
      "bias_cache"=parameters$bias_cache,
      "regularization" = regularization)

 #else both==FALSE, ignore momentum & cache
 } else {
   #otherwise, just save
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "regularization" = regularization)
      }

 },#end fwd
 
# BACKWARD
  backward = function(d_layer="layer object that occurs prior in backward pass",
                      layer="the layer object from the forward pass"){
    
    dvalues = d_layer$dinputs
    #Gradients on parameters
    dweights = t(layer$inputs)%*%dvalues
    dbiases = colSums(dvalues)
    
    #Gradients on regularization
    ##regularization hyperparams:
    layer_reg = layer$regularization ##a list of the set lambda values

    ##L1 Weights##
    if (layer_reg$weight_L1 > 0){
    dL1 = matrix(1, nrow = nrow(layer$weights), ncol = ncol(layer$weights))
      #make matrix filled with 1s
    dL1[layer$weight < 0] = -1
      #convert matrix value to -1 where weight is less than zero
    dweights = dweights + layer_reg$weight_L1 * dL1
    }
    ##L2 Weights##
    if (layer_reg$weight_L2 > 0){
    dweights = dweights + 2 * layer_reg$weight_L2 * layer$weights
    }                     #2 * lambda * weights
    
    ##L1 Biases##
    if (layer_reg$bias_L1 > 0){
    dL1 = matrix(1, nrow = nrow(layer$biases), ncol = ncol(layer$biases))
      #make matrix filled with 1s
    dL1[layer$bias < 0] = -1
      #convert matrix value to -1 where weight is less than zero
    dbiases = dbiases + layer_reg$bias_L1 * dL1
    }
    ##L2 Biases##
    if (layer_reg$bias_L2 > 0){
    dbiases = dbiases + 2 * layer_reg$bias_L2 * layer$biases
    }
    
    #Gradients on values
    dinputs = dvalues%*%t(layer$weights) 
    
    #saves:
    list("dinputs"=dinputs,
         "dweights"=dweights,
         "dbiases"=dbiases)
}#end bwd
)



### Dropout Layer -------------------------------------------------------------
layer_dropout = list(
## FORWARD PASS
  forward = function(
              input_layer, ##layer to apply dropout to
              dropout_rate ##rate of neuron deactivation
) {
  inputs = input_layer$output   ##the outputs from the previous layer
  
  #Dropout mask/filter
  dropout_filter = matrix(data = 
                           rbinom(n = nrow(inputs)*ncol(inputs),
                                  size = 1,        
                                  p = (1-dropout_rate)),
                          nrow = nrow(inputs),
                          ncol = ncol(inputs)) / 
                  (1 - dropout_rate)
  ##Creates matrix that is shape of the input layer's output (from nrow, ncol)
  ##and fills it with 1s and 0s from "Bernoulli". The length of the rbinom 
  ##output is equal to nrow*ncol so it fills the input layer's shape. 
  ##We also apply the scaling step to the filter directly since it makes the 
  ##backprop step even simpler.
  
  ##Apply mask to inputs and scale by (1 - dropout_rate)
  output = inputs * dropout_filter
  
  list("output" = output, "dropout_filter" = dropout_filter)

},
## BACKWARD PASS
  backward = function(
            d_layer = "layer object that occurs prior in backward pass",
            #derivative values being passed back from next layer
            layer_dropout = "the layer object from the forward pass"
                                    ##the forward-pass dropout layer object
){
  dvalues = d_layer$dinputs ##extract the derivative object from the layer
  dinputs = dvalues * layer_dropout$dropout_filter    
  ##Thus if the filter is 0 at the given index, the derivative is 0
  ##And if the filter is 1/(1-dropout_rate) at the given index, 
  ##the derivative is dvalues * 1/(1-dropout_rate)
  list("dinputs" = dinputs)
  }

)#end layer_dropout

### Activation Functions ------------------------------------------------------
## ReLU ##
activation_ReLU = list(
  #FORWARD PASS
  forward = function(input_layer){
    
    inputs = input_layer$output
    output = matrix(sapply(X = inputs, 
                    function(X){max(c(0, X))}
                    ), 
                  nrow = nrow(inputs), ncol = ncol(inputs))
    #ReLU function coerced into a matrix so the shape
    #is maintained (it will be equivalent to that of the input shape)
    
    #Function saves:
    list("output" = output, "inputs" = inputs)
  },
  #BACKWARD PASS
  backward = function(d_layer, layer){
    
    inputs = layer$inputs
    dinputs = d_layer$dinputs ##the dinputs from the next layer
    
    dinputs[inputs <= 0] = 0
    #save:
    list("dinputs"=dinputs)
  }
)

## SoftMax ##
activation_Softmax = list(
  forward = function(inputs){
          #scale inputs
          max_value = apply(X = inputs, MARGIN = 2,  FUN = max)
          scaled_inputs = sapply(X = 1:ncol(inputs), 
                 FUN = function(X){
                    inputs[,X] - abs(max_value[X])})

          # exponetiate
          exp_values = exp(scaled_inputs)
          # normalize
          norm_base = matrix(rowSums(exp_values),
                             nrow = nrow(inputs), ncol = 1)
          probabilities = sapply(X = 1:nrow(inputs),
                          FUN = function(X){exp_values[X,]/norm_base[X,]}) 
          return(t(probabilities))
          #(transpose probabilities)
          },
  
  backward = function(softmax_output, dvalues){
    #*INCOMPLETE SECTION - don't use*
    #flatten output array
    flat_output = as.vector(softmax_output)
    
    #calculate jacobian matrix of output
    jacobian_matrix = diag(flat_output) - flat_output%*%t(flat_output)
    
    #calculate sample-wise gradient
    dinputs = jacobian_matrix%*%flat_dvalues
    
    
  }
)

## Softmax X Cross Entropy ##
# combined softmax activation fn & cross-entropy loss for faster backprop
activation_loss_SoftmaxCrossEntropy = list(
  #FORWARD PASS
  forward = function(input_layer, y_true){
    
    inputs = input_layer$output
    #output layer's activation function
    softmax_out = activation_Softmax$forward(inputs)
    #calculate loss
    loss = Categorical_CrossEntropy$forward(softmax_out, y_true)
    #function saves:
    list("output"=softmax_out, "loss"=loss) 
  },
  #BACKWARD PASS
  backward = function(dvalues, y_true){

    #Detect number of samples
    if (is.vector(dvalues)) {      #if one sample
      samples = 1
    } else if (is.array(dvalues)) {  #else if multiple samples
      samples = nrow(dvalues)
    } else message("error checking shape of inputs")
    
    #Reverse One-Hot Encoding
    #if labels are one-hot encoded, turn them discrete values
     ##helper function 
    anti_ohe = function(y_true){
               unique_classes = ncol(y_true)
               samples = nrow(y_true)
               y_true_vec = as.vector(y_true)
                    
               class_key = rep(1:unique_classes, each = samples)
               y_true = class_key[y_true_vec==1]
                    #selects the classes that correspond to 1s in y_true vector
                    return(y_true)
                    }
     ##check & modify
    y_true = if(is.array(y_true)){ #if one-hot encoded
                    #change to sparse
                    anti_ohe(y_true)
              } else y_true
    
    #Calculate gradient
     #Copy so we can modify
     dinputs = dvalues
     #Calculate gradient
     #index the prediction array with the sample number and its
     #true value index, subtracting 1 from these values. Requires discrete,
     #not one-hot encoded, true labels (explaining the need for the above step)
     dinputs[cbind(1:samples, y_true)] = dinputs[cbind(1:samples, y_true)] - 1
     #Normalize gradient
     dinputs = dinputs/samples
    #save desired outputs
    list("dinputs" = dinputs, "samples" = samples, "y_true" = y_true)
  }
)

## Sigmoid ##
activation_Sigmoid = list(
  forward = function(input_layer){
    inputs = input_layer$output
    sigmoid = 1 / (1 + exp(-inputs))
    list("output" = sigmoid)
},
  backward = function(d_layer = "the layer object being passed back",
                      sig_layer = "the sigmoid layer obj from the forward pass"
                      ){
    dvalues = d_layer$dinputs ##the values being passed back
    output = sig_layer$output ##the output from the forward-pass
    dinputs = dvalues * (1 - output) * output 
    list("dinputs" = dinputs)
  }
)

## Linear ##
activation_Linear = list(
  forward = function(input_layer){
    output = input_layer$output
    list("output" = output)
  },
  
  backward = function(d_layer){
    dinputs = d_layer$dinputs
    list("dinputs" = dinputs)
    #derivative = 1, so 1 * dvalues = dvalues. dvalues correspond to the 
    #previously backpropagated layer's dinputs
    }
)

### Loss ----------------------------------------------------------------------
## Categorical Cross Entropy ##
Categorical_CrossEntropy = list(
  #FORWARD PASS
  forward = function(y_pred = "softmax output", y_true = "target labels"){
    
    #DETECT NUMBER OF SAMPLES
    samples = length(y_true)  

    #CLIP SAMPLES TO AVOID -Inf ERROR
    y_pred_clipped = ifelse(y_pred <= 1e-7, 1e-7, 
                        ifelse(y_pred >= (1-1e-7), (1-1e-7), y_pred))
    

    #DETERMINE IF Y_TRUE IS ONE-HOT-ENCODED AND SELECT CORRESPODNING CONFIDENCES
    confidences = ifelse(nrow(t(y_true)) == 1, 
      #if y_true is a single vector of labels (i.e, sparse), then confidences =
                    y_pred_clipped[cbind(1:samples, y_true)],
                      
                      ifelse(nrow(y_true) > 1,
                      #else, if y_true is one-hot encoded, then confidences =
                             rowSums(y_pred_clipped*y_true),
                             #else
                             "error indexing the predicted class confidences")
                  )
                    
    #CALC LOSS FOR EACH SAMPLE (ROW)
    neg_log_likelihoods = -log(confidences)
    return(neg_log_likelihoods)
    
  },
  #BACKWARD PASS
  backward = function(y_true, d_layer){
    dvalues = d_layer$dinputs
    
    #number of samples
    samples = length(dvalues)
    
    #number of labels
    labels = length(unique(dvalues[1,]))
    
    #if labels are sparse, turn them into one-hot encoded vector
    y_true = ifelse(#if
                    nrow(t(y_true)) ==1,
                    #one-hot-encode
                    y_true = do.call(rbind,
                                     lapply(X = y_true,
                                       function(X) as.integer(
                                          !is.na(match(unique(
                                          unlist(y_true)
                                                        ), X)
                                                ))
                                      )), 
                    #else
                    y_true)
    
    #calculate gradient
    dinputs = -y_true/dvalues
    #normalize gradient
    dinputs = dinputs/samples
    list("dinputs" = dinputs)
  }
)

## Binary Crossentropy ##
loss_BinaryCrossentropy = list(
  forward = function(y_pred, y_true){
    #clip data to prevent division by zero (both sides to keep mean unbiased)
    y_pred_clipped = ifelse(y_pred >= 1-1e-7, 1-1e-7,
                            ifelse(y_pred <= 1e-7, 1e-7, y_pred))
    
    #calculate sample-wise losse per neuron
    sample_losses = -(y_true*log(y_pred_clipped) + 
                                (1 - y_true)*log(1 - y_pred_clipped)
                        )
    #calculate total (mean) loss for each sample 
    ## (mean across neurons/vector of outputs, neurons = cols, samples = rows)
    sample_losses = rowMeans(sample_losses)
    
    #calculate mean loss across the entire batch
    data_loss = mean(sample_losses)
    
    list("sample_losses" = sample_losses, "data_loss" = data_loss)
  },
  backward = function(dvalues = "sigmoid output", y_true){
    
    #number of samples
    samples = nrow(dvalues)
    #number of output neurons
    n_outputs = ncol(dvalues)
    
    #clip data to prevent divide by zero
    clipped_dvalues = ifelse(dvalues >= 1-1e-7, 1-1e-7,
                            ifelse(dvalues <= 1e-7, 1e-7, dvalues))
    #calculate gradients
    dinputs = -(y_true / clipped_dvalues - 
                  (1 - y_true) / (1 - clipped_dvalues)
                  ) / n_outputs
    
    #normalize gradient
    dinputs = dinputs/samples
    ##We have to perform this normalization since each output returns its own
    ##derivative, and without normalization, each additional input will increase
    ##the gradients mechanically
    list("dinputs" = dinputs)
  }
)    

## MSE ##
loss_MeanSquaredError = list(
  forward = function(y_pred, y_true){
    ##calculate MSE for each sample (row)
    sample_losses = rowMeans( (y_true - y_pred)^2 )
      list("sample_losses" = sample_losses, "y_true" = y_true)
  },
  backward = function(dvalues = "linear activation fn output",
                      loss_layer = "loss layer object from forward pass"){
    
    y_true = loss_layer$y_true
    #number of samples
    samples = nrow(dvalues)
    #number of output neurons
    n_outputs = ncol(dvalues)
    
    #Gradient on values (dvalues = y_pred)
    dinputs = -2 * (y_true - dvalues) / n_outputs 
    
    #Normalize
    dinputs = dinputs / samples
    
    list("dinputs"=dinputs)
  }
)

## MAE ##
loss_MeanAbsoluteError = list(
  forward = function(y_pred, y_true){
    ##calculate MAE for each sample (row)
    sample_losses = rowMeans( abs(y_true - y_pred) )
      list("sample_losses" = sample_losses, "y_true" = y_true)
},
  backward = function(dvalues = "linear activation fn output",
                      loss_layer = "loss layer object from forward pass"){
    y_true = loss_layer$y_true
    #number of samples
    samples = nrow(dvalues)
    #number of output neurons
    n_outputs = ncol(dvalues)
    
    #calculate gradient (sign returns 1 for values >0, -1 for values <0)
    dinputs = sign(y_true - dvalues) / n_outputs
                            ##dvalues = y_pred
    #normalize gradients
    dinputs = dinputs / samples
    
    list("dinputs"=dinputs)
  }
)

## Regularization ##
regularization_loss = function(layer){
  #Regularization hyperparams:
  layer_reg = layer$regularization ##a list of the set lambda values
  
  #L1-regularization: weights
  l1_weight_apply = layer_reg$weight_L1 * sum(abs(layer$weights))
  #L1-regularization: bias
  l1_bias_apply = layer_reg$bias_L1 * sum(abs(layer$biases))
  
  #L2-regularization: weights
  l2_weight_apply = layer_reg$weight_L2 * sum(layer$weights^2)
  #L2-regularization: biases
  l2_bias_apply = layer_reg$bias_L2 * sum(layer$biases^2)
  
  #Overall regularization loss
  reg_loss = l1_weight_apply + l1_bias_apply + l2_weight_apply + l2_bias_apply
  #save:
  return(reg_loss)
}

### Optimizers----------------------------------------------------------------
## Stochastic Gradient Descent (vanilla + decay & momentum options) ##
optimizer_SGD = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 1,
                           lr_decay = 0, iteration = 1,
                           momentum_rate = 0 
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)
    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #param updates with momentum
     #If momentum == TRUE in parameter initialization, then weights_momentum
     #(and implictly, bias_momentum) will exist
    if (exists("weight_momentums", where = layer_forward)) {
      #current momentums
      weight_momentums = layer_forward$weight_momentums
      bias_momentums = layer_forward$bias_momentums
      
      #Update weights & biases with momentum:
      #Take prior updates X retainment factor (the "momentum rate"),
      #and update with current gradients
      weight_update = 
        (momentum_rate*weight_momentums) - (currnt_learn_rate*weight_gradients)
      bias_update = 
        (momentum_rate*bias_momentums) - (currnt_learn_rate*bias_gradients)
      #update params with the calculated updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
      #also update momentums
      weight_momentums = weight_update
      bias_momentums = bias_update
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
           "weight_momentums" = weight_momentums, 
           "bias_momentums" = bias_momentums,
           "n_neurons" = n_neurons)
    } else {
      
    #param updates without momentum (vanilla)
      #calculate updates
      weight_update = -currnt_learn_rate*weight_gradients
      bias_update = -currnt_learn_rate*bias_gradients
      #apply updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
           "n_neurons" = n_neurons)
    }
    
})
## AdaGrad ##
#NOTE: must initialize params with cache==TRUE
optimizer_AdaGrad = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 1,
                           lr_decay = 0, iteration = 1,
                           epsilon = 1e-7 
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)

    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #cache
    ##update cache with squared current gradients
    weight_cache = layer_forward$weight_cache + weight_gradients^2
    bias_cache = layer_forward$bias_cache + bias_gradients^2
    
    #SGD param updates with normalization 
      #calculate updates
      weight_update = -currnt_learn_rate*weight_gradients /
                        (sqrt(weight_cache) + epsilon)
      bias_update = -currnt_learn_rate*bias_gradients /
                        (sqrt(bias_cache) + epsilon)
      #apply updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
        #save:
        list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
             "weight_cache" = weight_cache, "bias_cache" = bias_cache,
             "n_neurons" = n_neurons)
} ##these are the new params to be passed on to the layers when loop restarts
)

## RMSProp ##
#NOTE: must initialize params with cache==TRUE
optimizer_RMSProp = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 0.001,
                           lr_decay = 0, iteration = 1,
                           epsilon = 1e-7,
                           rho = 0.9
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)

    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #cache
    ##update cache with squared current gradients
    weight_cache = rho * layer_forward$weight_cache + 
                  (1-rho) * weight_gradients^2
    bias_cache = rho * layer_forward$bias_cache + 
                  (1-rho) * bias_gradients^2
    
    #SGD param updates with normalization 
      #calculate updates
      weight_update = -currnt_learn_rate*weight_gradients /
                        (sqrt(weight_cache) + epsilon)
      bias_update = -currnt_learn_rate*bias_gradients /
                        (sqrt(bias_cache) + epsilon)
      #apply updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
        #save:
        list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
             "weight_cache" = weight_cache, "bias_cache" = bias_cache,
             "n_neurons" = n_neurons)
} ##these are the new params to be passed on to layers when the loop restarts
)

## Adam ##
#NOTE: must intialize paramters with both cache AND momentum
optimizer_Adam = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 0.001,
                           lr_decay = 0, iteration = 1,
                           epsilon = 1e-7,
                           beta_1 = 0.9, beta_2 = 0.999
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)

    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #momentums
    #update momentums with current gradients and bias correct
    weight_momentums = 
      (beta_1*layer_forward$weight_momentums + (1-beta_1)*weight_gradients ) /
                              (1 - (beta_1^iteration )) ##bias correction
    
    bias_momentums = 
      (beta_1*layer_forward$bias_momentums + (1-beta_1)*bias_gradients) /
                              (1 - (beta_1^iteration))
                    

    #cache
    #update cache with squared gradients and bias correct
    weight_cache = 
      (beta_2*layer_forward$weight_cache + (1-beta_2)*weight_gradients^2) /
                              (1 - (beta_2^iteration)) ##bias correction

    bias_cache = 
      (beta_2*layer_forward$bias_cache + (1-beta_2)*bias_gradients^2) /
                              (1 - (beta_2^iteration))
    
    #calculate param updates (with momentums, and normalize with cache)
    weight_update = -currnt_learn_rate*weight_momentums /
                        (sqrt(weight_cache) + epsilon)
    bias_update = -currnt_learn_rate*bias_momentums /
                        (sqrt(bias_cache) + epsilon)
    
    #apply updates
    weights = current_weights + weight_update
    biases = current_biases + bias_update
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
           "weight_momentums" = weight_momentums, 
           "bias_momentums" = bias_momentums,
           "weight_cache" = weight_cache,
           "bias_cache" = bias_cache,
           "n_neurons" = n_neurons)

}##these are the updated params to be passed back into the layers
)

##function prints reminders for settings needed for each optimizer
optimizer_REMINDME = function(optimizer = "sgd, adagrad, rmsprop, or adam"){
  if(optimizer == "sgd"){
    message("Momentum is optional, cache is not available. If you choose to set momentum rate in SGD arguments, set momentum = TRUE in parameter initialization function")
  }
  if(optimizer == "adagrad"){
    message("Initialize parameters with cache=TRUE, momentum=FALSE. The AdaGrad optimizer implements cache - a rolling average/history of past gradients - as a form of adaptive gradients (or 'per-parameter learning rates'.")
  }
  if(optimizer == "rmsprop"){
    message("Initialize parameters with cache=TRUE, momentum=FALSE. RMSProp implements cache like AdaGrad, but exponentially decays the cache. You will also have to set the 'rho' hyperparameter - the cache memory decay rate.")
  }
  if(optimizer == "adam"){
    message("Initialize parameters with cache=TRUE & momentum=TRUE. To calculate parameter updates, Adam implements momentums in place of vanilla gradients and also applies cache to save a rolling average of the momentums.")
  }
  
}



```

