---
title: "part4"
author: "Hans Elliott"
date: "4/28/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

NOTE: currently, for multi-class, the classes would have to be encoded starting at 1, since code relies on index (R cols start at 1.)

## Full code
In the last section, we worked on improving model generalization. In addition to testing our neural net's predictions on test data, we added L1 and L2 regularization and a dropout layer to our set of functions.  
I've also cleaned up the functions a bit so things run smoother. Here's a list of the updates:   
- Overall, my changes are a move towards a new system which I think will be easy to implement and will reduce the amount of overhead needed to actually use the functions. Basically, the heart of the process will be the layer objects that get created for each hidden and output layer. The layer objects are stocked full of other objects which they can pass on to the various other functions as needed. That way, all we really have to do is pass on one layer into the next, and the only other arguments we need to specify are hyperparameters.   
- The `init_params` function will save the n_neurons in addition to all of the parameters since it was redundant to include n_neurons as an argument in both the initialization function and the `layer_dense` function. The `layer_dense` function is updated to extract the n_neurons object from the parameters object.  
- Since the init_params fns defines the number of neurons, we will need the optimized paramaters object at the end of backpropagation to also contain a n_neurons object, so that when the forward pass begins again the layer_dense function can find it. This is fairly simple since we are already passing the forward layer object to the optimizer function, and thus it can extract the number of neurons from the forward layer's output matrix - the number of columns in the output matrix corresponds to the number of neurons.  
- There was some redundancy with the backprop step, where I was specifying "dvalues" as the derivative values being passed back from the next layer into the current layer. I simplify this so that all we have to do is pass back that next layer object (slightly confusing - this is backprop layer corresponding to the forward-pass layer that is the next layer in the forward pass, so technically it is the previous layer in the backprop). I'm calling this argument `d_layer.` The idea is that you just pass back each backpropagated layer sequentially. Thus every backpropagation step takes two simple arguments: the forward pass layer object, and the backward pass layer object of the "next" layer. This works fairly well except


```{r nn-fns}
### Initialize Layer Parameters-----------------------------------------------
init_params = function(n_inputs = "# of features",
                       n_neurons = "desired # of neurons",
                       momentum = FALSE,
                       cache = FALSE){
    
    weights = matrix(data = (0.1 * rnorm(n = n_inputs*n_neurons)),
                             nrow = n_inputs, ncol = n_neurons)
           #Number of weights = the number of inputs*number of neurons,
           #since every connection between the previous neurons (from input) and
           #the current neurons have an associated weight
    biases = matrix(data = 0, nrow = 1, ncol = n_neurons)
           #Number of biases = the number
    
    #momentum initialization
    weight_momentums = matrix(data = 0, 
                            nrow = nrow(weights),
                            ncol = ncol(weights))
    bias_momentums = matrix(data = 0,
                            nrow = nrow(biases),
                            ncol = ncol(biases))
    #cache initialization
    weight_cache = matrix(data = 0,
                          nrow = nrow(weights),
                          ncol = ncol(weights))
    bias_cache = matrix(data = 0,
                        nrow = nrow(biases),
                        ncol = ncol(biases))

    #saving:
    if (momentum == TRUE & cache == FALSE){ ##momentums only
          list("weights"=weights,"biases"=biases,
               "weight_momentums"=weight_momentums,
               "bias_momentums"=bias_momentums,
               "n_neurons"=n_neurons)
    } else if (momentum == FALSE & cache == TRUE){ ##cache only
          list("weights"=weights,"biases"=biases,
               "weight_cache"=weight_cache,
               "bias_cache"=bias_cache,
               "n_neurons"=n_neurons)
    } else if (momentum == TRUE &  cache == TRUE){ ##momentums and cache
          list("weights"=weights,"biases"=biases,
               "weight_momentums"=weight_momentums,
               "bias_momentums"=bias_momentums,
               "weight_cache"=weight_cache,
               "bias_cache"=bias_cache,
               "n_neurons"=n_neurons)
    } else if (momentum == FALSE & cache == FALSE){ ##no momentums or cache
          list("weights"=weights,"biases"=biases,"n_neurons"=n_neurons)
    }
}



### Dense Layer ---------------------------------------------------------------
layer_dense = list(
## FORWARD PASS 
 forward = function(    
            inputs,      
            parameters, ## from initialize_parameters
            weight_L1 = 0, weight_L2 = 0,  ##regularization
            bias_L1 = 0, bias_L2 = 0
){
  
 if(is.matrix(inputs) == FALSE ) message("Convert inputs to matrix first")
 
 n_inputs = ncol(inputs)
 n_neurons = parameters$n_neurons
 weights = parameters$weights
 biases = parameters$biases
 #Forward Pass 
 output = inputs%*%weights + biases[col(inputs%*%weights)]
 
 #Regularization values:
 regularization = list("weight_L1" = weight_L1, "weight_L2" = weight_L2,
                       "bias_L1" = bias_L1, "bias_L2" = bias_L2)

 #SAVING:
 #then layer saves momentum only
 if (exists(x = "weight_momentums", where = parameters) & 
     !exists(x = "weight_cache", where = parameters)){
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "weight_momentums"=parameters$weight_momentums, ##for momentum
      "bias_momentums"=parameters$bias_momentums,
      "regularization" = regularization)  ##for regularization
 #if momentum==FALSE & cache==TRUE, saves cache only
 } else if (!exists(x = "weight_momentums", where = parameters) &
            exists(x = "weight_cache", where = parameters)){
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "weight_cache"=parameters$weight_cache, ##for cache
      "bias_cache"=parameters$bias_cache,
      "regularization" = regularization)

 #if momentum==TRUE & cache==TRUE, saves both
 } else if (exists(x = "weight_momentums", where = parameters) &
            exists(x = "weight_cache", where = parameters)){
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "weight_momentums"=parameters$weight_momentums, ##for momentum
      "bias_momentums"=parameters$bias_momentums,
      "weight_cache"=parameters$weight_cache, ##for cache
      "bias_cache"=parameters$bias_cache,
      "regularization" = regularization)

 #else both==FALSE, ignore momentum & cache
 } else {
   #otherwise, just save
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "regularization" = regularization)
      }

 },#end fwd
 
# BACKWARD
  backward = function(d_layer="layer object that occurs prior in backward pass",
                      layer="the layer object from the forward pass"){
    
    dvalues = d_layer$dinputs
    #Gradients on parameters
    dweights = t(layer$inputs)%*%dvalues
    dbiases = colSums(dvalues)
    
    #Gradients on regularization
    ##regularization hyperparams:
    layer_reg = layer$regularization ##a list of the set lambda values

    ##L1 Weights##
    if (layer_reg$weight_L1 > 0){
    dL1 = matrix(1, nrow = nrow(layer$weights), ncol = ncol(layer$weights))
      #make matrix filled with 1s
    dL1[layer$weight < 0] = -1
      #convert matrix value to -1 where weight is less than zero
    dweights = dweights + layer_reg$weight_L1 * dL1
    }
    ##L2 Weights##
    if (layer_reg$weight_L2 > 0){
    dweights = dweights + 2 * layer_reg$weight_L2 * layer$weights
    }                     #2 * lambda * weights
    
    ##L1 Biases##
    if (layer_reg$bias_L1 > 0){
    dL1 = matrix(1, nrow = nrow(layer$biases), ncol = ncol(layer$biases))
      #make matrix filled with 1s
    dL1[layer$bias < 0] = -1
      #convert matrix value to -1 where weight is less than zero
    dbiases = dbiases + layer_reg$bias_L1 * dL1
    }
    ##L2 Biases##
    if (layer_reg$bias_L2 > 0){
    dbiases = dbiases + 2 * layer_reg$bias_L2 * layer$biases
    }
    
    #Gradients on values
    dinputs = dvalues%*%t(layer$weights) 
    
    #saves:
    list("dinputs"=dinputs,
         "dweights"=dweights,
         "dbiases"=dbiases)
}#end bwd
)



### Dropout Layer -------------------------------------------------------------
layer_dropout = list(
## FORWARD PASS
  forward = function(
              input_layer, ##layer to apply dropout to
              dropout_rate ##rate of neuron deactivation
) {
  inputs = input_layer$output   ##the outputs from the previous layer
  
  #Dropout mask/filter
  dropout_filter = matrix(data = 
                           rbinom(n = nrow(inputs)*ncol(inputs),
                                  size = 1,        
                                  p = (1-dropout_rate)),
                          nrow = nrow(inputs),
                          ncol = ncol(inputs)) / 
                  (1 - dropout_rate)
  ##Creates matrix that is shape of the input layer's output (from nrow, ncol)
  ##and fills it with 1s and 0s from "Bernoulli". The length of the rbinom 
  ##output is equal to nrow*ncol so it fills the input layer's shape. 
  ##We also apply the scaling step to the filter directly since it makes the 
  ##backprop step even simpler.
  
  ##Apply mask to inputs and scale by (1 - dropout_rate)
  output = inputs * dropout_filter
  
  list("output" = output, "dropout_filter" = dropout_filter)

},
## BACKWARD PASS
  backward = function(
            d_layer = "layer object that occurs prior in backward pass",
            #derivative values being passed back from next layer
            layer_dropout = "the layer object from the forward pass"
                                    ##the forward-pass dropout layer object
){
  dvalues = d_layer$dinputs ##extract the derivative object from the layer
  dinputs = dvalues * layer_dropout$dropout_filter    
  ##Thus if the filter is 0 at the given index, the derivative is 0
  ##And if the filter is 1/(1-dropout_rate) at the given index, 
  ##the derivative is dvalues * 1/(1-dropout_rate)
  list("dinputs" = dinputs)
  }

)#end layer_dropout

### Activation Functions ------------------------------------------------------
## Linear ##
activation_Linear = list(
  forward = function(input_layer){
    output = input_layer$output
    list("output" = output)
  },
  
  backward = function(d_layer){
    dinputs = d_layer$dinputs
    list("dinputs" = dinputs)
    #derivative = 1, so 1 * dvalues = dvalues. dvalues correspond to the 
    #previously backpropagated layer's dinputs
    }
)


## ReLU ##
activation_ReLU = list(
  #FORWARD PASS
  forward = function(input_layer){
    
    inputs = input_layer$output
    output = matrix(sapply(X = inputs, 
                    function(X){max(c(0, X))}
                    ), 
                  nrow = nrow(inputs), ncol = ncol(inputs))
    #ReLU function coerced into a matrix so the shape
    #is maintained (it will be equivalent to that of the input shape)
    
    #Function saves:
    list("output" = output, "inputs" = inputs)
  },
  #BACKWARD PASS
  backward = function(d_layer, layer){
    
    inputs = layer$inputs
    dinputs = d_layer$dinputs ##the dinputs from the next layer
    
    dinputs[inputs <= 0] = 0
    #save:
    list("dinputs"=dinputs)
  }
)

## SoftMax ##
activation_Softmax = list(
  forward = function(inputs){
          #scale inputs
          max_value = apply(X = inputs, MARGIN = 2,  FUN = max)
          scaled_inputs = sapply(X = 1:ncol(inputs), 
                 FUN = function(X){
                    inputs[,X] - abs(max_value[X])})
          
          # exponetiate
          exp_values = as.matrix(exp(scaled_inputs))
          # normalize
          norm_base = matrix(rowSums(exp_values),
                             nrow = nrow(inputs), ncol = 1)
          probabilities = sapply(X = 1:nrow(inputs),
                          FUN = function(X){exp_values[X,]/norm_base[X,]}) 
          return(t(probabilities))
          #(transpose probabilities)
          },
  
  backward = function(softmax_output, dvalues){
    #*INCOMPLETE SECTION - don't use*
    #flatten output array
    flat_output = as.vector(softmax_output)
    
    #calculate jacobian matrix of output
    jacobian_matrix = diag(flat_output) - flat_output%*%t(flat_output)
    
    #calculate sample-wise gradient
    dinputs = jacobian_matrix%*%flat_dvalues
    
    
  }
)

## Sigmoid ##
activation_Sigmoid = list(
  forward = function(input_layer){
    inputs = input_layer$output
    sigmoid = 1 / (1 + exp(-inputs))
    list("output" = sigmoid)
},
  backward = function(d_layer = "the layer object being passed back",
                      layer = "the sigmoid layer obj from the forward pass"
                      ){
    dvalues = d_layer$dinputs ##the values being passed back
    output = layer$output ##the output from the forward-pass
    dinputs = dvalues * (1 - output) * output 
    list("dinputs" = dinputs)
  }
)

## Softmax X Cross Entropy ##
# combined softmax activation fn & cross-entropy loss for faster backprop
activation_loss_SoftmaxCrossEntropy = list(
  #FORWARD PASS
  forward = function(input_layer, y_true){
    
    inputs = input_layer$output
    #output layer's activation function
    softmax_out = activation_Softmax$forward(inputs)
    #calculate loss
    sample_losses = Categorical_CrossEntropy$forward(softmax_out, y_true)
    #function saves:
    list("output"=softmax_out, "sample_losses"=sample_losses) 
  },
  #BACKWARD PASS
  backward = function(dvalues, y_true){

    #Detect number of samples
    if (is.vector(dvalues)) {      #if one sample
      samples = 1
    } else if (is.array(dvalues)) {  #else if multiple samples
      samples = nrow(dvalues)
    } else message("error checking shape of inputs")
    
    #Reverse One-Hot Encoding
    #if labels are one-hot encoded, turn them into discrete values
     ##helper function 
    anti_ohe = function(y_true){
               unique_classes = ncol(y_true)
               samples = nrow(y_true)
               y_true_vec = as.vector(y_true)
                    
               class_key = rep(1:unique_classes, each = samples)
               y_true = class_key[y_true_vec==1]
                    #selects the classes that correspond to 1s in y_true vector
                    return(y_true)
                    }
     ##check & modify
    y_true = if(is.array(y_true)){ #if one-hot encoded
                    #change to sparse
                    anti_ohe(y_true)
              } else y_true
    
    #Calculate gradient
     #Copy so we can modify
     dinputs = dvalues
     #Calculate gradient
     #index the prediction array with the sample number and its
     #true value index, subtracting 1 from these values. Requires discrete,
     #not one-hot encoded, true labels (explaining the need for the above step)
     dinputs[cbind(1:samples, y_true)] = dinputs[cbind(1:samples, y_true)] - 1
     #Normalize gradient
     dinputs = dinputs/samples
    #save desired outputs
    list("dinputs" = dinputs, "samples" = samples, "y_true" = y_true)
  }
)


### Loss ----------------------------------------------------------------------
## MSE ##
loss_MeanSquaredError = list(
  forward = function(y_pred, y_true){
    ##calculate MSE for each sample (row)
    sample_losses = rowMeans( (y_true - y_pred)^2 )
      list("sample_losses" = sample_losses, "y_true" = y_true)
  },
  backward = function(dvalues = "linear activation fn output",
                      loss_layer = "loss layer object from forward pass"){
    
    y_true = loss_layer$y_true
    #number of samples
    samples = nrow(dvalues)
    #number of output neurons
    n_outputs = ncol(dvalues)
    
    #Gradient on values (dvalues = y_pred)
    dinputs = -2 * (y_true - dvalues) / n_outputs 
    
    #Normalize
    dinputs = dinputs / samples
    
    list("dinputs"=dinputs)
  }
)

## MAE ##
loss_MeanAbsoluteError = list(
  forward = function(y_pred, y_true){
    ##calculate MAE for each sample (row)
    sample_losses = rowMeans( abs(y_true - y_pred) )
      list("sample_losses" = sample_losses, "y_true" = y_true)
},
  backward = function(dvalues = "linear activation fn output",
                      loss_layer = "loss layer object from forward pass"){
    y_true = loss_layer$y_true
    #number of samples
    samples = nrow(dvalues)
    #number of output neurons
    n_outputs = ncol(dvalues)
    
    #calculate gradient (sign returns 1 for values >0, -1 for values <0)
    dinputs = sign(y_true - dvalues) / n_outputs
                            ##dvalues = y_pred
    #normalize gradients
    dinputs = dinputs / samples
    
    list("dinputs"=dinputs)
  }
)

## Binary Crossentropy ##
loss_BinaryCrossentropy = list(
  forward = function(y_pred, y_true){
    #clip data to prevent division by zero (both sides to keep mean unbiased)
    y_pred_clipped = ifelse(y_pred >= 1-1e-7, 1-1e-7,
                            ifelse(y_pred <= 1e-7, 1e-7, y_pred))
    
    #calculate sample-wise losse per neuron
    sample_losses = -(y_true*log(y_pred_clipped) + 
                                (1 - y_true)*log(1 - y_pred_clipped)
                        )
    #calculate total (mean) loss for each sample 
    ## (mean across neurons/vector of outputs, neurons = cols, samples = rows)
    sample_losses = rowMeans(sample_losses)
    
    #calculate mean loss across the entire batch
    data_loss = mean(sample_losses)
    
    list("sample_losses" = sample_losses, "data_loss" = data_loss)
  },
  backward = function(dvalues = "sigmoid output", y_true){
    
    #number of samples
    samples = nrow(dvalues)
    #number of output neurons
    n_outputs = ncol(dvalues)
    
    #clip data to prevent divide by zero
    clipped_dvalues = ifelse(dvalues >= 1-1e-7, 1-1e-7,
                            ifelse(dvalues <= 1e-7, 1e-7, dvalues))
    #calculate gradients
    dinputs = -(y_true / clipped_dvalues - 
                  (1 - y_true) / (1 - clipped_dvalues)
                  ) / n_outputs
    
    #normalize gradient
    dinputs = dinputs/samples
    ##We have to perform this normalization since each output returns its own
    ##derivative, and without normalization, each additional input will increase
    ##the gradients mechanically
    list("dinputs" = dinputs)
  }
)    


## Categorical Cross Entropy ##
Categorical_CrossEntropy = list(
  #FORWARD PASS
  forward = function(y_pred = "softmax output", y_true = "target labels"){
    
    #DETECT NUMBER OF SAMPLES
    samples = length(y_true)  

    #CLIP SAMPLES TO AVOID -Inf ERROR
    y_pred_clipped = ifelse(y_pred <= 1e-7, 1e-7, 
                        ifelse(y_pred >= (1-1e-7), (1-1e-7), y_pred))
    
    if (nrow(t(y_true)) == 1){ ##if y_true is a vector of labels (sparse)
      if (min(y_true) == 0){ #check if the first label is 0.
        y_true = y_true + 1  #add 1 to use y_true as index
      }
      confidences = y_pred_clipped[cbind(1:samples, y_true)]
    } else if (nrow(y_true) > 1){ #else if y_true is one-hot encoded
      confidences = rowSums(y_pred_clipped * y_true)
    } else warning("error indexing predicted class confidences [cat crossent]")
    
    #CALC LOSS FOR EACH SAMPLE (ROW)
    sample_losses = -log(confidences)
    return(sample_losses)
    
  },
  #BACKWARD PASS
  backward = function(y_true, d_layer){
    dvalues = d_layer$dinputs
    
    #number of samples
    samples = length(dvalues)
    
    #number of labels
    labels = length(unique(dvalues[1,]))
    
    #if labels are sparse, turn them into one-hot encoded vector
    y_true = ifelse(#if
                    nrow(t(y_true)) ==1,
                    #one-hot-encode
                    y_true = do.call(rbind,
                                     lapply(X = y_true,
                                       function(X) as.integer(
                                          !is.na(match(unique(
                                          unlist(y_true)
                                                        ), X)
                                                ))
                                      )), 
                    #else
                    y_true)
    
    #calculate gradient
    dinputs = -y_true/dvalues
    #normalize gradient
    dinputs = dinputs/samples
    list("dinputs" = dinputs)
  }
)


## Regularization ##
regularization_loss = function(layer){
  #Regularization hyperparams:
  layer_reg = layer$regularization ##a list of the set lambda values
  
  #L1-regularization: weights
  l1_weight_apply = layer_reg$weight_L1 * sum(abs(layer$weights))
  #L1-regularization: bias
  l1_bias_apply = layer_reg$bias_L1 * sum(abs(layer$biases))
  
  #L2-regularization: weights
  l2_weight_apply = layer_reg$weight_L2 * sum(layer$weights^2)
  #L2-regularization: biases
  l2_bias_apply = layer_reg$bias_L2 * sum(layer$biases^2)
  
  #Overall regularization loss
  reg_loss = l1_weight_apply + l1_bias_apply + l2_weight_apply + l2_bias_apply
  #save:
  return(reg_loss)
}

### Optimizers----------------------------------------------------------------
## Stochastic Gradient Descent (vanilla + decay & momentum options) ##
optimizer_SGD = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 1,
                           lr_decay = 0, iteration = 1,
                           momentum_rate = 0 
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)
    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #param updates with momentum
     #If momentum == TRUE in parameter initialization, then weights_momentum
     #(and implictly, bias_momentum) will exist
    if (exists("weight_momentums", where = layer_forward)) {
      #current momentums
      weight_momentums = layer_forward$weight_momentums
      bias_momentums = layer_forward$bias_momentums
      
      #Update weights & biases with momentum:
      #Take prior updates X retainment factor (the "momentum rate"),
      #and update with current gradients
      weight_update = 
        (momentum_rate*weight_momentums) - (currnt_learn_rate*weight_gradients)
      bias_update = 
        (momentum_rate*bias_momentums) - (currnt_learn_rate*bias_gradients)
      #update params with the calculated updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
      #also update momentums
      weight_momentums = weight_update
      bias_momentums = bias_update
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
           "weight_momentums" = weight_momentums, 
           "bias_momentums" = bias_momentums,
           "n_neurons" = n_neurons)
    } else {
      
    #param updates without momentum (vanilla)
      #calculate updates
      weight_update = -currnt_learn_rate*weight_gradients
      bias_update = -currnt_learn_rate*bias_gradients
      #apply updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
           "n_neurons" = n_neurons)
    }
    
})
## AdaGrad ##
#NOTE: must initialize params with cache==TRUE
optimizer_AdaGrad = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 1,
                           lr_decay = 0, iteration = 1,
                           epsilon = 1e-7 
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)

    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #cache
    ##update cache with squared current gradients
    weight_cache = layer_forward$weight_cache + weight_gradients^2
    bias_cache = layer_forward$bias_cache + bias_gradients^2
    
    #SGD param updates with normalization 
      #calculate updates
      weight_update = -currnt_learn_rate*weight_gradients /
                        (sqrt(weight_cache) + epsilon)
      bias_update = -currnt_learn_rate*bias_gradients /
                        (sqrt(bias_cache) + epsilon)
      #apply updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
        #save:
        list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
             "weight_cache" = weight_cache, "bias_cache" = bias_cache,
             "n_neurons" = n_neurons)
} ##these are the new params to be passed on to the layers when loop restarts
)

## RMSProp ##
#NOTE: must initialize params with cache==TRUE
optimizer_RMSProp = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 0.001,
                           lr_decay = 0, iteration = 1,
                           epsilon = 1e-7,
                           rho = 0.9
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)

    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #cache
    ##update cache with squared current gradients
    weight_cache = rho * layer_forward$weight_cache + 
                  (1-rho) * weight_gradients^2
    bias_cache = rho * layer_forward$bias_cache + 
                  (1-rho) * bias_gradients^2
    
    #SGD param updates with normalization 
      #calculate updates
      weight_update = -currnt_learn_rate*weight_gradients /
                        (sqrt(weight_cache) + epsilon)
      bias_update = -currnt_learn_rate*bias_gradients /
                        (sqrt(bias_cache) + epsilon)
      #apply updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
        #save:
        list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
             "weight_cache" = weight_cache, "bias_cache" = bias_cache,
             "n_neurons" = n_neurons)
} ##these are the new params to be passed on to layers when the loop restarts
)

## Adam ##
#NOTE: must intialize paramters with both cache AND momentum
optimizer_Adam = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 0.001,
                           lr_decay = 0, iteration = 1,
                           epsilon = 1e-7,
                           beta_1 = 0.9, beta_2 = 0.999
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)

    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #momentums
    #update momentums with current gradients and bias correct
    weight_momentums = 
      (beta_1*layer_forward$weight_momentums + (1-beta_1)*weight_gradients ) /
                              (1 - (beta_1^iteration )) ##bias correction
    
    bias_momentums = 
      (beta_1*layer_forward$bias_momentums + (1-beta_1)*bias_gradients) /
                              (1 - (beta_1^iteration))
                    

    #cache
    #update cache with squared gradients and bias correct
    weight_cache = 
      (beta_2*layer_forward$weight_cache + (1-beta_2)*weight_gradients^2) /
                              (1 - (beta_2^iteration)) ##bias correction

    bias_cache = 
      (beta_2*layer_forward$bias_cache + (1-beta_2)*bias_gradients^2) /
                              (1 - (beta_2^iteration))
    
    #calculate param updates (with momentums, and normalize with cache)
    weight_update = -currnt_learn_rate*weight_momentums /
                        (sqrt(weight_cache) + epsilon)
    bias_update = -currnt_learn_rate*bias_momentums /
                        (sqrt(bias_cache) + epsilon)
    
    #apply updates
    weights = current_weights + weight_update
    biases = current_biases + bias_update
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
           "weight_momentums" = weight_momentums, 
           "bias_momentums" = bias_momentums,
           "weight_cache" = weight_cache,
           "bias_cache" = bias_cache,
           "n_neurons" = n_neurons)

}##these are the updated params to be passed back into the layers
)

##function prints reminders for settings needed for each optimizer
optimizer_REMINDME = function(optimizer = "sgd, adagrad, rmsprop, or adam"){
  if(optimizer == "sgd"){
    message("Momentum is optional, cache is not available. If you choose to set momentum rate in SGD arguments, set momentum = TRUE in parameter initialization function")
  }
  if(optimizer == "adagrad"){
    message("Initialize parameters with cache=TRUE, momentum=FALSE. The AdaGrad optimizer implements cache - a rolling average/history of past gradients - as a form of adaptive gradients (or 'per-parameter learning rates'.")
  }
  if(optimizer == "rmsprop"){
    message("Initialize parameters with cache=TRUE, momentum=FALSE. RMSProp implements cache like AdaGrad, but exponentially decays the cache. You will also have to set the 'rho' hyperparameter - the cache memory decay rate.")
  }
  if(optimizer == "adam"){
    message("Initialize parameters with cache=TRUE & momentum=TRUE. To calculate parameter updates, Adam implements momentums in place of vanilla gradients and also applies cache to save a rolling average of the momentums.")
  }
  
}



```

And here is our function to simulate the spiral data. 
```{r sim-data-fn}
## Creating Spiral Data ----
#Source: https://cs231n.github.io/neural-networks-case-study/
sim_data_fn = function(
  N = 100, # number of points per class
  D = 2,   # "dimensionality" (number of features)
  K = 3,   # number of classes
  random_order = FALSE #T: random order of classes (harder task)
){
   X = data.frame() # data matrix (each row = single sample)
   y = data.frame() # class labels

for (j in (1:K)){
  r = seq(0.05,1,length.out = N) # radius
  t = seq((j-1)*4.7,j*4.7, length.out = N) + rnorm(N, sd = 0.3) # theta
  Xtemp = data.frame(x =r*sin(t) , y = r*cos(t)) 
  ytemp = data.frame(matrix(j, N, 1))
  X = rbind(X, Xtemp)
  y = rbind(y, ytemp)
  }
spiral_data = cbind(X,y)
colnames(spiral_data) = c(colnames(X), 'label')
spiral_data$label = spiral_data$label

# Want randomly ordered labels?
if (random_order==TRUE) {spiral_data$label = sample(1:(K), size = N*K, 
                                                    replace = TRUE)}
return(spiral_data)
}
```

```{r}
## Create sine-wave Data ----
x  = seq(0, 2*pi, length.out = 100) 
sine_data = data.frame(x = x/max(x), ##normalized
                       y = sin(x))

##shuffle
sine_data = sine_data[sample(1:nrow(sine_data)), ]

plot(sine_data)
```


# A Model Object
Now we add batching into the training workflow. This requires just one additional argument (batch_size) but I'm going to have to tweak a lot of the train_model function.  

```{r build-model}
## Build Model----
initialize_model = function(){
    # Create an empty list for network objects
    
    ##if # of layers is predefined, pre-allocate the vector
    ##this is faster than the simple empty list
    # if (!is.null(n_layers)){
    #   layer_list = vector(mode = "list", length = n_layers)
    # } else layer_list = list()
    model = list()
    return(model)
}
add_dense = function(model, n_inputs, n_neurons, dropout_rate = 0,
                       weight_L1=0, weight_L2=0, bias_L1=0, bias_L2=0){
    pos = length(model) + 1 ##the layer's position in sequence of layers
    
    dense_args = list(n_inputs=n_inputs, n_neurons=n_neurons,
                        dropout_rate = dropout_rate,
                        weight_L1=weight_L1, weight_L2=weight_L2,
                        bias_L1=bias_L1, bias_L2=bias_L2,
                        class = "layer_dense", pos = pos)
    
    #layer_n = length(model) + 1 ##determine the layer's sequence number
    #layer_name = deparse(substitute(layer)) ##extract string with layer name
    
    # Add layer objects to the model
    model = c(model, "layer" = list(dense_args))
    # Give the layer a unique name corresponding to position in sequence
    names(model)[[pos]] = paste0("layer", pos)
    #names(model)[['new']] = layer_name ##rename the layer object appropriately
    return(model)
    
}
  # add_dropout = function(model, dropout_rate=0){
  #   pos = length(model) + 1
  #   base_layer = length(model) ##the dense layer that this layer is tied to
  #   dropout_args = list(dropout_rate=dropout_rate,
  #                       class = "layer_dropout", 
  #                       pos = pos, base_layer = base_layer)
  #   
  #   model = c(model, "layer" = list(dropout_args))
  #   names(model)[[pos]] = paste0("layer",pos)
  #   
  #   return(model)
  #},
add_activation =  function(model, activ_fn = c("linear", "relu", "sigmoid",
                                   "softmax_crossentropy")){
    class = activ_fn
    pos = length(model) + 1
    ##We want to tie the activation function back to the dense layer 
    ##its applied upon. This depends on if dropout was used in between
    base_layer = pos-1
    activ_args = list(class=class, pos=pos, base_layer=base_layer)
    model = c(model, "layer" = list(activ_args))
    names(model)[[pos]] = paste0("layer",pos)
    
    return(model)
}
add_loss = function(model, 
                      loss_fn=c("mse", "mae",
                                "binary_crossentropy",
                                "categorical_crossentropy")){
    
    pos = length(model) + 1
    loss_args = list(class=loss_fn, pos=pos)
    model = c(model, "layer" = list(loss_args))
    names(model)[[pos]] = paste0("layer",pos)
    
    return(model)
}
  # add_optimizer = function(model, optimizer = c("adam"),
  #                          learning_rate, lr_decay, epsilon){
  #   pos = length(model) + 1
  #   opt_args = list(class=optimizer, learning_rate=learning_rate,
  #                   lr_decay=lr_decay, epsilon=epsilon)
  #   model = c(model, "layer" = list(opt_args))
  #   names(model)[[pos]] = paste0("layer",pos)
  # }

```


```{r train_model}
## train_model----
train_model = function(model, inputs, y_true, epochs,
                       optimizer = c("adam"), 
                       learning_rate=0, lr_decay=0, epsilon=1e-7,...,
                       metric_list=NULL, 
                       validation_X=NULL, validation_y=NULL,
                       batch_size=NULL,
                       print_every=100){
  
  optim_args = list(...)
  if (!is.null(metric_list)){
      ##set metrics list to store metrics
      metrics = data.frame(epoch = 1:epochs) 
  }
  
# Setup ----
  ##How many unique "base layers" (dense layers)  
  baselayers = c()
  for (i in seq_along(model)){
    base_i = model[[i]]$base_layer
    baselayers = c(baselayers, base_i)
  }
  baselayers = unique(baselayers) ##return vector of each baselayer position
  
  # Set up the layer parameters
  layer_parameters = list()
  for (i in seq_along(baselayers)){
    ##Initialize empty list
    layer_parameters = c(layer_parameters, "layer"=list(NULL))
    names(layer_parameters)[[i]] = paste0("layer",baselayers[i])
  }
  ##the layer params list will be the length of unique baselayers and will
  ##be overwritten with the updated params at the end of the training loop
  
  ## Derive parameter settings from the selected optimizer
  if (optimizer=="adam"){
    ##requires cache and momentum
    cache_set=TRUE
    momentum_set=TRUE
    ##set default betas if not provided by user in `...`
    if (!exists(x = "beta_1", where = optim_args)){
      warning("Adam utilizes beta_1 and beta_2 hyperparameters which were not provided, and have been set to defaults of 0.9 and 0.999 respectively. Include specific 'beta_1' and 'beta_2' values in the arguments if desired.")
      optim_args$beta_1 = 0.9
      optim_args$beta_2 = 0.999
    }
    
  } else if (optimizer=="sgd"){
    ##momentum is optional, no cache
    if (exists(x = "momentum_rate", where = optim_args)){
      momentum_set=TRUE
      cache_set=FALSE
    } else {
      momentum_set=FALSE
      cache_set=FALSE
      optim_args$momentum_rate = 0
    }
  } else if (optimizer=="adagrad"){
    ##requires cache, no momentum
    cache_set=TRUE
    momentum_set=FALSE
  } else if (optimizer=="rmsprop"){
    ##requires cache, no momentum
    cache_set=TRUE
    momentum_set=FALSE
    ##requires rho hyperparameter
    if (!exists(x = "rho", where = optim_args)){
      warning("RMSProp utilizes the rho hyperparameter which was not provided, and has been set to a default of 0.9. Include a specific 'rho' value in the arguments if desired.")
      optim_args$rho = 0.9
    }
    
  } else {
    warning("Please use one of the following strings to select an optimizer: 'sgd', 'adagrad', 'rmsprop', 'adam'. Otherwise, the network will not perform as intended.")
  }
  
  # Initialize random params for every layer
  for (b in baselayers){
    current_layer = paste0("layer", b)
    n_inputs = model[[current_layer]]$n_inputs
    
    random_params = init_params(n_inputs = model[[current_layer]]$n_inputs,
                                n_neurons = model[[current_layer]]$n_neurons,
                                momentum = momentum_set, 
                                cache = cache_set)

    layer_parameters[[current_layer]] = random_params
    #names(layer_parameters)[[length(layer_parameters)]] = paste0("layer",b)
  }
  
  # Batch size
  if (!is.null(batch_size)){
    ##determine how many training "steps" will be needed 
    train_steps = nrow(inputs) %/% batch_size	 #(integer division)
    
    ##Since integer division rounds down, there may be some remaining samples
    ##not in a full batch. So we add 1 to include this last mini-batch
    if (train_steps*batch_size < nrow(inputs)){
      train_steps = train_steps + 1
    }
    # Now we repeat this process for the validation data
    if (!is.null(validation_X)){
      validation_steps = nrow(validation_X) %/% batch_size
      if(validation_steps*batch_size < nrow(validation_X)){
        validation_steps = validation_steps + 1
      }
    }
  } else { ##if no batch_size given, train in one batch
    batch_size = nrow(inputs)
    train_steps = 1
    validation_steps = 1
  }
  
# Training Loop ----  
for (epoch in 1:epochs){
  
 step_loss = 0      ##we will accumulate these over the steps
 step_reg_loss = 0  
 accumulated_count = 0
   
 ##set metrics list to store batch metrics (reset each epoch) 
 batch_metrics = data.frame(step = 1:train_steps) 

 for(step in 1:train_steps){
   
  #Deal with batches:
   ##if no batch_size given, then train using one step and full dataset
   if (train_steps == 1){
     batch_X = inputs
     batch_y = y_true
     
   ##otherwise slice a batch
   } else { 
     if (step != train_steps){
       batch_X = inputs[((step-1)*batch_size +1):(step*batch_size), ]
       batch_y = y_true[((step-1)*batch_size +1):(step*batch_size) ]
     } else {
       ##for the last step, need to index precisely to the last row
       batch_X = inputs[((step-1)*batch_size +1):(nrow(inputs)), ]
       batch_y = y_true[((step-1)*batch_size +1):(nrow(inputs)) ]
     }
     ## +1 keeps the process from reusing the sample at the end of last batch
   }
    
  # Forward Pass----
  layers = list() ##container to store layer objects, resets every step/epoch
  ##First layer
  dense1 = layer_dense$forward(inputs = batch_X,
                              parameters = layer_parameters[[1]], ##the randoms
                              weight_L1 = model[[1]]$weight_L1,
                              weight_L2 = model[[1]]$weight_L2,
                              bias_L1 = model[[1]]$bias_L1,
                              bias_L2 = model[[1]]$bias_L2
                              )
  dropout1 = layer_dropout$forward(input_layer = dense1,
                                  dropout_rate = model[[1]]$dropout_rate)
  
  ###determine activation function  
  activ_fn1 = model[[2]]$class ##second layer is the first activation layer
  
  if (activ_fn1=="linear"){
    activ1 = activation_Linear$forward(input_layer = dropout1)
  } else if (activ_fn1=="relu"){
    activ1 = activation_ReLU$forward(input_layer = dropout1)
  } else if (activ_fn1=="sigmoid"){
    activ1 = activation_Sigmoid$forward(input_layer = dropout1)
  } else if (activ_fn1=="softmax"){
    warning("Pure Softmax is broken Hans")
    activ1 = activation_Softmax$forward(inputs = dropout1$output)
  } else if (activ_fn1=="softmax_crossentropy"){
    activ1 = activation_loss_SoftmaxCrossEntropy$forward(
              input_layer = dropout1,
              y_true = y_true)
  } else message("Please enter one of the following strings in $add_activation to select an activation function: 'linear', 'relu', 'sigmoid', 'softmax_crossentropy'")
  
  output1 = activ1
  
  layer1 = list(dense=dense1,dropout=dropout1, activ=activ1, output=output1)
  layers = c(layers, "layer1" = list(layer1))

if (length(baselayers) > 1){
  ## Next Layers
  for (b in baselayers[-1]){##for all other base layers (- first element/layer)
    
    ##index of prior baselayer (the baselayer that comes next in seq, hence +1)
    prior_baselayer = baselayers[which(baselayers==b)-1] 
    ##which will serve as the input layer
    input_layer = paste0("layer",prior_baselayer)
    current_layer = paste0("layer",b)

    #prior_baselayer = baselayers[b] ##index of prior baselayer (includ. 1st)
    #input_layer = paste0("layer",prior_baselayer)
    
    dense_b = layer_dense$forward(inputs = layers[[input_layer]]$output$output,
                                parameters = layer_parameters[[current_layer]],
                                weight_L1 = model[[current_layer]]$weight_L1,
                                weight_L2 = model[[current_layer]]$weight_L2,
                                bias_L1 = model[[current_layer]]$bias_L1,
                                bias_L2 = model[[current_layer]]$bias_L2
                                )
    dropout_b = layer_dropout$forward(input_layer = dense_b,
                            dropout_rate = model[[current_layer]]$dropout_rate)
    
    activ_fn = model[[b+1]]$class
    ##current layer+1 since activation follows dense.
    
    if (activ_fn=="linear"){
      activ_b = activation_Linear$forward(input_layer = dropout_b)
    } else if (activ_fn=="relu"){
      activ_b = activation_ReLU$forward(input_layer = dropout_b)
    } else if (activ_fn=="sigmoid"){
      activ_b = activation_Sigmoid$forward(input_layer = dropout_b)
    } else if (activ_fn=="softmax"){
      warning("Pure Softmax is broken Hans")
      activ_b = activation_Softmax$forward(inputs = dropout_b$output)
    } else if (activ_fn=="softmax_crossentropy"){
      activ_b = activation_loss_SoftmaxCrossEntropy$forward(
                input_layer = dropout_b,
                y_true = batch_y)
    } else message("Please enter one of the following strings in $add_activation to select an activation function: 'linear', 'relu', 'sigmoid', 'softmax_crossentropy'")
  
    output_b = activ_b
    
    layer = list(dense=dense_b,dropout=dropout_b,activ=activ_b,output=output_b)
    layers = c(layers, "layer" = list(layer))
    names(layers)[[length(layers)]] = paste0("layer",b)
  }#end loop
}  
  
  # Calculate Loss ----
  ##Determine the selected loss function
  loss_fn = model[[length(model)]]$class ##last layer should be a loss layer
  
  if (loss_fn=="mse"){
    loss_layer = loss_MeanSquaredError$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = batch_y
      )
  } else if (loss_fn=="mae"){
    loss_layer = loss_MeanAbsoluteError$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = batch_y
    )
  } else if (loss_fn=="binary_crossentropy"){
    loss_layer = loss_BinaryCrossentropy$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = batch_y
    )
  } else if (loss_fn=="categorical_crossentropy"){
    loss_layer = layers[[length(layers)]]$output
  }
  
  # Calculate Regularized Loss
  ##If regularization is not used, this code will still run but reg_loss
  ##will equal zero.
  layers_reg_loss = c()
  for (b in baselayers){
    current_layer = paste0("layer",b)
    reg_loss_b = regularization_loss(layers[[current_layer]]$dense)
    layers_reg_loss = c(layers_reg_loss, reg_loss_b)
  }
  reg_loss = sum(layers_reg_loss)
  step_reg_loss = step_reg_loss + reg_loss
  
  # Calculate any other metrics ----
  ##Determine which metrics to calculate, then calculate and save
  for (metric in metric_list){
    
    if (metric=="mse"){
      mse = loss_MeanSquaredError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = batch_y
      )
      metric_i = (mse$sample_losses)
    } else if (metric=="mae"){
      mae = loss_MeanAbsoluteError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = batch_y
      )
      metric_i = (mae$sample_losses)
    } else if (metric=="accuracy"){ ##depends on if binary or cateogircal task
      if (loss_fn=="binary_crossentropy"){
        pred = (layers[[length(layers)]]$output$output > 0.5) * 1 
                  ##returns TRUE (1) if true, and FALSE (0) if false
      } else if (loss_fn=="categorical_crossentropy"){
        pred = max.col(layers[[length(layers)]]$output$output,
                                 ties.method = "random")
      }
      accuracy = (pred==batch_y)
      metric_i = accuracy
    } else if (metric=="regression_accuracy"){
      reg_pred = layers[[length(layers)]]$output$output
      accuracy_precision = sd(batch_y)/max(batch_y)
      ##count number of "accurate" preictions
      reg_accuracy = (abs(reg_pred - batch_y) < accuracy_precision)
      metric_i = reg_accuracy
    }
    
    ##add metric to the batch_metrics dataframe (unless not tracking metrics)
    ##accumulate the sums of the sample losses and then calculate epoch-wise
    ##average at the end
    if (!is.null(metric_list)){
      batch_metrics[step, "count"] = length(metric_i)
      batch_metrics[step, paste0(metric)] = sum(metric_i, na.rm = T)
    }
  }#end metrics loop
  
  # Backward Pass ----
  layers_back = list()
  
  ## The first backprop layer (the last layer in the training sequence)
  last_layer = paste0("layer",baselayers[length(baselayers)])
  ###First backprop the loss function
  ###Determine which loss function was chosen and backprop it
  loss_fn_last = model[[length(model)]]$class
  
  if (loss_fn_last=="mse"){
    loss_backprop1 = loss_MeanSquaredError$backward(
                      dvalues = layers[[length(layers)]]$output$output,
                      loss_layer = loss_layer
    )
  } else if (loss_fn_last=="mae"){
    loss_backprop1 = loss_MeanAbsoluteError$backward(
                      dvalues = layers[[length(layers)]]$output$output,
                      loss_layer = loss_layer
    )
  } else if (loss_fn_last=="binary_crossentropy"){
    loss_backprop1 = loss_BinaryCrossentropy$backward(
                      dvalues = layers[[length(layers)]]$output$output,
                      y_true = batch_y
    )
  } else if (loss_fn_last=="categorical_crossentropy"){
    loss_backprop1 = activation_loss_SoftmaxCrossEntropy$backward(
                      dvalues = layers[[length(layers)]]$output$output,
                      y_true = batch_y
    )
  } else warning("Please select an available loss function when building the model.")  
  
  ###Then backprop the activation function
  activ_fn_last = model[[length(model)-1]]$class ##last layer before loss layer
  
  if (activ_fn_last=="linear"){
    activ_backprop1 = activation_Linear$backward(d_layer = loss_backprop1)
  } else if (activ_fn_last=="relu"){
    activ_backprop1 = activation_ReLU$backward(d_layer = loss_backprop1,
                                      layer = layers[[last_layer]]$activ)
  } else if (activ_fn_last=="sigmoid"){
    activ_backprop1 = activation_Sigmoid$backward(d_layer = loss_backprop1,
                                      layer = layers[[last_layer]]$activ)
  } else if (activ_fn_last=="softmax"){
    warning("pure softmax is incomplete")
  } else if (activ_fn_last=="softmax_crossentropy"){
    activ_backprop1 = loss_backprop1 ##due to combined softmax/crossent fn
  }
  ###Then backprop the dropout layer, IF dropout was used
  if (model[[baselayers[length(baselayers)]]]$dropout_rate == 0){
    #dropout1 = FALSE 
    d_layer_todense1=activ_backprop1
  } else {
    #dropout1 = TRUE
    dropout_backprop1 = layer_dropout$backward(
                        d_layer = activ_backprop1, 
                        layer_dropout = layers[[length(layers)]]$dropout
      )
    d_layer_todense1=dropout_backprop1
  }

  ###Finally, backprop the dense layer
  ###Note: the layer_dense backward fn auto-adjusts if regularization is used
  dense_backprop1 = layer_dense$backward(d_layer = d_layer_todense1,
                                        layer = layers[[length(layers)]]$dense)
  
  layer_b = list(dense_backprop=dense_backprop1)
  layers_back = c(layers_back, "layer" = list(layer_b))
  names(layers_back)[[length(layers_back)]] = ##this is the last baselayer
                          paste0("layer",baselayers[length(baselayers)])
  
  ## Next Layers
  for (b in rev(baselayers[-length(baselayers)])){
    ##for all layers but the last (rev order since moving back through layers)
    
    ##index of prior baselayer (the baselayer that comes next in seq, hence +1)
    prior_baselayer = baselayers[which(baselayers==b)+1] 
    ##which will serve as the input layer
    input_layer = paste0("layer",prior_baselayer)

    current_layer = paste0("layer", b)
    
    activ_fn = model[[b+1]]$class
    if (activ_fn=="linear"){
      activ_backprop = activation_Linear$backward(
        d_layer = layers_back[[input_layer]]$dense_backprop)
    } else if (activ_fn=="relu"){
      activ_backprop = activation_ReLU$backward(
        d_layer = layers_back[[input_layer]]$dense_backprop,
        layer = layers[[current_layer]]$activ)
    } else if (activ_fn=="sigmoid"){
      activation_Sigmoid$backward(
        d_layer = layers_back[[input_layer]]$dense_backprop,
        layer = layers[[current_layer]]$activ)
    } #else if "softmax" once pure softmax is fixed
    
    if (model[[baselayers[b]]]$dropout_rate == 0){
      #dropout_rt = FALSE 
      d_layer_todense=activ_backprop
    } else {
      #dropout_rt = TRUE
      dropout_backprop = layer_dropout$backward(
                       d_layer = activ_backprop, 
                       layer_dropout = layers[[current_layer]]$dropout
      )
      d_layer_todense=dropout_backprop
    }

    dense_backprop = layer_dense$backward(d_layer = d_layer_todense,
                                      layer = layers[[current_layer]]$dense)
    layer_b = list(dense_backprop=dense_backprop)
    layers_back = c(layers_back, "layer" = list(layer_b))
    names(layers_back)[[length(layers_back)]] = paste0("layer",b)
  }
  
  # Optimize the Parameters ----
  for (b in baselayers){
    current_layer = paste0("layer", b)
    
    if (optimizer=="sgd"){
      optimal_params_b = optimizer_SGD$update_params(
        layer_forward = layers[[current_layer]]$dense,
        layer_backward = layers_back[[current_layer]]$dense_backprop,
        learning_rate = learning_rate,
        lr_decay = lr_decay, iteration = epoch,
        momentum_rate = optim_args$momentum_rate
      )
    } else if (optimizer=="adagrad"){
      optimal_params_b = optimizer_AdaGrad$update_params(
        layer_forward = layers[[current_layer]]$dense,
        layer_backward = layers_back[[current_layer]]$dense_backprop,
        learning_rate = learning_rate,
        lr_decay = lr_decay, iteration = epoch,
        epsilon = epsilon
      )
    } else if (optimizer=="rmsprop"){
      optimal_params_b = optimizer_RMSProp$update_params(
        layer_forward = layers[[current_layer]]$dense,
        layer_backward = layers_back[[current_layer]]$dense_backprop,
        learning_rate = learning_rate,
        lr_decay = lr_decay, iteration = epoch,
        epsilon = epsilon, 
        rho = optim_args$rho
      )
    } else if (optimizer=="adam"){
      optimal_params_b = optimizer_Adam$update_params(
        layer_forward = layers[[current_layer]]$dense,
        layer_backward = layers_back[[current_layer]]$dense_backprop,
        learning_rate = learning_rate,
        lr_decay = lr_decay, iteration = epoch,
        epsilon = epsilon,
        beta_1 = optim_args$beta_1,
        beta_2 = optim_args$beta_2
      )
    }
                      
    layer_parameters[[current_layer]] = optimal_params_b
  }
  
  ## Calculate loss for the batch per sample
  ## accumulate over the batches and we will use the average for the epoch
  step_loss = step_loss + sum(loss_layer$sample_losses)
      ##each step, we sum up the loss for all of the samples. we accumulate
      ##this sum over all the steps. thus, to get the average for the epoch
      ##we will divide this accumulated sum by the total # of sample_losses,
      ##which will be equivalent to the total number of inputs
      accumulated_count = accumulated_count + length(loss_layer$sample_losses)
      ##this should be about equal but may differ due to overlapping
 
 }##end batch steps loop

 # Calculate loss for the epoch
 ##First calculate the average sample loss
 avg_sample_loss = step_loss / accumulated_count
 avg_reg_loss = step_reg_loss / train_steps ##will = 0 if no regularization
   
 data_loss = avg_sample_loss
 loss = avg_sample_loss + avg_reg_loss
 
 # Calculate the epoch-wise metrics     
 for (metric in metric_list){
   ##sum up the losses/accuracies across the batches
   ##(each column in batch_metrics is a metric, each row a batch step)
   ##divide by the total number of inputs to get the average
   epoch_metric = sum(batch_metrics[[paste0(metric)]]) / 
                      sum(batch_metrics[["count"]])
   
   # mean_metric = mean(batch_metrics[[paste0(metric)]])
   metrics[epoch, paste0(metric)] = epoch_metric
  
 } 


# Validation Steps ----
# (If batching the inputs, probably want to batch the validation data too)
# (If validation data is small, this will all happen in one step anyways)
if(!is.null(validation_X)){
 batch_valid_loss = c()
 
 for (step in validation_steps){
   if (validation_steps == 1){
     valid_batch_X = validation_X
     valid_batch_y = validation_y
     
   ##otherwise slice a batch
   } else { 
     if (step != validation_steps){
       valid_batch_X = 
                  validation_X[((step-1)*batch_size +1):(step*batch_size), ]
       valid_batch_y = 
                  validation_y[((step-1)*batch_size +1):(step*batch_size) ]
     } else {
       ##for the last step, need to index precisely to the last row
       valid_batch_X = 
                  validation_X[((step-1)*batch_size +1):(nrow(validation_X)), ]
       valid_batch_y = 
                  validation_y[((step-1)*batch_size +1):(nrow(validation_X)) ]
     }
   }

  ###Utilize the test_model() fn to test on validation data, if provided
  current_fit = list("parameters" = layer_parameters)
  
  if (is.null(validation_y)){
    warning("provide both validation_X and validation_y")
    } else{
      validation = test_model(model = model,
                              trained_model = current_fit,
                              X_test = valid_batch_X,
                              y_test = valid_batch_y,
                              metric_list = metric_list)
    valid_loss_i = validation$loss
    ##includes regularization if used
    }
  batch_valid_loss = c(batch_valid_loss, valid_loss_i)
  
  } #end validation steps loop

 validation_loss = mean(batch_valid_loss) 

}
 
 # Status Report----
  if (epoch == 1){
    report_head = c("epoch", "loss")
    if (!is.null(validation_X)) report_head = c(report_head, "validation_loss")
    report_head = c(report_head, metric_list)
    print(report_head)
    }
  if (epoch %in% seq(0,epochs,by=print_every)){
    report = c(epoch, loss)
    if (!is.null(validation_X)) report = c(report, validation_loss)
    for (metric in metric_list){
      value = metrics[epoch, paste0(metric)]
      report = c(report, value)
    }
     
    
   report = sapply(report, round, 7)
   print(report)
   #report_history = rbind(report_history, report)
  } 
  
  ## Send the losses to the metrics dataframe for tracking
  metrics[epoch, "loss"] = loss
  metrics[epoch, "data_loss"] = data_loss
  if (!is.null(validation_X)){
    metrics[epoch, "validation_loss"] = validation_loss
  }
  
}##end epoch loop
  
  ##Final Output----
  final_metrics = list()
  validation_metrics = list()
  final_metrics[["loss"]] = loss
  final_metrics[["data_loss"]] = data_loss
  if (!is.null(validation_X)){
    final_metrics[["validation_loss"]] = validation_loss
  }
  for (metric in metric_list){
    final_metrics[[paste0(metric)]] = metrics[epochs, paste0(metric)]
      if (!is.null(validation_X)){
        validation_metrics[[paste0(metric)]] =
                          validation$metrics[[paste0(metric)]]
      }
  }
  
  ##Final Predictions
  raw_predictions = layers[[length(layers)]]$output$output
  if (loss_fn=="categorical_crossentropy"){
    predictions = max.col(raw_predictions, ties.method = "random")
  } else if (loss_fn=="binary_crossentropy"){
    predictions = (raw_predictions > 0.5) * 1 
  }
  else {
    if (ncol(raw_predictions) > 1){
      ##if using multiple output neurons for a regression or binary problem...
      ##this is temporary, may want to change
      predictions = rowMeans(raw_predictions)
    } else predictions = raw_predictions
  }
  
  
  ##Save final model parameters
  parameters = list()
  for (b in baselayers){
    params_b = layer_parameters[[paste0("layer",b)]]
    parameters = c(parameters, "layer"=list(params_b))    
    names(parameters)[[length(parameters)]] = paste0("layer",b)
  }
  
  ##Returns
  list(final_metrics=final_metrics, validation_metrics=validation_metrics,
       metrics=metrics, 
       predictions=predictions, raw_predictions=raw_predictions,
       parameters=parameters)
}


```

```{r test_model}
# test_model()----
test_model = function(model, trained_model, 
                      X_test, y_test,
                      metric_list=NULL){
  inputs = X_test
  y_true = y_test
  ##How many baselayers were used
  baselayers = c()
  for (i in seq_along(model)){
    base_i = model[[i]]$base_layer
    baselayers = c(baselayers, base_i)
  }
  baselayers = unique(baselayers) ##return vector of each baselayer position

 # Forward Pass----
  layers = list()
  ##First layer
  dense1 = layer_dense$forward(inputs = inputs,
                parameters = trained_model$parameters[[1]],
                              weight_L1 = model[[1]]$weight_L1,
                              weight_L2 = model[[1]]$weight_L2,
                              bias_L1 = model[[1]]$bias_L1,
                              bias_L2 = model[[1]]$bias_L2
                              )
  ###determine activation function  
  activ_fn1 = model[[2]]$class ##second layer is the first activation layer
  
  if (activ_fn1=="linear"){
    activ1 = activation_Linear$forward(input_layer = dense1)
  } else if (activ_fn1=="relu"){
    activ1 = activation_ReLU$forward(input_layer = dense1)
  } else if (activ_fn1=="sigmoid"){
    activ1 = activation_Sigmoid$forward(input_layer = dense1)
  } else if (activ_fn1=="softmax"){
    warning("Pure Softmax is broken Hans")
    activ1 = activation_Softmax$forward(inputs = dense1$output)
  } else if (activ_fn1=="softmax_crossentropy"){
    activ1 = activation_loss_SoftmaxCrossEntropy$forward(
              input_layer = dense1,
              y_true = y_true)
  }
  output1 = activ1
  
  layer1 = list(dense=dense1, activ=activ1, output=output1)
  layers = c(layers, "layer1" = list(layer1))

if (length(baselayers) > 1){
  # Next Layers
  for (b in baselayers[-1]){##for all other base layers (- first element/layer)
    
    ##index of prior baselayer (the baselayer that comes next in seq, hence +1)
    prior_baselayer = baselayers[which(baselayers==b)-1] 
    ##which will serve as the input layer
    input_layer = paste0("layer",prior_baselayer)
    current_layer = paste0("layer",b)

    dense_b = layer_dense$forward(inputs = layers[[input_layer]]$output$output,
                      parameters = trained_model$parameters[[current_layer]],
                                weight_L1 = model[[current_layer]]$weight_L1,
                                weight_L2 = model[[current_layer]]$weight_L2,
                                bias_L1 = model[[current_layer]]$bias_L1,
                                bias_L2 = model[[current_layer]]$bias_L2
                                )

    activ_fn = model[[b+1]]$class
    ##current layer+1 since activation follows dense.
    
    if (activ_fn=="linear"){
      activ_b = activation_Linear$forward(input_layer = dense_b)
    } else if (activ_fn=="relu"){
      activ_b = activation_ReLU$forward(input_layer = dense_b)
    } else if (activ_fn=="sigmoid"){
      activ_b = activation_Sigmoid$forward(input_layer = dense_b)
    } else if (activ_fn=="softmax"){
      warning("Pure Softmax is broken Hans")
      activ_b = activation_Softmax$forward(inputs = dense_b$output)
    } else if (activ_fn=="softmax_crossentropy"){
      activ_b = activation_loss_SoftmaxCrossEntropy$forward(
                input_layer = dense_b,
                y_true = y_true)
    }
    output_b = activ_b
    
    layer = list(dense=dense_b,activ=activ_b,output=output_b)
    layers = c(layers, "layer" = list(layer))
    names(layers)[[length(layers)]] = paste0("layer",b)
  }#end loop
}  

  # Calculate Loss Layer
  ##Determine the selected loss function
  loss_fn = model[[length(model)]]$class ##last layer should be a loss layer
  
  if (loss_fn=="mse"){
    loss_layer = loss_MeanSquaredError$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = y_true
      )
  } else if (loss_fn=="mae"){
    loss_layer = loss_MeanAbsoluteError$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = y_true
    )
  } else if (loss_fn=="binary_crossentropy"){
    loss_layer = loss_BinaryCrossentropy$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = y_true
    )
  } else if (loss_fn=="categorical_crossentropy"){
    loss_layer = layers[[length(layers)]]$output
  }
  
  # Calculate Regularized Loss
  ##If regularization is not used, this code will still run but reg_loss
  ##will equal zero.
  layers_reg_loss = c()
  for (b in baselayers){
    current_layer = paste0("layer",b)
    reg_loss_b = regularization_loss(layers[[current_layer]]$dense)
    layers_reg_loss = c(layers_reg_loss, reg_loss_b)
  }
  reg_loss = sum(layers_reg_loss)
  
  ## Calculate final loss----
  data_loss = mean(loss_layer$sample_losses)
  loss = data_loss + reg_loss ##loss = data_loss if regularization not used
  
  ## Calculate other metrics
  metrics = list()
  for (metric in metric_list){
    if (metric=="mse"){
      mse = loss_MeanSquaredError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = y_true
      )
      metric_i = mean(mse$sample_losses)
    } else if (metric=="mae"){
      mae = loss_MeanAbsoluteError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = y_true
      )
      metric_i = mean(mae$sample_losses)
    } else if (metric=="accuracy"){ ##depends on if binary or cateogircal task
      if (loss_fn=="binary_crossentropy"){
        pred = (layers[[length(layers)]]$output$output > 0.5) * 1 
                  ##returns TRUE (1) if true, and FALSE (0) if false
      } else if (loss_fn=="categorical_crossentropy"){
        pred = max.col(layers[[length(layers)]]$output$output,
                                 ties.method = "random")
      }
      accuracy = mean(pred==y_true, na.rm = T)
      metric_i = accuracy
    } else if (metric=="regression_accuracy"){
      reg_pred = layers[[length(layers)]]$output$output
      accuracy_precision = sd(y_true)/max(y_true)
      reg_accuracy = mean(abs(reg_pred - y_true) < accuracy_precision)
      metric_i = reg_accuracy
    }
    
    ##add metric to the metrics list (unless don't want to track them)
    metrics = c(metrics, "m" = list(metric_i))
    names(metrics)[[length(metrics)]] = paste0(metric)
  
  }#end metrics loop
  
  ## Final Predictions
  raw_predictions = layers[[length(layers)]]$output$output
  if (loss_fn=="categorical_crossentropy"){
    predictions = max.col(raw_predictions, ties.method = "random")
  } else if (loss_fn=="binary_crossentropy"){
    predictions = (raw_predictions > 0.5) * 1 
  }
  else {
    if (ncol(raw_predictions) > 1){
      ##if using multiple output neurons for a regression or binary problem...
      ##this is temporary, may want to change
      predictions = rowMeans(raw_predictions)
    } else predictions = raw_predictions
  }
 
  
  ##Output:
  list(predictions=predictions, raw_predictions = raw_predictions,
       loss=loss, data_loss=data_loss,
       metrics=metrics)
  
}

```

TODO: 
- finish the batching... was working on accumulating loss across batches and determining the average for the epoch. then replicating with other metrics.  
 - then need to do the same for the validation set  
 - nnfs pg 550


- add other classification metrics (specificity, sensitivity, precision, F1...)







Now we can run the same training loop as above with just a few lines of code.  

```{r}
##Create sine-wave datset
x  = seq(0, 2*pi, length.out = 100) 
sine_data = data.frame(x = x/max(x), ##normalized
                       y = sin(x))
sine_data = sine_data[sample(1:nrow(sine_data)), ] ##shuffle

X = as.matrix(sine_data$x)

# Build model
model = initialize_model()
model = add_dense(model, n_inputs = 1, n_neurons = 128, 
                        dropout_rate = 0)
model = add_activation(model, activ_fn = "relu")
model = add_dense(model, n_inputs = 128, n_neurons = 64,
                        dropout_rate = 0)
model = add_activation(model, activ_fn = "relu")
model = add_dense(model, n_inputs = 64, n_neurons = 1,
                        dropout_rate = 0)
model = add_activation(model, activ_fn = "linear")
model = add_loss(model, loss_fn = "mse")


fit = train_model(model, inputs = X, y_true = sine_data$y, 
            epochs = 1000, optimizer = "adam",
            learning_rate = 0.005, lr_decay = 1e-3,
            epsilon = 1e-7, beta_1 = 0.5, beta_2 = 0.5,
            metric_list = c("mse", "mae", "regression_accuracy")
            )

```

Using the same hyperparameters as before, we can see that the new workflow yields the exact same results.  
```{r}
plot(X, fit$predictions)
```

We can also plot the fit history over the epochs.  
```{r}
plot(fit$metrics$epoch, fit$metrics$mse, type = "l")
```


Let's try a classification task. 
First, a binary problem where we'll use a sigmoid function for the final activation function and binary crossentropy for a loss function. Note that piping makes building a model even easier. 
```{r}
spiral_binary = sim_data_fn(N = 200, K = 2)
X = spiral_binary[,c("x", "y")] ## leave out 'label'
X = as.matrix(X)
y_true = spiral_binary$label - 1

binary_model = initialize_model() |>
  add_dense(n_inputs = ncol(X), n_neurons = 64, dropout_rate = 0.2) |>
  add_activation(activ_fn = "relu") |>
  add_dense(n_inputs = 64, n_neurons = 1, dropout_rate = 0) |>
  add_activation(activ_fn = "sigmoid") |>
  add_loss(loss_fn = "binary_crossentropy")

fit_binary = train_model(binary_model, inputs = X, y_true = y_true,
                    epochs = 100, optimizer = "adam", 
                    learning_rate = 0.001, lr_decay = 5e-7,
                    metric_list = c("accuracy"),
                    beta_1 = 0.2, beta_2 = 0.2)

fit_binary$final_metrics
```


Now let's go back to multi-class classification, where we'll use a softmax function for the final activation function and categorical crossentropy for a loss function (we'll use our combined softmax activation + categorical crossentropy function as before). I'll also add some L2 regularization to the parameters. Validation data gives a sense of how the model does out-of-sample. Finally, we'll train in batches to demonstrate the new feature of the training function.  

```{r}
# Training data
spiral_data = sim_data_fn(N=100, K=3)
X = as.matrix(spiral_data[,c("x","y")])
y_true = spiral_data$label
# Validation data
validation_data = sim_data_fn(N=50,K=3)
X_valid = as.matrix(validation_data[,c("x","y")])
y_valid = validation_data$label

categ_model = initialize_model() |>
  add_dense(n_inputs = ncol(X), n_neurons = 128, dropout_rate = 0.01,
            weight_L2 = 5e-4, bias_L2 = 5e-4) |>
  add_activation("relu") |>
  add_dense(n_inputs = 128, n_neurons = 64, dropout_rate = 0.001,
            weight_L2 = 5e-4, bias_L2 = 5e-4) |>
  add_activation("relu") |>
  add_dense(n_inputs = 64, n_neurons = 3) |>
  add_activation("softmax_crossentropy") |>
  add_loss("categorical_crossentropy")

#options(error=recover); debug(train_model)
fit_categ = train_model(categ_model, inputs = X, y_true = y_true,
                        epochs = 100, optimizer = "adam", 
                        learning_rate = 0.01, lr_decay = 1e-3, 
                        beta_1 = 0.5, beta_2 = 0.5,
                        metric_list = c("accuracy"),
                        validation_X = X_valid, validation_y = y_valid,
                        batch_size = 128)

fit_categ$final_metrics
```

```{r}
plot(fit_categ$metrics$epoch, fit_categ$metrics$loss, 
     type = "l", col = "red", xlab = "epoch", ylab = "loss")
lines(fit_categ$metrics$epoch, fit_categ$metrics$validation_loss, 
     type = "l", col = "blue")
legend(x = "topleft", legend = c("training", "validation"),
       lty = c(1, 1), col = c("red", "blue"))

```
The model's performance suffers slighltly when using batches, but this will be useful for larger datasets.  

Let's test our last categorical model on testing data.  

```{r}
# First make some testing data
test_data = sim_data_fn(200, K=3) ##600 samples, bigger than the training data!
X_test = as.matrix(test_data[,c("x","y")])
y_test = test_data$label

# Now run the test_model function
#options(error=recover); debug(test_model)
final_categ = test_model(model = categ_model,
                         trained_model = fit_categ,
                         X_test = X_test, y_test = y_test,
                         metric_list = "accuracy")


final_categ$metrics
final_categ$loss

#True test data
plot(X_test, col = y_test)

#Predicted test data
plot(X_test, col = final_categ$predictions)
```






