---
title: "part4"
author: "Hans Elliott"
date: "4/28/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

NOTE: currently, for multi-class, the classes would have to be encoded starting at 1, since code relies on index (R cols start at 1.)

## Full code
In the last section, we worked on improving model generalization. In addition to testing our neural net's predictions on test data, we added L1 and L2 regularization and a dropout layer to our set of functions.  
I've also cleaned up the functions a bit so things run smoother. Here's a list of the updates:   
- Overall, my changes are a move towards a new system which I think will be easy to implement and will reduce the amount of overhead needed to actually use the functions. Basically, the heart of the process will be the layer objects that get created for each hidden and output layer. The layer objects are stocked full of other objects which they can pass on to the various other functions as needed. That way, all we really have to do is pass on one layer into the next, and the only other arguments we need to specify are hyperparameters.   
- The `init_params` function will save the n_neurons in addition to all of the parameters since it was redundant to include n_neurons as an argument in both the initialization function and the `layer_dense` function. The `layer_dense` function is updated to extract the n_neurons object from the parameters object.  
- Since the init_params fns defines the number of neurons, we will need the optimized paramaters object at the end of backpropagation to also contain a n_neurons object, so that when the forward pass begins again the layer_dense function can find it. This is fairly simple since we are already passing the forward layer object to the optimizer function, and thus it can extract the number of neurons from the forward layer's output matrix - the number of columns in the output matrix corresponds to the number of neurons.  
- There was some redundancy with the backprop step, where I was specifying "dvalues" as the derivative values being passed back from the next layer into the current layer. I simplify this so that all we have to do is pass back that next layer object (slightly confusing - this is backprop layer corresponding to the forward-pass layer that is the next layer in the forward pass, so technically it is the previous layer in the backprop). I'm calling this argument `d_layer.` The idea is that you just pass back each backpropagated layer sequentially. Thus every backpropagation step takes two simple arguments: the forward pass layer object, and the backward pass layer object of the "next" layer. This works fairly well except


```{r nn-fns}
### Initialize Layer Parameters-----------------------------------------------
init_params = function(n_inputs = "# of features",
                       n_neurons = "desired # of neurons",
                       momentum = FALSE,
                       cache = FALSE){
    
    weights = matrix(data = (0.1 * rnorm(n = n_inputs*n_neurons)),
                             nrow = n_inputs, ncol = n_neurons)
           #Number of weights = the number of inputs*number of neurons,
           #since every connection between the previous neurons (from input) and
           #the current neurons have an associated weight
    biases = matrix(data = 0, nrow = 1, ncol = n_neurons)
           #Number of biases = the number
    
    #momentum initialization
    weight_momentums = matrix(data = 0, 
                            nrow = nrow(weights),
                            ncol = ncol(weights))
    bias_momentums = matrix(data = 0,
                            nrow = nrow(biases),
                            ncol = ncol(biases))
    #cache initialization
    weight_cache = matrix(data = 0,
                          nrow = nrow(weights),
                          ncol = ncol(weights))
    bias_cache = matrix(data = 0,
                        nrow = nrow(biases),
                        ncol = ncol(biases))

    #saving:
    if (momentum == TRUE & cache == FALSE){ ##momentums only
          list("weights"=weights,"biases"=biases,
               "weight_momentums"=weight_momentums,
               "bias_momentums"=bias_momentums,
               "n_neurons"=n_neurons)
    } else if (momentum == FALSE & cache == TRUE){ ##cache only
          list("weights"=weights,"biases"=biases,
               "weight_cache"=weight_cache,
               "bias_cache"=bias_cache,
               "n_neurons"=n_neurons)
    } else if (momentum == TRUE &  cache == TRUE){ ##momentums and cache
          list("weights"=weights,"biases"=biases,
               "weight_momentums"=weight_momentums,
               "bias_momentums"=bias_momentums,
               "weight_cache"=weight_cache,
               "bias_cache"=bias_cache,
               "n_neurons"=n_neurons)
    } else if (momentum == FALSE & cache == FALSE){ ##no momentums or cache
          list("weights"=weights,"biases"=biases,"n_neurons"=n_neurons)
    }
}



### Dense Layer ---------------------------------------------------------------
layer_dense = list(
## FORWARD PASS 
 forward = function(    
            inputs,      
            parameters, ## from initialize_parameters
            weight_L1 = 0, weight_L2 = 0,  ##regularization
            bias_L1 = 0, bias_L2 = 0
){
  
 if(is.matrix(inputs) == FALSE ) message("Convert inputs to matrix first")
 
 n_inputs = ncol(inputs)
 n_neurons = parameters$n_neurons
 weights = parameters$weights
 biases = parameters$biases
 #Forward Pass 
 output = inputs%*%weights + biases[col(inputs%*%weights)]
 
 #Regularization values:
 regularization = list("weight_L1" = weight_L1, "weight_L2" = weight_L2,
                       "bias_L1" = bias_L1, "bias_L2" = bias_L2)

 #SAVING:
 #then layer saves momentum only
 if (exists(x = "weight_momentums", where = parameters) & 
     !exists(x = "weight_cache", where = parameters)){
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "weight_momentums"=parameters$weight_momentums, ##for momentum
      "bias_momentums"=parameters$bias_momentums,
      "regularization" = regularization)  ##for regularization
 #if momentum==FALSE & cache==TRUE, saves cache only
 } else if (!exists(x = "weight_momentums", where = parameters) &
            exists(x = "weight_cache", where = parameters)){
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "weight_cache"=parameters$weight_cache, ##for cache
      "bias_cache"=parameters$bias_cache,
      "regularization" = regularization)

 #if momentum==TRUE & cache==TRUE, saves both
 } else if (exists(x = "weight_momentums", where = parameters) &
            exists(x = "weight_cache", where = parameters)){
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "weight_momentums"=parameters$weight_momentums, ##for momentum
      "bias_momentums"=parameters$bias_momentums,
      "weight_cache"=parameters$weight_cache, ##for cache
      "bias_cache"=parameters$bias_cache,
      "regularization" = regularization)

 #else both==FALSE, ignore momentum & cache
 } else {
   #otherwise, just save
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "regularization" = regularization)
      }

 },#end fwd
 
# BACKWARD
  backward = function(d_layer="layer object that occurs prior in backward pass",
                      layer="the layer object from the forward pass"){
    
    dvalues = d_layer$dinputs
    #Gradients on parameters
    dweights = t(layer$inputs)%*%dvalues
    dbiases = colSums(dvalues)
    
    #Gradients on regularization
    ##regularization hyperparams:
    layer_reg = layer$regularization ##a list of the set lambda values

    ##L1 Weights##
    if (layer_reg$weight_L1 > 0){
    dL1 = matrix(1, nrow = nrow(layer$weights), ncol = ncol(layer$weights))
      #make matrix filled with 1s
    dL1[layer$weight < 0] = -1
      #convert matrix value to -1 where weight is less than zero
    dweights = dweights + layer_reg$weight_L1 * dL1
    }
    ##L2 Weights##
    if (layer_reg$weight_L2 > 0){
    dweights = dweights + 2 * layer_reg$weight_L2 * layer$weights
    }                     #2 * lambda * weights
    
    ##L1 Biases##
    if (layer_reg$bias_L1 > 0){
    dL1 = matrix(1, nrow = nrow(layer$biases), ncol = ncol(layer$biases))
      #make matrix filled with 1s
    dL1[layer$bias < 0] = -1
      #convert matrix value to -1 where weight is less than zero
    dbiases = dbiases + layer_reg$bias_L1 * dL1
    }
    ##L2 Biases##
    if (layer_reg$bias_L2 > 0){
    dbiases = dbiases + 2 * layer_reg$bias_L2 * layer$biases
    }
    
    #Gradients on values
    dinputs = dvalues%*%t(layer$weights) 
    
    #saves:
    list("dinputs"=dinputs,
         "dweights"=dweights,
         "dbiases"=dbiases)
}#end bwd
)



### Dropout Layer -------------------------------------------------------------
layer_dropout = list(
## FORWARD PASS
  forward = function(
              input_layer, ##layer to apply dropout to
              dropout_rate ##rate of neuron deactivation
) {
  inputs = input_layer$output   ##the outputs from the previous layer
  
  #Dropout mask/filter
  dropout_filter = matrix(data = 
                           rbinom(n = nrow(inputs)*ncol(inputs),
                                  size = 1,        
                                  p = (1-dropout_rate)),
                          nrow = nrow(inputs),
                          ncol = ncol(inputs)) / 
                  (1 - dropout_rate)
  ##Creates matrix that is shape of the input layer's output (from nrow, ncol)
  ##and fills it with 1s and 0s from "Bernoulli". The length of the rbinom 
  ##output is equal to nrow*ncol so it fills the input layer's shape. 
  ##We also apply the scaling step to the filter directly since it makes the 
  ##backprop step even simpler.
  
  ##Apply mask to inputs and scale by (1 - dropout_rate)
  output = inputs * dropout_filter
  
  list("output" = output, "dropout_filter" = dropout_filter)

},
## BACKWARD PASS
  backward = function(
            d_layer = "layer object that occurs prior in backward pass",
            #derivative values being passed back from next layer
            layer_dropout = "the layer object from the forward pass"
                                    ##the forward-pass dropout layer object
){
  dvalues = d_layer$dinputs ##extract the derivative object from the layer
  dinputs = dvalues * layer_dropout$dropout_filter    
  ##Thus if the filter is 0 at the given index, the derivative is 0
  ##And if the filter is 1/(1-dropout_rate) at the given index, 
  ##the derivative is dvalues * 1/(1-dropout_rate)
  list("dinputs" = dinputs)
  }

)#end layer_dropout

### Activation Functions------------------------------------------------------
## Linear ##
activation_Linear = list(
  forward = function(input_layer){
    output = input_layer$output
    list("output" = output)
  },
  
  backward = function(d_layer){
    dinputs = d_layer$dinputs
    list("dinputs" = dinputs)
    #derivative = 1, so 1 * dvalues = dvalues. dvalues correspond to the 
    #previously backpropagated layer's dinputs
    }
)


## ReLU ##
activation_ReLU = list(
  #FORWARD PASS
  forward = function(input_layer){
    
    inputs = input_layer$output
    output = matrix(sapply(X = inputs, 
                    function(X){max(c(0, X))}
                    ), 
                  nrow = nrow(inputs), ncol = ncol(inputs))
    #ReLU function coerced into a matrix so the shape
    #is maintained (it will be equivalent to that of the input shape)
    
    #Function saves:
    list("output" = output, "inputs" = inputs)
  },
  #BACKWARD PASS
  backward = function(d_layer, layer){
    
    inputs = layer$inputs
    dinputs = d_layer$dinputs ##the dinputs from the next layer
    
    dinputs[inputs <= 0] = 0
    #save:
    list("dinputs"=dinputs)
  }
)

## SoftMax ##
activation_Softmax = list(
  forward = function(inputs){
          #scale inputs
          max_value = apply(X = inputs, MARGIN = 2,  FUN = max)
          scaled_inputs = sapply(X = 1:ncol(inputs), 
                 FUN = function(X){
                    inputs[,X] - abs(max_value[X])})
          
          # exponetiate
          exp_values = as.matrix(exp(scaled_inputs))
          # normalize
          norm_base = matrix(rowSums(exp_values),
                             nrow = nrow(inputs), ncol = 1)
          probabilities = sapply(X = 1:nrow(inputs),
                          FUN = function(X){exp_values[X,]/norm_base[X,]}) 
          return(t(probabilities))
          #(transpose probabilities)
          },
  
  backward = function(softmax_output, dvalues){
    #*INCOMPLETE SECTION - don't use*
    #flatten output array
    flat_output = as.vector(softmax_output)
    
    #calculate jacobian matrix of output
    jacobian_matrix = diag(flat_output) - flat_output%*%t(flat_output)
    
    #calculate sample-wise gradient
    dinputs = jacobian_matrix%*%flat_dvalues
    
    
  }
)

## Sigmoid ##
activation_Sigmoid = list(
  forward = function(input_layer){
    inputs = input_layer$output
    sigmoid = 1 / (1 + exp(-inputs))
    list("output" = sigmoid)
},
  backward = function(d_layer = "the layer object being passed back",
                      layer = "the sigmoid layer obj from the forward pass"
                      ){
    dvalues = d_layer$dinputs ##the values being passed back
    output = layer$output ##the output from the forward-pass
    dinputs = dvalues * (1 - output) * output 
    list("dinputs" = dinputs)
  }
)

## Softmax X Cross Entropy ##
# combined softmax activation fn & cross-entropy loss for faster backprop
activation_loss_SoftmaxCrossEntropy = list(
  #FORWARD PASS
  forward = function(input_layer, y_true){
    
    inputs = input_layer$output
    #output layer's activation function
    softmax_out = activation_Softmax$forward(inputs)
    #calculate loss (unless this were in a prediction phase)
    if (!is.null(y_true)){
      sample_losses = Categorical_CrossEntropy$forward(softmax_out, y_true)
    } else sample_losses = NULL
    #function saves:
    list("output"=softmax_out, "sample_losses"=sample_losses) 
  },
  #BACKWARD PASS
  backward = function(dvalues, y_true){

    #Detect number of samples
    if (is.vector(dvalues)) {      #if one sample
      samples = 1
    } else if (is.array(dvalues)) {  #else if multiple samples
      samples = nrow(dvalues)
    } else message("error checking shape of inputs")
    
    #Reverse One-Hot Encoding
    #if labels are one-hot encoded, turn them into discrete values
     ##helper function 
    anti_ohe = function(y_true){
               unique_classes = ncol(y_true)
               samples = nrow(y_true)
               y_true_vec = as.vector(y_true)
                    
               class_key = rep(1:unique_classes, each = samples)
               y_true = class_key[y_true_vec==1]
                    #selects the classes that correspond to 1s in y_true vector
                    return(y_true)
                    }
     ##check & modify
    y_true = if(is.array(y_true)){ #if one-hot encoded
                    #change to sparse
                    anti_ohe(y_true)
              } else y_true
    
    #Calculate gradient
     #Copy so we can modify
     dinputs = dvalues
     #Calculate gradient
     #index the prediction array with the sample number and its
     #true value index, subtracting 1 from these values. Requires discrete,
     #not one-hot encoded, true labels (explaining the need for the above step)
     dinputs[cbind(1:samples, y_true)] = dinputs[cbind(1:samples, y_true)] - 1
     #Normalize gradient
     dinputs = dinputs/samples
    #save desired outputs
    list("dinputs" = dinputs, "samples" = samples, "y_true" = y_true)
  }
)


### Loss ----------------------------------------------------------------------
## MSE ##
loss_MeanSquaredError = list(
  forward = function(y_pred, y_true){
    ##calculate MSE for each sample (row)
    sample_losses = rowMeans( (y_true - y_pred)^2 )
      list("sample_losses" = sample_losses, "y_true" = y_true)
  },
  backward = function(dvalues = "linear activation fn output",
                      loss_layer = "loss layer object from forward pass"){
    
    y_true = loss_layer$y_true
    #number of samples
    samples = nrow(dvalues)
    #number of output neurons
    n_outputs = ncol(dvalues)
    
    #Gradient on values (dvalues = y_pred)
    dinputs = -2 * (y_true - dvalues) / n_outputs 
    
    #Normalize
    dinputs = dinputs / samples
    
    list("dinputs"=dinputs)
  }
)

## MAE ##
loss_MeanAbsoluteError = list(
  forward = function(y_pred, y_true){
    ##calculate MAE for each sample (row)
    sample_losses = rowMeans( abs(y_true - y_pred) )
      list("sample_losses" = sample_losses, "y_true" = y_true)
},
  backward = function(dvalues = "linear activation fn output",
                      loss_layer = "loss layer object from forward pass"){
    y_true = loss_layer$y_true
    #number of samples
    samples = nrow(dvalues)
    #number of output neurons
    n_outputs = ncol(dvalues)
    
    #calculate gradient (sign returns 1 for values >0, -1 for values <0)
    dinputs = sign(y_true - dvalues) / n_outputs
                            ##dvalues = y_pred
    #normalize gradients
    dinputs = dinputs / samples
    
    list("dinputs"=dinputs)
  }
)

## Binary Crossentropy ##
loss_BinaryCrossentropy = list(
  forward = function(y_pred, y_true){
    #clip data to prevent division by zero (both sides to keep mean unbiased)
    y_pred_clipped = ifelse(y_pred >= 1-1e-7, 1-1e-7,
                            ifelse(y_pred <= 1e-7, 1e-7, y_pred))
    
    #calculate sample-wise losse per neuron
    sample_losses = -(y_true*log(y_pred_clipped) + 
                                (1 - y_true)*log(1 - y_pred_clipped)
                        )
    #calculate total (mean) loss for each sample 
    ## (mean across neurons/vector of outputs, neurons = cols, samples = rows)
    sample_losses = rowMeans(sample_losses)
    
    #calculate mean loss across the entire batch
    data_loss = mean(sample_losses)
    
    list("sample_losses" = sample_losses, "data_loss" = data_loss)
  },
  backward = function(dvalues = "sigmoid output", y_true){
    
    #number of samples
    samples = nrow(dvalues)
    #number of output neurons
    n_outputs = ncol(dvalues)
    
    #clip data to prevent divide by zero
    clipped_dvalues = ifelse(dvalues >= 1-1e-7, 1-1e-7,
                            ifelse(dvalues <= 1e-7, 1e-7, dvalues))
    #calculate gradients
    dinputs = -(y_true / clipped_dvalues - 
                  (1 - y_true) / (1 - clipped_dvalues)
                  ) / n_outputs
    
    #normalize gradient
    dinputs = dinputs/samples
    ##We have to perform this normalization since each output returns its own
    ##derivative, and without normalization, each additional input will increase
    ##the gradients mechanically
    list("dinputs" = dinputs)
  }
)    


## Categorical Cross Entropy ##
Categorical_CrossEntropy = list(
  #FORWARD PASS
  forward = function(y_pred = "softmax output", y_true = "target labels"){
    
    #DETECT NUMBER OF SAMPLES
    samples = length(y_true)  

    #CLIP SAMPLES TO AVOID -Inf ERROR
    y_pred_clipped = ifelse(y_pred <= 1e-7, 1e-7, 
                        ifelse(y_pred >= (1-1e-7), (1-1e-7), y_pred))
    
    if (nrow(t(y_true)) == 1){ ##if y_true is a vector of labels (sparse)
      if (min(y_true) == 0){ #check if the first label is 0.
        y_true = y_true + 1  #add 1 to use y_true as index
      }
      confidences = y_pred_clipped[cbind(1:samples, y_true)]
    } else if (nrow(y_true) > 1){ #else if y_true is one-hot encoded
      confidences = rowSums(y_pred_clipped * y_true)
    } else warning("error indexing predicted class confidences [cat crossent]")
    
    #CALC LOSS FOR EACH SAMPLE (ROW)
    sample_losses = -log(confidences)
    return(sample_losses)
    
  },
  #BACKWARD PASS
  backward = function(y_true, d_layer){
    dvalues = d_layer$dinputs
    
    #number of samples
    samples = length(dvalues)
    
    #number of labels
    labels = length(unique(dvalues[1,]))
    
    #if labels are sparse, turn them into one-hot encoded vector
    y_true = ifelse(#if
                    nrow(t(y_true)) ==1,
                    #one-hot-encode
                    y_true = do.call(rbind,
                                     lapply(X = y_true,
                                       function(X) as.integer(
                                          !is.na(match(unique(
                                          unlist(y_true)
                                                        ), X)
                                                ))
                                      )), 
                    #else
                    y_true)
    
    #calculate gradient
    dinputs = -y_true/dvalues
    #normalize gradient
    dinputs = dinputs/samples
    list("dinputs" = dinputs)
  }
)


## Regularization ##
regularization_loss = function(layer){
  #Regularization hyperparams:
  layer_reg = layer$regularization ##a list of the set lambda values
  
  #L1-regularization: weights
  l1_weight_apply = layer_reg$weight_L1 * sum(abs(layer$weights))
  #L1-regularization: bias
  l1_bias_apply = layer_reg$bias_L1 * sum(abs(layer$biases))
  
  #L2-regularization: weights
  l2_weight_apply = layer_reg$weight_L2 * sum(layer$weights^2)
  #L2-regularization: biases
  l2_bias_apply = layer_reg$bias_L2 * sum(layer$biases^2)
  
  #Overall regularization loss
  reg_loss = l1_weight_apply + l1_bias_apply + l2_weight_apply + l2_bias_apply
  #save:
  return(reg_loss)
}

### Optimizers----------------------------------------------------------------
## Stochastic Gradient Descent (vanilla + decay & momentum options) ##
optimizer_SGD = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 1,
                           lr_decay = 0, iteration = 1,
                           momentum_rate = 0 
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)
    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #param updates with momentum
     #If momentum == TRUE in parameter initialization, then weights_momentum
     #(and implictly, bias_momentum) will exist
    if (exists("weight_momentums", where = layer_forward)) {
      #current momentums
      weight_momentums = layer_forward$weight_momentums
      bias_momentums = layer_forward$bias_momentums
      
      #Update weights & biases with momentum:
      #Take prior updates X retainment factor (the "momentum rate"),
      #and update with current gradients
      weight_update = 
        (momentum_rate*weight_momentums) - (currnt_learn_rate*weight_gradients)
      bias_update = 
        (momentum_rate*bias_momentums) - (currnt_learn_rate*bias_gradients)
      #update params with the calculated updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
      #also update momentums
      weight_momentums = weight_update
      bias_momentums = bias_update
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
           "weight_momentums" = weight_momentums, 
           "bias_momentums" = bias_momentums,
           "n_neurons" = n_neurons)
    } else {
      
    #param updates without momentum (vanilla)
      #calculate updates
      weight_update = -currnt_learn_rate*weight_gradients
      bias_update = -currnt_learn_rate*bias_gradients
      #apply updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
           "n_neurons" = n_neurons)
    }
    
})
## AdaGrad ##
#NOTE: must initialize params with cache==TRUE
optimizer_AdaGrad = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 1,
                           lr_decay = 0, iteration = 1,
                           epsilon = 1e-7 
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)

    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #cache
    ##update cache with squared current gradients
    weight_cache = layer_forward$weight_cache + weight_gradients^2
    bias_cache = layer_forward$bias_cache + bias_gradients^2
    
    #SGD param updates with normalization 
      #calculate updates
      weight_update = -currnt_learn_rate*weight_gradients /
                        (sqrt(weight_cache) + epsilon)
      bias_update = -currnt_learn_rate*bias_gradients /
                        (sqrt(bias_cache) + epsilon)
      #apply updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
        #save:
        list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
             "weight_cache" = weight_cache, "bias_cache" = bias_cache,
             "n_neurons" = n_neurons)
} ##these are the new params to be passed on to the layers when loop restarts
)

## RMSProp ##
#NOTE: must initialize params with cache==TRUE
optimizer_RMSProp = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 0.001,
                           lr_decay = 0, iteration = 1,
                           epsilon = 1e-7,
                           rho = 0.9
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)

    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #cache
    ##update cache with squared current gradients
    weight_cache = rho * layer_forward$weight_cache + 
                  (1-rho) * weight_gradients^2
    bias_cache = rho * layer_forward$bias_cache + 
                  (1-rho) * bias_gradients^2
    
    #SGD param updates with normalization 
      #calculate updates
      weight_update = -currnt_learn_rate*weight_gradients /
                        (sqrt(weight_cache) + epsilon)
      bias_update = -currnt_learn_rate*bias_gradients /
                        (sqrt(bias_cache) + epsilon)
      #apply updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
        #save:
        list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
             "weight_cache" = weight_cache, "bias_cache" = bias_cache,
             "n_neurons" = n_neurons)
} ##these are the new params to be passed on to layers when the loop restarts
)

## Adam ##
#NOTE: must intialize paramters with both cache AND momentum
optimizer_Adam = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 0.001,
                           lr_decay = 0, iteration = 1,
                           epsilon = 1e-7,
                           beta_1 = 0.9, beta_2 = 0.999
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)

    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #momentums
    #update momentums with current gradients and bias correct
    weight_momentums = 
      (beta_1*layer_forward$weight_momentums + (1-beta_1)*weight_gradients ) /
                              (1 - (beta_1^iteration )) ##bias correction
    
    bias_momentums = 
      (beta_1*layer_forward$bias_momentums + (1-beta_1)*bias_gradients) /
                              (1 - (beta_1^iteration))
                    

    #cache
    #update cache with squared gradients and bias correct
    weight_cache = 
      (beta_2*layer_forward$weight_cache + (1-beta_2)*weight_gradients^2) /
                              (1 - (beta_2^iteration)) ##bias correction

    bias_cache = 
      (beta_2*layer_forward$bias_cache + (1-beta_2)*bias_gradients^2) /
                              (1 - (beta_2^iteration))
    
    #calculate param updates (with momentums, and normalize with cache)
    weight_update = -currnt_learn_rate*weight_momentums /
                        (sqrt(weight_cache) + epsilon)
    bias_update = -currnt_learn_rate*bias_momentums /
                        (sqrt(bias_cache) + epsilon)
    
    #apply updates
    weights = current_weights + weight_update
    biases = current_biases + bias_update
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
           "weight_momentums" = weight_momentums, 
           "bias_momentums" = bias_momentums,
           "weight_cache" = weight_cache,
           "bias_cache" = bias_cache,
           "n_neurons" = n_neurons)

}##these are the updated params to be passed back into the layers
)

##function prints reminders for settings needed for each optimizer
optimizer_REMINDME = function(
                    optimizer = "sgd, adagrad, rmsprop, adam, or general"){
  if(optimizer == "sgd"){
    message("
Momentum is optional (set TRUE or FALSE), cache is not available (set FALSE).
If you choose to set 'momentum_rate' in SGD arguments to use momentum and control the exponential decay, set momentum = TRUE in parameter initialization function.")
  }
  if(optimizer == "adagrad"){
    message("
Initialize parameters with cache=TRUE, momentum=FALSE.")
  }
  if(optimizer == "rmsprop"){
    message("
Initialize parameters with cache=TRUE, momentum=FALSE. 
Also set the 'rho' hyperparameter - the cache memory decay rate.")
  }
  if(optimizer == "adam"){
    message("
Initialize parameters with cache=TRUE & momentum=TRUE. 
Also set the 'beta_1' and 'beta_2' hyperparameters, the exponential decay rates for the momentums and cache respectively.")
  }
  if(optimizer == "general"){
    message("
'learning_rate' (also known as alpha) determines the proportion of the gradient values used to update the weights & biases. This controls the speed the model learns. 
'lr_decay' controls the rate at which that proportion decays (gets smaller) over the course of the epochs. 

'epsilon' is a small value meant to prevent any division by zero.

'momentum' is an exponentially decaying moving average of past gradients applied in the update equation so that current updates are informed by past updates. 

'cache', or an adaptive learning rate, keeps a history of previous parameter updates to normalize them, effectively reducing the learning rate for parameters receiving large gradients and increasing the learning rate for those receiving small or infrequent updates." )
  }
  
}



```

Here I'm implementing a basic function to compute some classification metrics. I won't get into the details, but it essentially just makes a confusion matrix, derives the true positives, false positives, true negatives, and false negatives, and uses them to calculate various metrics.  
[Here's a good introduction to confusion matrices and classification metrics](https://raw.githack.com/edrubin/EC524W22/master/lecture/006/006-slides.html#73).  

_Problem_: If the model doesn't predict every class at least once, table wont 
produce the right kind of confusion matrix (it will drop off the class like below). Need to find a fix for that.  
    true
pred  0  1
   1 50 50
```{r}
#Classess: Assess Classification Performance ----
classess = function(truths, predictions){
  y_true = as.vector(truths)
  y_hat = as.vector(predictions)

#Binary Confusion Matrix
  if (length(unique(y_true)) <= 2){
    #Note: positive class will be whichever class is assigned to label 1
    #Confusion Matrix
    ##Need to initialize a matrix of the correct length with zeros due to 
    ##possibility of predictions not containing every possible class
    if (min(y_true) == 0){ ##if classes are encoded as 0 and 1
      y_true = y_true + 1 ##cant index using 0 and 1
      y_hat = y_hat + 1
    }
    conf_mat = table(y_hat, y_true) #tabulate the predictions and truths
    diff = setdiff(y_true, y_hat) ##figure out which if any class not predicted
    #fill out table if missing class, but order depends on which class
    for (class in diff){
      newrow = array(0, dim = length(unique(y_true)))  
      conf_mat = rbind(conf_mat,newrow)
      conf_mat = conf_mat[order(c(1:(nrow(conf_mat)-1),class-0.5)),]
    }

    #Format Confusion Matrix
    dimnames(conf_mat) = list("pred"=c(sort(unique(y_true)-1)),
                              "true"=c(sort(unique(y_true)-1))
                              )
    tp = conf_mat[1,1]
    fp = conf_mat[1,2]
    tn = conf_mat[2,2]
    fn = conf_mat[2,1]
    total = tp + fp + tn + fn
    
  #Calculate metrics  
  m = data.frame(
    accuracy = (tp + tn) / total, #share of correctly predicted labels
    sensitivity = tp / (tp + fn), #share of true pos we predict correct
    recall = tp / (tp + fn), #recall = sensitivity
    specificity = tn / (tn + fp), #share of true no's we predicted correct
    precision = tp / (tp + fp) #share of predicted pos that are correct
  )
  m = cbind(m,
    #F1 Score: the harmonic mean of precision and recall
    f1_score = 2 * (m$precision * m$recall) / (m$precision + m$recall)
                  )
  overall_accuracy = sum(y_hat == y_true) / length(y_true)
    #return
    list(conf_mat = conf_mat, metrics = m,
         overall_accuracy = overall_accuracy)
    
#Multiclass Confusion Matrix
  } else if (length(unique(y_true)) > 2){
      # conf_mat = matrix(0, 
      #        nrow = length(unique(y_true)), ncol = length(unique(y_true)))
    conf_mat = table(y_hat, y_true)
    
    diff = setdiff(y_true, y_hat)
    for (class in diff){
      newrow = array(0, dim = length(unique(y_true)))  
      conf_mat = rbind(conf_mat,newrow)
      conf_mat = conf_mat[order(c(1:(nrow(conf_mat)-1),class-0.5)),]
    }
    #Idea source: https://stackoverflow.com/questions/11561856/add-new-row-to-dataframe-at-specific-row-index-not-appended.
    
    #Format Confusion Matrix
    dimnames(conf_mat) = list("pred"=c(sort(unique(y_true))),
                              "true"=c(sort(unique(y_true)))
                              )
    
    #Find TP, TN, FP and FN for each individual class
    #Add metrics to a dataframe
    metrics = data.frame(class = 1:length(unique(y_true))) 
        #if a class is not in y_hat, it will produce NA. good for user to see
    for (class in unique(y_hat)){##iterate through classes actually predicted
      tp = conf_mat[class, class]
      fp = sum(conf_mat[class, ]) - tp
      tn = sum(conf_mat[-class, -class])
      fn = sum(conf_mat[class, ]) - tp   
      total = tp + fp + tn + fn
    
      accuracy = (tp + tn) / total
        metrics[class, "accuracy"] = accuracy
      sensitivity = tp / (tp + fn)
        metrics[class, "sensitivity"] = sensitivity
      recall = sensitivity
        metrics[class, "recall"] = recall
      specificity = tn / (tn + fp)
        metrics[class, "specificity"] = specificity
      precision = tp / (tp + fp)
        metrics[class, "precision"] = precision
      
      f1_score = 2 * (precision * recall) / (precision + recall)
        metrics[class, "f1_score"] = f1_score
    }
    overall_accuracy = sum(y_hat == y_true) / length(y_true)
    #return
    list(conf_mat = conf_mat, metrics = metrics,
         overall_accuracy=overall_accuracy)
  }
}

```




And here is our function to simulate the spiral data. 
```{r sim-data-fn}
## Creating Spiral Data ----
#Source: https://cs231n.github.io/neural-networks-case-study/
sim_data_fn = function(
  N = 100, # number of points per class
  D = 2,   # "dimensionality" (number of features)
  K = 3,   # number of classes
  random_order = FALSE #T: random order of classes (harder task)
){
   X = data.frame() # data matrix (each row = single sample)
   y = data.frame() # class labels

for (j in (1:K)){
  r = seq(0.05,1,length.out = N) # radius
  t = seq((j-1)*4.7,j*4.7, length.out = N) + rnorm(N, sd = 0.3) # theta
  Xtemp = data.frame(x =r*sin(t) , y = r*cos(t)) 
  ytemp = data.frame(matrix(j, N, 1))
  X = rbind(X, Xtemp)
  y = rbind(y, ytemp)
  }
spiral_data = cbind(X,y)
colnames(spiral_data) = c(colnames(X), 'label')
spiral_data$label = spiral_data$label

# Want randomly ordered labels?
if (random_order==TRUE) {spiral_data$label = sample(1:(K), size = N*K, 
                                                    replace = TRUE)}
return(spiral_data)
}
```

```{r}
## Create sine-wave Data ----
sine_data_sim = function(samples, multiple){
  x  = seq(0, multiple*pi, length.out = samples) 
  sine_data = data.frame(x = x/max(x), ##normalized
                           y = sin(x))
  ##shuffle up the samples
  sine_data = sine_data[sample(1:nrow(sine_data)), ]
  return(sine_data)
}
```


# A Model Object
Now we add batching into the training workflow. This requires just one additional argument (batch_size) but I'm going to have to tweak a lot of the train_model function.  

```{r build-model}
## Build Model----
initialize_model = function(){
    # Create an empty list for network objects
    model = list()
    return(model)
}
add_dense = function(model, n_inputs, n_neurons, dropout_rate = 0,
                       weight_L1=0, weight_L2=0, bias_L1=0, bias_L2=0){
    pos = length(model) + 1 ##the layer's position in sequence of layers
    
    dense_args = list(n_inputs=n_inputs, n_neurons=n_neurons,
                        dropout_rate = dropout_rate,
                        weight_L1=weight_L1, weight_L2=weight_L2,
                        bias_L1=bias_L1, bias_L2=bias_L2,
                        class = "layer_dense", pos = pos)

    # Add layer objects to the model
    model = c(model, "layer" = list(dense_args))
    # Give the layer a unique name corresponding to position in sequence
    names(model)[[pos]] = paste0("layer", pos)
    #names(model)[['new']] = layer_name ##rename the layer object appropriately
    return(model)
    
}
add_activation =  function(model, activ_fn = c("linear", "relu", "sigmoid",
                                   "softmax_crossentropy")){
    class = activ_fn
    pos = length(model) + 1
    ##We want to tie the activation function back to the dense layer 
    ##its applied upon. This depends on if dropout was used in between
    base_layer = pos-1
    activ_args = list(class=class, pos=pos, base_layer=base_layer)
    model = c(model, "layer" = list(activ_args))
    names(model)[[pos]] = paste0("layer",pos)
    
    return(model)
}
add_loss = function(model, 
                      loss_fn=c("mse", "mae",
                                "binary_crossentropy",
                                "categorical_crossentropy")){
    
    pos = length(model) + 1
    loss_args = list(class=loss_fn, pos=pos)
    model = c(model, "layer" = list(loss_args))
    names(model)[[pos]] = paste0("layer",pos)
    
    return(model)
}
  # add_optimizer = function(model, optimizer = c("adam"),
  #                          learning_rate, lr_decay, epsilon){
  #   pos = length(model) + 1
  #   opt_args = list(class=optimizer, learning_rate=learning_rate,
  #                   lr_decay=lr_decay, epsilon=epsilon)
  #   model = c(model, "layer" = list(opt_args))
  #   names(model)[[pos]] = paste0("layer",pos)
  # }

```


```{r train_model}
## train_model----
train_model = function(model, inputs, y_true, epochs,
                       optimizer = c("adam"), 
                       learning_rate=0, lr_decay=0, epsilon=1e-7,...,
                       metric_list=NULL, 
                       validation_X=NULL, validation_y=NULL,
                       batch_size=NULL,
                       epoch_print=100, batch_print=0){
  
  optim_args = list(...)
  if (!is.null(metric_list)){
      ##set metrics list to store metrics
      metrics = data.frame(epoch = 1:epochs) 
  }
  
# Setup ----
  ##How many unique "base layers" (dense layers)  
  baselayers = c()
  for (i in seq_along(model)){
    base_i = model[[i]]$base_layer
    baselayers = c(baselayers, base_i)
  }
  baselayers = unique(baselayers) ##return vector of each baselayer position
  
  # Set up the layer parameters
  layer_parameters = list()
  for (i in seq_along(baselayers)){
    ##Initialize empty list
    layer_parameters = c(layer_parameters, "layer"=list(NULL))
    names(layer_parameters)[[i]] = paste0("layer",baselayers[i])
  }
  ##the layer params list will be the length of unique baselayers and will
  ##be overwritten with the updated params at the end of the training loop
  
  ## Derive parameter settings from the selected optimizer
  if (optimizer=="adam"){
    ##requires cache and momentum
    cache_set=TRUE
    momentum_set=TRUE
    ##set default betas if not provided by user in `...`
    if (!exists(x = "beta_1", where = optim_args)){
      warning("Adam utilizes beta_1 and beta_2 hyperparameters which were not provided, and have been set to defaults of 0.9 and 0.999 respectively. Include specific 'beta_1' and 'beta_2' values in the arguments if desired.")
      optim_args$beta_1 = 0.9
      optim_args$beta_2 = 0.999
    }
    
  } else if (optimizer=="sgd"){
    ##momentum is optional, no cache
    if (exists(x = "momentum_rate", where = optim_args)){
      momentum_set=TRUE
      cache_set=FALSE
    } else {
      momentum_set=FALSE
      cache_set=FALSE
      optim_args$momentum_rate = 0
    }
  } else if (optimizer=="adagrad"){
    ##requires cache, no momentum
    cache_set=TRUE
    momentum_set=FALSE
  } else if (optimizer=="rmsprop"){
    ##requires cache, no momentum
    cache_set=TRUE
    momentum_set=FALSE
    ##requires rho hyperparameter
    if (!exists(x = "rho", where = optim_args)){
      warning("RMSProp utilizes the rho hyperparameter which was not provided, and has been set to a default of 0.9. Include a specific 'rho' value in the arguments if desired.")
      optim_args$rho = 0.9
    }
    
  } else {
    warning("Please use one of the following strings to select an optimizer: 'sgd', 'adagrad', 'rmsprop', 'adam'. Otherwise, the network will not perform as intended.")
  }
  
  # Initialize random params for every layer
  for (b in baselayers){
    current_layer = paste0("layer", b)
    n_inputs = model[[current_layer]]$n_inputs
    
    random_params = init_params(n_inputs = model[[current_layer]]$n_inputs,
                                n_neurons = model[[current_layer]]$n_neurons,
                                momentum = momentum_set, 
                                cache = cache_set)

    layer_parameters[[current_layer]] = random_params
    #names(layer_parameters)[[length(layer_parameters)]] = paste0("layer",b)
  }
  
  # Batch size
  if (!is.null(batch_size)){
    ##determine how many training "steps" will be needed 
    train_steps = nrow(inputs) %/% batch_size	 #(integer division)
    
    ##Since integer division rounds down, there may be some remaining samples
    ##not in a full batch. So we add 1 to include this last mini-batch
    if (train_steps*batch_size < nrow(inputs)){
      train_steps = train_steps + 1
    }
    # Now we repeat this process for the validation data
    if (!is.null(validation_X)){
      validation_steps = nrow(validation_X) %/% batch_size
      if(validation_steps*batch_size < nrow(validation_X)){
        validation_steps = validation_steps + 1
      }
    }
  } else { ##if no batch_size given, train in one batch
    batch_size = nrow(inputs)
    train_steps = 1
    validation_steps = 1
  }
  
# Training Loop ----  
for (epoch in 1:epochs){
  
 step_loss = 0      ##we will accumulate these over the steps
 step_reg_loss = 0  
 accumulated_count = 0
   
 ##set metrics list to store batch metrics (reset each epoch) 
 batch_metrics = data.frame(step = 1:train_steps) 

 for(step in 1:train_steps){ ##batching
   
  #Deal with batches:
   ##if no batch_size given, then train using one step and full dataset
   if (train_steps == 1){
     batch_X = inputs
     batch_y = y_true
     
   ##otherwise slice a batch
   } else { 
     if (step != train_steps){
       batch_X = inputs[((step-1)*batch_size +1):(step*batch_size), ]
       batch_y = y_true[((step-1)*batch_size +1):(step*batch_size) ]
     } else {
       ##for the last step, need to index precisely to the last row
       batch_X = inputs[((step-1)*batch_size +1):(nrow(inputs)), ]
       batch_y = y_true[((step-1)*batch_size +1):(nrow(inputs)) ]
     }
     ## +1 keeps the process from reusing the sample at the end of last batch
   }
    
  # Forward Pass----
  layers = list() ##container to store layer objects, resets every step/epoch
  ##First layer
  dense1 = layer_dense$forward(inputs = batch_X,
                              parameters = layer_parameters[[1]], ##the randoms
                              weight_L1 = model[[1]]$weight_L1,
                              weight_L2 = model[[1]]$weight_L2,
                              bias_L1 = model[[1]]$bias_L1,
                              bias_L2 = model[[1]]$bias_L2
                              )
  dropout1 = layer_dropout$forward(input_layer = dense1,
                                  dropout_rate = model[[1]]$dropout_rate)
  
  ###determine activation function  
  activ_fn1 = model[[2]]$class ##second layer is the first activation layer
  
  if (activ_fn1=="linear"){
    activ1 = activation_Linear$forward(input_layer = dropout1)
  } else if (activ_fn1=="relu"){
    activ1 = activation_ReLU$forward(input_layer = dropout1)
  } else if (activ_fn1=="sigmoid"){
    activ1 = activation_Sigmoid$forward(input_layer = dropout1)
  } else if (activ_fn1=="softmax"){
    warning("Pure Softmax is broken Hans")
    activ1 = activation_Softmax$forward(inputs = dropout1$output)
  } else if (activ_fn1=="softmax_crossentropy"){
    activ1 = activation_loss_SoftmaxCrossEntropy$forward(
              input_layer = dropout1,
              y_true = y_true)
  } else message("Please enter one of the following strings in $add_activation to select an activation function: 'linear', 'relu', 'sigmoid', 'softmax_crossentropy'")
  
  output1 = activ1
  
  layer1 = list(dense=dense1,dropout=dropout1, activ=activ1, output=output1)
  layers = c(layers, "layer1" = list(layer1))

if (length(baselayers) > 1){
  ## Next Layers
  for (b in baselayers[-1]){##for all other base layers (- first element/layer)
    
    ##index of prior baselayer (the baselayer that comes next in seq, hence +1)
    prior_baselayer = baselayers[which(baselayers==b)-1] 
    ##which will serve as the input layer
    input_layer = paste0("layer",prior_baselayer)
    current_layer = paste0("layer",b)

    #prior_baselayer = baselayers[b] ##index of prior baselayer (includ. 1st)
    #input_layer = paste0("layer",prior_baselayer)
    
    dense_b = layer_dense$forward(inputs = layers[[input_layer]]$output$output,
                                parameters = layer_parameters[[current_layer]],
                                weight_L1 = model[[current_layer]]$weight_L1,
                                weight_L2 = model[[current_layer]]$weight_L2,
                                bias_L1 = model[[current_layer]]$bias_L1,
                                bias_L2 = model[[current_layer]]$bias_L2
                                )
    dropout_b = layer_dropout$forward(input_layer = dense_b,
                            dropout_rate = model[[current_layer]]$dropout_rate)
    
    activ_fn = model[[b+1]]$class
    ##current layer+1 since activation follows dense.
    
    if (activ_fn=="linear"){
      activ_b = activation_Linear$forward(input_layer = dropout_b)
    } else if (activ_fn=="relu"){
      activ_b = activation_ReLU$forward(input_layer = dropout_b)
    } else if (activ_fn=="sigmoid"){
      activ_b = activation_Sigmoid$forward(input_layer = dropout_b)
    } else if (activ_fn=="softmax"){
      warning("Pure Softmax is broken Hans")
      activ_b = activation_Softmax$forward(inputs = dropout_b$output)
    } else if (activ_fn=="softmax_crossentropy"){
      activ_b = activation_loss_SoftmaxCrossEntropy$forward(
                input_layer = dropout_b,
                y_true = batch_y)
    } else message("Please enter one of the following strings in $add_activation to select an activation function: 'linear', 'relu', 'sigmoid', 'softmax_crossentropy'")
  
    output_b = activ_b
    
    layer = list(dense=dense_b,dropout=dropout_b,activ=activ_b,output=output_b)
    layers = c(layers, "layer" = list(layer))
    names(layers)[[length(layers)]] = paste0("layer",b)
  }#end loop
}  
  
  # Calculate Loss ----
  ##Determine the selected loss function
  loss_fn = model[[length(model)]]$class ##last layer should be a loss layer
  
  if (loss_fn=="mse"){
    loss_layer = loss_MeanSquaredError$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = batch_y
      )
  } else if (loss_fn=="mae"){
    loss_layer = loss_MeanAbsoluteError$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = batch_y
    )
  } else if (loss_fn=="binary_crossentropy"){
    loss_layer = loss_BinaryCrossentropy$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = batch_y
    )
  } else if (loss_fn=="categorical_crossentropy"){
    loss_layer = layers[[length(layers)]]$output
  }
  
  # Calculate Regularized Loss
  ##If regularization is not used, this code will still run but reg_loss
  ##will equal zero.
  layers_reg_loss = c()
  for (b in baselayers){
    current_layer = paste0("layer",b)
    reg_loss_b = regularization_loss(layers[[current_layer]]$dense)
    layers_reg_loss = c(layers_reg_loss, reg_loss_b)
  }
  reg_loss = sum(layers_reg_loss)
  step_reg_loss = step_reg_loss + reg_loss
  
  # Calculate any other metrics ----
  ##Determine which metrics to calculate, then calculate and save
  for (metric in metric_list){
    
    if (metric=="mse"){
      mse = loss_MeanSquaredError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = batch_y
      )
      metric_i = (mse$sample_losses)
    } else if (metric=="mae"){
      mae = loss_MeanAbsoluteError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = batch_y
      )
      metric_i = (mae$sample_losses)
    } else if (metric=="accuracy"){ ##depends on if binary or cateogircal task
      if (loss_fn=="binary_crossentropy"){
        pred = (layers[[length(layers)]]$output$output > 0.5) * 1 
                  ##returns TRUE (1) if true, and FALSE (0) if false
      } else if (loss_fn=="categorical_crossentropy"){
        pred = max.col(layers[[length(layers)]]$output$output,
                                 ties.method = "random")
      }
      accuracy = (pred==batch_y)
      metric_i = accuracy
    } else if (metric=="regression_accuracy"){
      reg_pred = layers[[length(layers)]]$output$output
      accuracy_precision = sd(batch_y)/max(batch_y)
      ##count number of "accurate" preictions
      reg_accuracy = (abs(reg_pred - batch_y) < accuracy_precision)
      metric_i = reg_accuracy
    }
    
    ##add metric to the batch_metrics dataframe (unless not tracking metrics)
    ##accumulate the sums of the sample losses and then calculate epoch-wise
    ##average at the end
    if (!is.null(metric_list)){
      batch_metrics[step, "count"] = length(metric_i)
      batch_metrics[step, paste0(metric)] = sum(metric_i, na.rm = T)
    }
  }#end metrics loop
  
  # Backward Pass ----
  layers_back = list()
  
  ## The first backprop layer (the last layer in the training sequence)
  last_layer = paste0("layer",baselayers[length(baselayers)])
  ###First backprop the loss function
  ###Determine which loss function was chosen and backprop it
  loss_fn_last = model[[length(model)]]$class
  
  if (loss_fn_last=="mse"){
    loss_backprop1 = loss_MeanSquaredError$backward(
                      dvalues = layers[[length(layers)]]$output$output,
                      loss_layer = loss_layer
    )
  } else if (loss_fn_last=="mae"){
    loss_backprop1 = loss_MeanAbsoluteError$backward(
                      dvalues = layers[[length(layers)]]$output$output,
                      loss_layer = loss_layer
    )
  } else if (loss_fn_last=="binary_crossentropy"){
    loss_backprop1 = loss_BinaryCrossentropy$backward(
                      dvalues = layers[[length(layers)]]$output$output,
                      y_true = batch_y
    )
  } else if (loss_fn_last=="categorical_crossentropy"){
    loss_backprop1 = activation_loss_SoftmaxCrossEntropy$backward(
                      dvalues = layers[[length(layers)]]$output$output,
                      y_true = batch_y
    )
  } else warning("Please select an available loss function when building the model.")  
  
  ###Then backprop the activation function
  activ_fn_last = model[[length(model)-1]]$class ##last layer before loss layer
  
  if (activ_fn_last=="linear"){
    activ_backprop1 = activation_Linear$backward(d_layer = loss_backprop1)
  } else if (activ_fn_last=="relu"){
    activ_backprop1 = activation_ReLU$backward(d_layer = loss_backprop1,
                                      layer = layers[[last_layer]]$activ)
  } else if (activ_fn_last=="sigmoid"){
    activ_backprop1 = activation_Sigmoid$backward(d_layer = loss_backprop1,
                                      layer = layers[[last_layer]]$activ)
  } else if (activ_fn_last=="softmax"){
    warning("pure softmax is incomplete")
  } else if (activ_fn_last=="softmax_crossentropy"){
    activ_backprop1 = loss_backprop1 ##due to combined softmax/crossent fn
  }
  ###Then backprop the dropout layer, IF dropout was used
  if (model[[baselayers[length(baselayers)]]]$dropout_rate == 0){
    #dropout1 = FALSE 
    d_layer_todense1=activ_backprop1
  } else {
    #dropout1 = TRUE
    dropout_backprop1 = layer_dropout$backward(
                        d_layer = activ_backprop1, 
                        layer_dropout = layers[[length(layers)]]$dropout
      )
    d_layer_todense1=dropout_backprop1
  }

  ###Finally, backprop the dense layer
  ###Note: the layer_dense backward fn auto-adjusts if regularization is used
  dense_backprop1 = layer_dense$backward(d_layer = d_layer_todense1,
                                        layer = layers[[length(layers)]]$dense)
  
  layer_b = list(dense_backprop=dense_backprop1)
  layers_back = c(layers_back, "layer" = list(layer_b))
  names(layers_back)[[length(layers_back)]] = ##this is the last baselayer
                          paste0("layer",baselayers[length(baselayers)])
  
  ## Next Layers
  for (b in rev(baselayers[-length(baselayers)])){
    ##for all layers but the last (rev order since moving back through layers)
    
    ##index of prior baselayer (the baselayer that comes next in seq, hence +1)
    prior_baselayer = baselayers[which(baselayers==b)+1] 
    ##which will serve as the input layer
    input_layer = paste0("layer",prior_baselayer)

    current_layer = paste0("layer", b)
    
    activ_fn = model[[b+1]]$class
    if (activ_fn=="linear"){
      activ_backprop = activation_Linear$backward(
        d_layer = layers_back[[input_layer]]$dense_backprop)
    } else if (activ_fn=="relu"){
      activ_backprop = activation_ReLU$backward(
        d_layer = layers_back[[input_layer]]$dense_backprop,
        layer = layers[[current_layer]]$activ)
    } else if (activ_fn=="sigmoid"){
      activation_Sigmoid$backward(
        d_layer = layers_back[[input_layer]]$dense_backprop,
        layer = layers[[current_layer]]$activ)
    } #else if "softmax" once pure softmax is fixed
    
    if (model[[b]]$dropout_rate == 0){
      #dropout_rt = FALSE 
      d_layer_todense=activ_backprop
    } else {
      #dropout_rt = TRUE
      dropout_backprop = layer_dropout$backward(
                       d_layer = activ_backprop, 
                       layer_dropout = layers[[current_layer]]$dropout
      )
      d_layer_todense=dropout_backprop
    }

    dense_backprop = layer_dense$backward(d_layer = d_layer_todense,
                                      layer = layers[[current_layer]]$dense)
    layer_b = list(dense_backprop=dense_backprop)
    layers_back = c(layers_back, "layer" = list(layer_b))
    names(layers_back)[[length(layers_back)]] = paste0("layer",b)
  }
  
  # Optimize the Parameters ----
  for (b in baselayers){
    current_layer = paste0("layer", b)
    
    if (optimizer=="sgd"){
      optimal_params_b = optimizer_SGD$update_params(
        layer_forward = layers[[current_layer]]$dense,
        layer_backward = layers_back[[current_layer]]$dense_backprop,
        learning_rate = learning_rate,
        lr_decay = lr_decay, iteration = epoch,
        momentum_rate = optim_args$momentum_rate
      )
    } else if (optimizer=="adagrad"){
      optimal_params_b = optimizer_AdaGrad$update_params(
        layer_forward = layers[[current_layer]]$dense,
        layer_backward = layers_back[[current_layer]]$dense_backprop,
        learning_rate = learning_rate,
        lr_decay = lr_decay, iteration = epoch,
        epsilon = epsilon
      )
    } else if (optimizer=="rmsprop"){
      optimal_params_b = optimizer_RMSProp$update_params(
        layer_forward = layers[[current_layer]]$dense,
        layer_backward = layers_back[[current_layer]]$dense_backprop,
        learning_rate = learning_rate,
        lr_decay = lr_decay, iteration = epoch,
        epsilon = epsilon, 
        rho = optim_args$rho
      )
    } else if (optimizer=="adam"){
      optimal_params_b = optimizer_Adam$update_params(
        layer_forward = layers[[current_layer]]$dense,
        layer_backward = layers_back[[current_layer]]$dense_backprop,
        learning_rate = learning_rate,
        lr_decay = lr_decay, iteration = epoch,
        epsilon = epsilon,
        beta_1 = optim_args$beta_1,
        beta_2 = optim_args$beta_2
      )
    }
                      
    layer_parameters[[current_layer]] = optimal_params_b
  }
  
  ## Calculate loss for the batch per sample
  ## accumulate over the batches and we will use the average for the epoch
  step_loss = step_loss + sum(loss_layer$sample_losses)
      ##each step, we sum up the loss for all of the samples. we accumulate
      ##this sum over all the steps. thus, to get the average for the epoch
      ##we will divide this accumulated sum by the total # of sample_losses,
      ##which will be equivalent to the total number of inputs
      accumulated_count = accumulated_count + length(loss_layer$sample_losses)
      ##this should be about equal but may differ due to overlapping
 
      
  ##Batch-wise Status Report
if (batch_print > 0){
  if (step %in% seq(0, train_steps, by = batch_print)){
      report_head = c("batch_step", "loss", metric_list)
      report = c(step, mean(loss_layer$sample_losses))
      for (metric in metric_list){
        value = batch_metrics[step, paste0(metric)] /
                              batch_metrics[step, "count"]
        report = c(report, value)
      }
      
      report = sapply(report, round, 7)
      print(paste(report_head, collapse = " | "), quote = FALSE)
      print(paste(report, collapse = " | "), digits = 5, quote = FALSE)
  }
}
      
 }##end batch steps loop

 # Calculate loss for the epoch
 ##First calculate the average sample loss
 avg_sample_loss = step_loss / accumulated_count
 avg_reg_loss = step_reg_loss / train_steps ##will = 0 if no regularization
   
 data_loss = avg_sample_loss
 loss = avg_sample_loss + avg_reg_loss
 
 # Calculate the epoch-wise metrics     
 for (metric in metric_list){
   ##sum up the losses/accuracies across the batches
   ##(each column in batch_metrics is a metric, each row a batch step)
   ##divide by the total number of inputs to get the average
   epoch_metric = sum(batch_metrics[[paste0(metric)]]) / 
                      sum(batch_metrics[["count"]])
   ##send to metrics df
   metrics[epoch, paste0(metric)] = epoch_metric
   
 } 


# Validation Steps ----
# (If batching the inputs, probably want to batch the validation data too)
# (If validation data is small, this will all happen in one step anyways)
if(!is.null(validation_X)){
 batch_valid_loss = c()
 
 for (step in validation_steps){
   if (validation_steps == 1){
     valid_batch_X = validation_X
     valid_batch_y = validation_y
     
   ##otherwise slice a batch
   } else { 
     if (step != validation_steps){
       valid_batch_X = 
                  validation_X[((step-1)*batch_size +1):(step*batch_size), ]
       valid_batch_y = 
                  validation_y[((step-1)*batch_size +1):(step*batch_size) ]
     } else {
       ##for the last step, need to index precisely to the last row
       valid_batch_X = 
                  validation_X[((step-1)*batch_size +1):(nrow(validation_X)), ]
       valid_batch_y = 
                  validation_y[((step-1)*batch_size +1):(nrow(validation_X)) ]
     }
   }

  ###Utilize the test_model() fn to test on validation data
  current_fit = list("parameters" = layer_parameters)
  
  if (is.null(validation_y)){
    warning("provide both validation_X and validation_y")
    } else{
      validation = test_model(model = model,
                              trained_model = current_fit,
                              X_test = valid_batch_X,
                              y_test = valid_batch_y,
                              metric_list = metric_list)
    valid_loss_i = validation$loss
    ##includes regularization if used
    }
  batch_valid_loss = c(batch_valid_loss, valid_loss_i)
  
  } #end validation steps loop

 validation_loss = mean(batch_valid_loss) 

}
 
 # Status Report----
  #if (epoch == 1){
    report_head = c("epoch", "loss")
    if (!is.null(validation_X)) report_head = c(report_head, "validation_loss")
    report_head = c(report_head, metric_list)
    #print(report_head)
    #}
  if (epoch %in% seq(0,epochs,by=epoch_print)){
    report = c(epoch, loss)
    if (!is.null(validation_X)) report = c(report, validation_loss)
    for (metric in metric_list){
      value = metrics[epoch, paste0(metric)]
      report = c(report, value)
    }
   
   report = sapply(report, round, 7)
   message(paste(report_head, collapse = " | "))
   message(paste(report, collapse = " | ")) #, digits = 5, quote = FALSE)
  } 
  
    
  ## Send losses to the metrics dataframe for tracking
  metrics[epoch, "loss"] = loss
  metrics[epoch, "data_loss"] = data_loss
  if (!is.null(validation_X)){
    metrics[epoch, "validation_loss"] = validation_loss
  }
  ## Also send any validation metrics to the metrics dataframe
  for (metric in metric_list){
    value = validation$metrics[[paste0(metric)]]
    metrics[epoch, paste0("val_",metric)] = value
  }
  
}##end epoch loop
  
  
#Final Output----
  ##Save final model parameters
  parameters = list()
  for (b in baselayers){
    params_b = layer_parameters[[paste0("layer",b)]]
    parameters = c(parameters, "layer"=list(params_b))    
    names(parameters)[[length(parameters)]] = paste0("layer",b)
  }
  

  ##Final Predictions
  ##Need to do a forward pass of the data due to batching
  ##Might just have user use test_model() on training data to generate preds
  # last_fit = list("parameters" = parameters)
  # make_prediction = test_model(model = model,
  #                              trained_model = last_fit,
  #                              X_test = inputs,
  #                              y_test = y_true
  #                              ) 
  # raw_predictions = make_prediction$raw_predictions
  # predictions = make_prediction$predictions 
  
  ##Final Metrics
  final_metrics = list()
  final_metrics[["loss"]] = loss
  final_metrics[["data_loss"]] = data_loss
  if (!is.null(validation_X)){
    final_metrics[["validation_loss"]] = validation_loss
  }
  for (metric in metric_list){ ##from the final epoch
    final_metrics[[paste0(metric)]] = metrics[epochs, paste0(metric)]
      if (!is.null(validation_X)){
        final_metrics[[paste0("val_",metric)]] =
                          validation$metrics[[paste0(metric)]]
      }
  }

  ##Returns
  list(final_metrics=final_metrics,
       metrics=metrics,
       parameters=parameters)
}


```

```{r test_model}
# test_model()----
test_model = function(model, trained_model, 
                      X_test, y_test=NULL,
                      metric_list=NULL){
  inputs = X_test
  y_true = y_test
  ##How many baselayers were used
  baselayers = c()
  for (i in seq_along(model)){
    base_i = model[[i]]$base_layer
    baselayers = c(baselayers, base_i)
  }
  baselayers = unique(baselayers) ##return vector of each baselayer position

 # Forward Pass----
  layers = list()
  ##First layer
  dense1 = layer_dense$forward(inputs = inputs,
                parameters = trained_model$parameters[[1]],
                              weight_L1 = model[[1]]$weight_L1,
                              weight_L2 = model[[1]]$weight_L2,
                              bias_L1 = model[[1]]$bias_L1,
                              bias_L2 = model[[1]]$bias_L2
                              )
  ###determine activation function  
  activ_fn1 = model[[2]]$class ##second layer is the first activation layer
  
  if (activ_fn1=="linear"){
    activ1 = activation_Linear$forward(input_layer = dense1)
  } else if (activ_fn1=="relu"){
    activ1 = activation_ReLU$forward(input_layer = dense1)
  } else if (activ_fn1=="sigmoid"){
    activ1 = activation_Sigmoid$forward(input_layer = dense1)
  } else if (activ_fn1=="softmax"){
    warning("Pure Softmax is broken Hans")
    activ1 = activation_Softmax$forward(inputs = dense1$output)
  } else if (activ_fn1=="softmax_crossentropy"){
    activ1 = activation_loss_SoftmaxCrossEntropy$forward(
              input_layer = dense1,
              y_true = y_true)
  }
  output1 = activ1
  
  layer1 = list(dense=dense1, activ=activ1, output=output1)
  layers = c(layers, "layer1" = list(layer1))

if (length(baselayers) > 1){
  # Next Layers
  for (b in baselayers[-1]){##for all other base layers (- first element/layer)
    
    ##index of prior baselayer (the baselayer that comes next in seq, hence +1)
    prior_baselayer = baselayers[which(baselayers==b)-1] 
    ##which will serve as the input layer
    input_layer = paste0("layer",prior_baselayer)
    current_layer = paste0("layer",b)

    dense_b = layer_dense$forward(inputs = layers[[input_layer]]$output$output,
                      parameters = trained_model$parameters[[current_layer]],
                                weight_L1 = model[[current_layer]]$weight_L1,
                                weight_L2 = model[[current_layer]]$weight_L2,
                                bias_L1 = model[[current_layer]]$bias_L1,
                                bias_L2 = model[[current_layer]]$bias_L2
                                )

    activ_fn = model[[b+1]]$class
    ##current layer+1 since activation follows dense.
    
    if (activ_fn=="linear"){
      activ_b = activation_Linear$forward(input_layer = dense_b)
    } else if (activ_fn=="relu"){
      activ_b = activation_ReLU$forward(input_layer = dense_b)
    } else if (activ_fn=="sigmoid"){
      activ_b = activation_Sigmoid$forward(input_layer = dense_b)
    } else if (activ_fn=="softmax"){
      warning("Pure Softmax is broken Hans")
      activ_b = activation_Softmax$forward(inputs = dense_b$output)
    } else if (activ_fn=="softmax_crossentropy"){
      activ_b = activation_loss_SoftmaxCrossEntropy$forward(
                input_layer = dense_b,
                y_true = y_true)
    }
    output_b = activ_b
    
    layer = list(dense=dense_b,activ=activ_b,output=output_b)
    layers = c(layers, "layer" = list(layer))
    names(layers)[[length(layers)]] = paste0("layer",b)
  }#end loop
}  

# Calculate Loss Layer if y_true is given
  ##Determine the selected loss function
  loss_fn = model[[length(model)]]$class ##last layer should be a loss layer
  
if(!is.null(y_true)){  
  if (loss_fn=="mse"){
    loss_layer = loss_MeanSquaredError$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = y_true
      )
  } else if (loss_fn=="mae"){
    loss_layer = loss_MeanAbsoluteError$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = y_true
    )
  } else if (loss_fn=="binary_crossentropy"){
    loss_layer = loss_BinaryCrossentropy$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = y_true
    )
  } else if (loss_fn=="categorical_crossentropy"){
    loss_layer = layers[[length(layers)]]$output
  }

  # Calculate Regularized Loss
  ##If regularization is not used, this code will still run but reg_loss
  ##will equal zero.
  layers_reg_loss = c()
  for (b in baselayers){
    current_layer = paste0("layer",b)
    reg_loss_b = regularization_loss(layers[[current_layer]]$dense)
    layers_reg_loss = c(layers_reg_loss, reg_loss_b)
  }
  reg_loss = sum(layers_reg_loss)
  
  ## Calculate final loss----
  data_loss = mean(loss_layer$sample_losses)
  loss = data_loss + reg_loss ##loss = data_loss if regularization not used
  
  ## Calculate other metrics
  metrics = list()
  for (metric in metric_list){
    if (metric=="mse"){
      mse = loss_MeanSquaredError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = y_true
      )
      metric_i = mean(mse$sample_losses)
    } else if (metric=="mae"){
      mae = loss_MeanAbsoluteError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = y_true
      )
      metric_i = mean(mae$sample_losses)
    } else if (metric=="accuracy"){ ##depends on if binary or cateogircal task
      if (loss_fn=="binary_crossentropy"){
        pred = (layers[[length(layers)]]$output$output > 0.5) * 1 
                  ##returns TRUE (1) if true, and FALSE (0) if false
      } else if (loss_fn=="categorical_crossentropy"){
        pred = max.col(layers[[length(layers)]]$output$output,
                                 ties.method = "random")
      }
      accuracy = mean(pred==y_true, na.rm = T)
      metric_i = accuracy
    } else if (metric=="regression_accuracy"){
      reg_pred = layers[[length(layers)]]$output$output
      accuracy_precision = sd(y_true)/max(y_true)
      reg_accuracy = mean(abs(reg_pred - y_true) < accuracy_precision)
      metric_i = reg_accuracy
    }
    
    ##add metric to the metrics list (unless don't want to track them)
    metrics = c(metrics, "m" = list(metric_i))
    names(metrics)[[length(metrics)]] = paste0(metric)
  
  }#end metrics loop
} else { ##if y_true is not given, then no loss/metrics calculated
  loss = NULL
  data_loss = NULL
  metrics = NULL
}
  
  ## Final Predictions
  raw_predictions = layers[[length(layers)]]$output$output
  if (loss_fn=="categorical_crossentropy"){
    predictions = max.col(raw_predictions, ties.method = "random")
  } else if (loss_fn=="binary_crossentropy"){
    predictions = (raw_predictions > 0.5) * 1 
  }
  else {
    if (ncol(raw_predictions) > 1){
      ##if using multiple output neurons for a regression or binary problem...
      ##this is temporary, may want to change
      predictions = rowMeans(raw_predictions)
    } else predictions = raw_predictions
  }
 
  
  ##Output:
  if(!is.null(y_true)){  
    list(predictions=predictions, raw_predictions = raw_predictions,
         loss=loss, data_loss=data_loss,
        metrics=metrics)
  } else {
    list(predictions=predictions, raw_predictions = raw_predictions)
  }
  
}

```


Now we can run the same training loop as above with just a few lines of code.  

```{r}
##Create sine-wave datset
x  = seq(0, 2*pi, length.out = 100) 
sine_data = data.frame(x = x/max(x), ##normalized
                       y = sin(x))
sine_data = sine_data[sample(1:nrow(sine_data)), ] ##shuffle

X = as.matrix(sine_data$x)

# Build model
model = initialize_model()
model = add_dense(model, n_inputs = 1, n_neurons = 128, 
                        dropout_rate = 0)
model = add_activation(model, activ_fn = "relu")
model = add_dense(model, n_inputs = 128, n_neurons = 64,
                        dropout_rate = 0)
model = add_activation(model, activ_fn = "relu")
model = add_dense(model, n_inputs = 64, n_neurons = 1,
                        dropout_rate = 0)
model = add_activation(model, activ_fn = "linear")
model = add_loss(model, loss_fn = "mse")


fit = train_model(model, inputs = X, y_true = sine_data$y, 
            epochs = 1000, optimizer = "adam",
            learning_rate = 0.005, lr_decay = 1e-3,
            epsilon = 1e-7, beta_1 = 0.5, beta_2 = 0.5,
            metric_list = c("mse", "mae", "regression_accuracy")
            )

```

Using the same hyperparameters as before, we can see that the new workflow yields the exact same results.  
```{r}
plot(X, fit$predictions)
```

We can also plot the fit history over the epochs.  
```{r}
plot(fit$metrics$epoch, fit$metrics$mse, type = "l")
```


Let's try a classification task. 
First, a binary problem where we'll use a sigmoid function for the final activation function and binary crossentropy for a loss function. Note that piping makes building a model even easier. 
```{r}
spiral_binary = sim_data_fn(N = 100, K = 2)
X = spiral_binary[,c("x", "y")] ## leave out 'label'
X = as.matrix(X)
y_true = spiral_binary$label - 1

binary_model = initialize_model() |>
  add_dense(n_inputs = ncol(X), n_neurons = 64, dropout_rate = 0.2) |>
  add_activation(activ_fn = "relu") |>
  add_dense(n_inputs = 64, n_neurons = 1, dropout_rate = 0) |>
  add_activation(activ_fn = "sigmoid") |>
  add_loss(loss_fn = "binary_crossentropy")

fit_binary = train_model(binary_model, inputs = X, y_true = y_true,
                    epochs = 100, optimizer = "adam", 
                    learning_rate = 0.001, lr_decay = 5e-7,
                    metric_list = c("accuracy"),
                    beta_1 = 0.2, beta_2 = 0.2,batch_print = 0)

fit_binary$final_metrics
```


Now let's go back to multi-class classification, where we'll use a softmax function for the final activation function and categorical crossentropy for a loss function (we'll use our combined softmax activation + categorical crossentropy function as before). I'll also add some L2 regularization to the parameters. Validation data gives a sense of how the model does out-of-sample. Finally, we'll train in batches to demonstrate the new feature of the training function.  

```{r}
# Training data
classes = 3
spiral_data = sim_data_fn(N=50, K=classes)
X = as.matrix(spiral_data[,c("x","y")])
y_true = spiral_data$label
# Validation data
validation_data = sim_data_fn(N=50,K=classes)
X_valid = as.matrix(validation_data[,c("x","y")])
y_valid = validation_data$label

categ_model = initialize_model() |>
  add_dense(n_inputs = ncol(X), n_neurons = 128, dropout_rate = 0.01,
            weight_L2 = 5e-4, bias_L2 = 5e-4) |>
  add_activation("relu") |>
  add_dense(n_inputs = 128, n_neurons = 128, dropout_rate = 0.01,
            weight_L2 = 5e-4, bias_L2 = 5e-4) |>
  add_activation("relu") |>
  add_dense(n_inputs = 128, n_neurons = 64, dropout_rate = 0.001,
            weight_L2 = 5e-4, bias_L2 = 5e-4) |>
  add_activation("relu") |>
  add_dense(n_inputs = 64, n_neurons = length(unique(y_true))) |>
  add_activation("softmax_crossentropy") |>
  add_loss("categorical_crossentropy")

#options(error=recover); debug(train_model)
fit_categ = train_model(categ_model, inputs = X, y_true = y_true,
                        epochs = 10, optimizer = "adam", 
                        learning_rate = 0.01, lr_decay = 1e-3, 
                        beta_1 = 0.5, beta_2 = 0.5,
                        metric_list = c("accuracy"),
                        validation_X = X_valid, validation_y = y_valid,
                        batch_size = NULL,
                        epoch_print = 10, batch_print = 0)

fit_categ$final_metrics
```

```{r}
plot(fit_categ$metrics$epoch, fit_categ$metrics$loss, 
     type = "l", col = "red", xlab = "epoch", ylab = "loss")
lines(fit_categ$metrics$epoch, fit_categ$metrics$validation_loss, 
     type = "l", col = "blue")
legend(x = "topleft", legend = c("training", "validation"),
       lty = c(1, 1), col = c("red", "blue"))

```
The model's performance suffers slighltly when using batches, but this will be useful for larger datasets.  

To get predictions for the training data, we can "test" the model on the training data.
```{r}
train_pred = test_model(model = categ_model,
                         trained_model = fit_categ,
                         X_test = X, y_test = y_true,
                         metric_list = "accuracy")
```


Let's check out the performance of the model using the `classess()` function:  

```{r}
classification_assessment = classess(truths = spiral_data$label,
                                     predictions = train_pred$predictions)
## Confusion Matrix (multiclass)
classification_assessment$conf_mat

## Metrics
classification_assessment$metrics
```
Note that the accuracies for each class are different than if we just looked at the proportion of correctly guessed labels, which is what the `train_model()` and `test_model()` functions do to calculate overall accuracy:
```{r}
# sum of where labels = predicted labels / total labels
sum(spiral_data$label == train_pred$predictions)/length(spiral_data$label)
```
This is due to how the `classess()` function handles assessment of multiclass classification models. It calculates the accuracy for class $i$, treating all other classes $j \neq i$ as one negative class, effectively simplifying the problem to that of a binary classification task. This gives us a more granular understanding of the model's performance at predicting each class.  



Let's test our last categorical model on testing data. This is to simulate making final predictions, onto data which we do not know the classes of.   
This time, we will simply ignore the `y_test` argument.

```{r}
# First make some testing data----
test_data = sim_data_fn(200, K=3) ##600 samples, bigger than the training data!
X_test = as.matrix(test_data[,c("x","y")])

# Now run the test_model function
#options(error=recover); debug(test_model)
final_categ = test_model(model = categ_model,
                         trained_model = fit_categ,
                         X_test = X_test,
                         metric_list = "accuracy")

#True test data
plot(X_test, col = test_data$label)

#Predicted test data
plot(X_test, col = final_categ$predictions)


#Plot multiclass ROC AUC curve
library(yardstick)
pred_df = data.frame(final_categ$raw_predictions, y_test)
colnames(pred_df) = c("class1", "class2", "class3", "y_true")
roc = yardstick::roc_curve(pred_df, truth = as.factor(y_true),
                           c(class2, class2, class3), .level = class1)
plot(x = 1 - roc$specificity, y = roc$sensitivity, col = roc$.level,
     type = "l")

```


```{r}
# print_model() ----
print_model = function(model){
  ##How many unique "base layers" (dense layers)  
  baselayers = c()
  for (i in seq_along(model)){
    base_i = model[[i]]$base_layer
    baselayers = c(baselayers, base_i)
  }
  baselayers = unique(baselayers) ##return vector of each baselayer position
  
  #Extract information from each layer and format 
  model_print = data.frame()
  for (b in baselayers){
    layer_b = c(
      inputs = model[[b]]$n_inputs,
      neurons = model[[b]]$n_neurons,
      weights_L1 = model[[b]]$weight_L1,
      weights_L2 = model[[b]]$weight_L2,
      biases_L1 = model[[b]]$bias_L1,
      biases_L2 = model[[b]]$bias_L2,
      dropout = model[[b]]$dropout_rate,
      activation = model[[b+1]]$class
    )
    model_print = rbind(model_print, layer_b)
  }
  model_print = cbind(layer = 1:length(baselayers), model_print)
  colnames(model_print) = c("layer","inputs","neurons","weights_L1",
                         "weights_L2","biases_L1","biases_L2",
                         "dropout","activation")
  #Last layer is a loss layer
  loss = model[[length(model)]]$class
  
  list("network"=model_print, "loss"=loss)
}

```




