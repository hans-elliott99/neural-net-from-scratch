---
title: "part2"
author: "Hans Elliott"
date: "4/13/2022"
output: html_document
---
# Chapter 11: It's Learning
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, I turn the spiral data generation into a callable function so we can easily mess with the scale of the data. (Idea courtesy of NNFS of course)
```{r sim-data}
## Creating Data
# Source:
sim_data_fn = function(
  N = 100, # number of points per class
  D = 2, # dimensionality (number of features)
  K = 3 # number of classes
){
set.seed(123)
   X = data.frame() # data matrix (each row = single sample)
   y = data.frame() # class labels

for (j in (1:K)){
  r = seq(0.05,1,length.out = N) # radius
  t = seq((j-1)*4.7,j*4.7, length.out = N) + rnorm(N, sd = 0.3) # theta
  Xtemp = data.frame(x =r*sin(t) , y = r*cos(t)) 
  ytemp = data.frame(matrix(j, N, 1))
  X = rbind(X, Xtemp)
  y = rbind(y, ytemp)
  }
spiral_data = cbind(X,y)
colnames(spiral_data) = c(colnames(X), 'label')
return(spiral_data)
# Want a random y?
#spiral_data$label = sample(1:3, size = 99, replace = TRUE)
}


```

```{r}
### Initialize Parameters for a Layer
init_params = function(n_inputs = "# of features", n_neurons){
        
    weights = matrix(data = (0.1 * rnorm(n = n_inputs*n_neurons)),
                             nrow = n_inputs, ncol = n_neurons)
           #Number of weights = the number of inputs*number of neurons,
           #since every connection between the previous neurons (from input) and
           #the current neurons have an associated weight
    biases = matrix(data = 0, nrow = 1, ncol = n_neurons)
           #Number of biases = the number
    #saves:
    list("weights"=weights, "biases"=biases)
}


### Dense Layers ----
layer_dense = list(
## FORWARD PASS FUNCTION 
 forward = function(    #default vals for weights/biases are random
            inputs, 
            n_neurons, weights, biases
            #weights = 
            #    matrix(data = (0.10 * rnorm(n = ncol(inputs)*n_neurons)),
            #                 nrow = ncol(inputs), ncol = n_neurons), 
            #biases = 
            #    matrix(data = 0, nrow = 1, ncol = n_neurons)
 ) {#begin fn
   
  n_inputs = ncol(inputs)
      #determine number of inputs per sample from dims of the input matrix
      #should be equal to # of features (i.e, columns) in a sample (i.e, a row)
  
  #Initalize Weights and Biases
  #if (exists("new_weights")){
  #  weights = new_weights
  #} else {
  #  weights = matrix(data = (0.10 * rnorm(n = n_inputs*n_neurons)),
  #                   nrow = n_inputs, ncol = n_neurons)
  #}
       #Number of weights = the number of inputs*number of neurons. 
       #(Random numbers multipled by 0.10 to keep small.)
  #if (exists("new_biases")){
  #  biases = new_biases
  #} else {
  #  biases = matrix(data = 0, nrow = 1, ncol = n_neurons)
  #}
       #bias will have shape 1 by number of neurons. we initialize with zeros
   
 #Forward Pass 
 output = inputs%*%weights + biases[col(inputs%*%weights)]
 
 #UPDATE: 
 #need to save the inputs for backprop, so we create a list of objects.
 
 #function saves:
 list("output" = output, "inputs" = inputs, 
      "weights"= weights, "biases" = biases)
 #and prints output by default, but invisibly
 #invisible(output)

 },
## BACKWARD PASS
 backward = function(inputs, weights, dvalues){
    #Gradients on parameters
    dweights = t(inputs)%*%dvalues
    dbiases = colSums(dvalues)
    #Gradient on values
    dinputs = dvalues%*%t(weights) 
    #save:
    list("dinputs"=dinputs,
         "dweights"=dweights,
         "dbiases"=dbiases)
 }
 
)
### Activation Functions ----
## ReLU
activation_ReLU = list(
  #FORWARD PASS
  forward = function(input_layer){
    
    output = matrix(sapply(X = input_layer, 
                    function(X){max(c(0, X))}
                    ), 
                  nrow = nrow(input_layer), ncol = ncol(input_layer))
    #ReLU function coerced into a matrix so the shape
    #is maintained (it will be equivalent to that of the input shape)
    
    #Function saves:
    list("output" = output, "inputs" = input_layer)
    #And prints
    #invisible(output)
  },
  #BACKWARD PASS
  backward = function(inputs, dvalues){
    dinputs = dvalues
    dinputs[inputs <= 0] = 0
    #save:
    list("dinputs"=dinputs)
  }
)

## SoftMax
activation_Softmax = list(
  forward = function(inputs){
          #scale inputs
          max_value = apply(X = inputs, MARGIN = 2,  FUN = max)
          scaled_inputs = sapply(X = 1:ncol(inputs), 
                 FUN = function(X){
                    inputs[,X] - abs(max_value[X])})

          # exponetiate
          exp_values = exp(scaled_inputs)
          # normalize
          norm_base = matrix(rowSums(exp_values),
                             nrow = nrow(inputs), ncol = 1)
          probabilities = sapply(X = 1:nrow(inputs),
                          FUN = function(X){exp_values[X,]/norm_base[X,]}) 
          return(t(probabilities))
          #(transpose probabilities)
          },
  
  backward = function(softmax_output, dvalues){
    #*INCOMPLETE SECTION - don't use*
    #flatten output array
    flat_output = as.vector(softmax_output)
    
    #calculate jacobian matrix of output
    jacobian_matrix = diag(flat_output) - flat_output%*%t(flat_output)
    
    #calculate sample-wise gradient
    dinputs = jacobian_matrix%*%flat_dvalues
    
    
  }
)

### Loss ----
Categorical_CrossEntropy = list(
  #FORWARD PASS
  forward = function(y_pred = "softmax output", y_true = "targets"){
    
    #DETECT NUMBER OF SAMPLES
    samples = length(y_true)  

    #CLIP SAMPLES TO AVOID -Inf ERROR
    y_pred_clipped = ifelse(y_pred <= 1e-7, 1e-7, 
                        ifelse(y_pred >= (1-1e-7), (1-1e-7), y_pred))
    

    #DETERMINE IF Y_TRUE IS ONE-HOT-ENCODED AND SELECT CORRESPODNING CONFIDENCES
    confidences = ifelse(nrow(t(y_true)) == 1, 
      #if y_true is a single vector of labels (i.e, sparse), then confidences =
                    y_pred_clipped[cbind(1:samples, y_true)],
                      
                      ifelse(nrow(y_true) > 1,
                      #else, if y_true is one-hot encoded, then confidences =
                             rowSums(y_pred_clipped*y_true),
                             #else
                             "error indexing the predicted class confidences")
                  )
                    
    #CALC LOSS FOR EACH SAMPLE (ROW)
    neg_log_likelihoods = -log(confidences)
    return(neg_log_likelihoods)
    
  },
  #BACKWARD PASS
  backward = function(y_true, dvalues){
    #number of samples
    samples = length(dvalues)
    
    #number of labels
    labels = length(unique(dvalues[1,]))
    
    #if labels are sparse, turn them into one-hot encoded vector
    y_true = ifelse(#if
                    nrow(t(y_true)) ==1,
                    #one-hot-encode
                    y_true = do.call(rbind,
                                     lapply(X = y_true,
                                       function(X) as.integer(
                                          !is.na(match(unique(
                                          unlist(y_true)
                                                        ), X)
                                                ))
                                      )), 
                    #else
                    y_true)
    
    #calculate gradient
    dinputs = -y_true/dvalues
    #normalize gradient
    dinputs = dinputs/samples
    return(dinputs)
  }
)
#---
#Softmax X Cross Entropy- combined softmax activation
# & cross-entropy loss for faster backprop
activation_loss_SoftmaxCrossEntropy = list(
  #FORWARD PASS
  forward = function(inputs, y_true){
    #output layer's activation function
    softmax_out = activation_Softmax$forward(inputs)
    #calculate loss
    loss = Categorical_CrossEntropy$forward(softmax_out, y_true)
    #function saves:
    list("softmax_output"=softmax_out, "loss"=loss) 
  },
  #BACKWARD PASS
  backward = function(dvalues, y_true){
    
    #Detect number of samples
    if (is.vector(dvalues)) {      #if one sample
      samples = 1
    } else if (is.array(dvalues)) {  #else if multiple samples
      samples = nrow(dvalues)
    } else print("error checking shape of inputs")
    
    #Reverse One-Hot Encoding
    #if labels are one-hot encoded, turn them discrete values
     ##helper function 
    anti_ohe = function(y_true){
               unique_classes = ncol(y_true)
               samples = nrow(y_true)
               y_true_vec = as.vector(y_true)
                    
               class_key = rep(1:unique_classes, each = samples)
               y_true = class_key[y_true_vec==1]
                    #selects the classes that correspond to 1s in y_true vector
                    return(y_true)
                    }
     ##check & modify
    y_true = if(is.array(y_true)){ #if one-hot encoded
                    #change to sparse
                    anti_ohe(y_true)
              } else y_true
    
    #Calculate gradient
     #Copy so we can modify
     dinputs = dvalues
     #Calculate gradient
     #index the prediction array with the sample number and its
     #true value index, subtracting 1 from these values. Requires discrete,
     #not one-hot encoded, true labels (explaining the need for the above step)
     dinputs[cbind(1:samples, y_true)] = dinputs[cbind(1:samples, y_true)] - 1
     #Normalize gradient
     dinputs = dinputs/samples
    #save desired outputs
    list("dinputs" = dinputs, "samples" = samples, "y_true" = y_true)
  }
)

```


```{r SGD-optimizer}
optimizer_SGD = list(
  update_params = function(layer_forward, layer_backward, 
                           learning_rate = 1){ #default value
    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    #update
    weights = current_weights - learning_rate*weight_gradients
    biases = current_biases - learning_rate*bias_gradients
      #save:
      list("weights" = weights, "biases" = biases)
  }
)
```

**EPOCH 1**

```{r}
set.seed(1)
#EPOCH 1 - don't specify weights or biases so that they are randomized.
spiral_X = as.matrix(spiral_X)
## Forward Pass
#Hidden Layer
layer1 = layer_dense$forward(inputs = spiral_X, n_neurons = 64)
  layer1_relu = activation_ReLU$forward(input_layer = layer1$output)
#Output Layer
    layer2 = layer_dense$forward(inputs = layer1_relu$output,n_neurons = 3)
        output1 = 
          activation_loss_SoftmaxCrossEntropy$forward(inputs = layer2$output,
                                              y_true = spiral_data$label)
          loss = output1$loss
          paste("loss:", loss)
          predictions = max.col(output1$softmax_output) 
          accuracy = mean(predictions==spiral_data$label)
          paste("accuracy:", accuracy)  
## Backpropagation
        loss_softm_b = activation_loss_SoftmaxCrossEntropy$backward(
                    dvalues = output1$softmax_output, 
                    y_true = spiral_data$label)
    l2_b = layer_dense$backward(inputs = layer2$inputs,
                               weights = layer2$weights,
                               dvalues = loss_softm_b$dinputs)
  l1_relu_b = activation_ReLU$backward(inputs = layer1_relu$inputs,
                                     dvalues = l2_b$dinputs)     
l1_b = layer_dense$backward(inputs = layer1$inputs,
                            weights = layer1$weights,
                            dvalues = l1_relu_b$dinputs)      


#Optimize Layer Parameters
l1_optim = optimizer_SGD$update_params(layer_forward = layer1,
                                       layer_backward = l1_b,
                                       learning_rate = 1)
l2_optim = optimizer_SGD$update_params(layer_forward = layer2,
                                       layer_backward = l2_b,
                                       learning_rate = 1)
```

**EPOCH LOOP**  
```{r}
set.seed(1) ##set seed for results replication

#Inputs
spiral_data = sim_data_fn(N=33) #33 obs per class, total of 99 obs.
spiral_X = spiral_data[,c("x","y")]
spiral_X = as.matrix(spiral_X)

#Initalize Weights & Biases Outside of Loop
l1_params = init_params(n_inputs = 2,      ## ncol(spiral_X) would also work
                         n_neurons = 64)   ## = to desired # neurons in layer 
l2_params = init_params(n_inputs = 64,     ## = to n_neurons in prior layer
                         n_neurons = 3)    ## = to desired # neurons in layer

# EPOCH LOOP
tot_epochs = 1500L

for (epoch in 1:tot_epochs) {
#forward
layer1 = layer_dense$forward(inputs = spiral_X, n_neurons = 64,
                                    weights = l1_params$weights,
                                      biases = l1_params$biases)
  layer1_relu = activation_ReLU$forward(input_layer = layer1$output)
      layer2 = layer_dense$forward(inputs = layer1_relu$output, n_neurons = 3,
                                                   weights = l2_params$weights,
                                                     biases = l2_params$biases)
          output = 
          activation_loss_SoftmaxCrossEntropy$forward(inputs = layer2$output,
                                                  y_true = spiral_data$label)
              #metrics:
              LOSS = output$loss
              PRED = max.col(output$softmax_output, ties.method = "random") 
              ACC = mean(PRED==spiral_data$label)

          #backward
          loss_softm_b = activation_loss_SoftmaxCrossEntropy$backward(
                                      dvalues = output$softmax_output, 
                                      y_true = spiral_data$label)
      l2_b = layer_dense$backward(inputs = layer2$inputs,
                            weights = layer2$weights,
                            dvalues = loss_softm_b$dinputs)
  l1_relu_b = activation_ReLU$backward(inputs = layer1_relu$inputs,
                                       dvalues = l2_b$dinputs)     
l1_b = layer_dense$backward(inputs = layer1$inputs,
                            weights = layer1$weights,
                            dvalues = l1_relu_b$dinputs)      

#optimize
l1_optim_params = optimizer_SGD$update_params(layer_forward = layer1,
                                       layer_backward = l1_b,
                                       learning_rate = 1)
                                       
l2_optim_params = optimizer_SGD$update_params(layer_forward = layer2,
                                       layer_backward = l2_b,
                                       learning_rate = 1)
#update weights & biases
l1_params = l1_optim_params
l2_params = l2_optim_params

#repeat w new weights & biases
##  
#Status Report:
  if (epoch == 1){print(c("epoch","ovrflw err","loss","accuracy"))}
  if (epoch %in% seq(0,tot_epochs,by=100)){
        print(c( epoch, 
                 sum(is.nan(output$softmax_output)),#make sure NaN error is gone
                 LOSS,
                 ACC) )}


#Save Final Metrics:
  if (epoch==tot_epochs){
             out = list("loss"=LOSS,
                         "accuracy"=ACC,
                         "predictions"=PRED)}
}#end loop
```


```{r}
out$loss
out$accuracy
out$predictions
```
Wow! The neural net was able to almost perfectly learn the labels of the spiral data in under 1000 epochs. Of course this wasn't a very difficult dataset for the model to learn, but we've come a long way.  
A note:
In a previous attempt, I trained the model on a dataset of 300 obs. and with a learning rate of 1, the NN broke down after about 1000 epochs. Specifically, it became filled with NaN values, rendering it useless. I'm guessing that this is related to exploding gradients. I stopped the NN around epoch 900 and found that the size of the weights had grown to be quite large. By about epoch 1000, it would start returning NaN values. This can be solved using a lower learning rate (e.g, 0.5 worked). In practice, a learning rate of 1 is quite high, allowing every step in the gradient descent process to be rather large, directly leading to gradient explosion (NNFS, Ch. 10 pg 24). 
This problem is relatively common and can be solved in [various ways](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/). My initial tendency is to re-code something so that this simply cannot happen, which points towards _gradient clipping_ or _weight regularization_, but alternative options (lower learning rates or reshuffling the shape of the network) are also viable solutions, so I won't touch anything for now.  
Either way, this is a good segue into adding some further hyperparameter options to the Neural Network.   


```{r}
#Actual
plot(spiral_data$x, spiral_data$y, col = spiral_data$label)
#Predicted
plot(spiral_data$x, spiral_data$y, col = out$predictions)
```


# Chapter 12: Hyperparameters
## Learning Rate Decay
Learning rate decay describes a process whereby the learning rate decreases during the course of model training. 
This could be achieved by programmatically or manually adjusting the learning rate when the model's loss plateaus or begins jumping randomly.  
It could also be achieved by programming a simple **Decay Rate**, which lowers the learning rate each epoch or each batch. This is also known as 1/t decaying or exponential decaying. For example:  

```{r}
start_learn_rate = 1
learn_rate_decay = 0.1
for (step in 1:20){
  learning_rate = start_learn_rate * (1 / (1 + learn_rate_decay*step))
                                      ## ensures that learn rate never gets >1
  print(learning_rate)
}
```

We will simply add this as an optional hyperparameter to our SGD optimizer. Since I'm currently going with the for loop method, iteration will just be set equal to the current epoch.  
The default values ensure that if we decided to ignore decay, it will not affect the learning rate.  
**Note:** I implemented a simpler version. The function _should ideally_ take the current learning rate and decrease from that, but given my current methodology we would have to save the current learning rate as an object to be continously updated and input back into the optimizer... so for now, this is a simpler solution. 
```{r}
optimizer_SGD = list(
  update_params = function(layer_forward, layer_backward, 
                           learning_rate = 1,
                           decay = 0, iteration = 1){ #default values
    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + decay*iteration)) 
    
    #update
    weights = current_weights - currnt_learn_rate*weight_gradients
    biases = current_biases - currnt_learn_rate*bias_gradients
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate)
  }
)
```


Even with my "simple" decay rate, this solved the previous issue of exploding gradients for a 300 obs. dataset starting with a learning rate of 1:
```{r}
set.seed(1) ##set seed for results replication

#Inputs
#Inputs
spiral_data = sim_data_fn(N=100) #100 obs per class, total of 99 obs.
spiral_X = spiral_data[,c("x","y")]
spiral_X = as.matrix(spiral_X)

#Initalize Weights & Biases Outside of Loop
l1_params = init_params(n_inputs = 2,      ## ncol(spiral_X) would also work
                         n_neurons = 64)   ## = to desired # neurons in layer 
l2_params = init_params(n_inputs = 64,     ## = to n_neurons in prior layer
                         n_neurons = 3)    ## = to desired # neurons in layer

# EPOCH LOOP
tot_epochs = 1500L

for (epoch in 1:tot_epochs) {
#forward
layer1 = layer_dense$forward(inputs = spiral_X, n_neurons = 64,
                                    weights = l1_params$weights,
                                      biases = l1_params$biases)
  layer1_relu = activation_ReLU$forward(input_layer = layer1$output)
      layer2 = layer_dense$forward(inputs = layer1_relu$output, n_neurons = 3,
                                                   weights = l2_params$weights,
                                                     biases = l2_params$biases)
          output = 
          activation_loss_SoftmaxCrossEntropy$forward(inputs = layer2$output,
                                                  y_true = spiral_data$label)
              #metrics:
              LOSS = output$loss
              PRED = max.col(output$softmax_output, ties.method = "random") 
              ACC = mean(PRED==spiral_data$label)

          #backward
          loss_softm_b = activation_loss_SoftmaxCrossEntropy$backward(
                                      dvalues = output$softmax_output, 
                                      y_true = spiral_data$label)
      l2_b = layer_dense$backward(inputs = layer2$inputs,
                            weights = layer2$weights,
                            dvalues = loss_softm_b$dinputs)
  l1_relu_b = activation_ReLU$backward(inputs = layer1_relu$inputs,
                                       dvalues = l2_b$dinputs)     
l1_b = layer_dense$backward(inputs = layer1$inputs,
                            weights = layer1$weights,
                            dvalues = l1_relu_b$dinputs)      

#optimize
l1_optim_params = optimizer_SGD$update_params(layer_forward = layer1,
                                       layer_backward = l1_b,
                                       learning_rate = 1,
                                       decay = .01,iteration = epoch
                                       )
l2_optim_params = optimizer_SGD$update_params(layer_forward = layer2,
                                       layer_backward = l2_b,
                                       learning_rate = 1,
                                       decay = .01, iteration = epoch)
#update weights & biases
l1_params = l1_optim_params
l2_params = l2_optim_params

#repeat w new weights & biases
##  
#Status Report:
  if (epoch == 1){print(c("epoch","ovrflw err","loss","accuracy", "learn rate"))}
  if (epoch %in% seq(0,tot_epochs,by=100)){
        print(c( epoch, 
                 sum(is.nan(output$softmax_output)),#make sure NaN error is gone
                 LOSS,
                 ACC,
                 l1_optim_params$lr) )} #(include learn rate)


#Save Final Metrics:
  if (epoch==tot_epochs){
             out = list("loss"=LOSS,
                         "accuracy"=ACC,
                         "predictions"=PRED)}
}#end loop
```
The learning rate decreases quite quickly, but this seems to work. Rather than losing control, the NN steadily decreases the loss and reaches an accuracy of 87%, which is much better than random guessing.

