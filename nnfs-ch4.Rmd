---
title: "part4"
author: "Hans Elliott"
date: "4/28/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

NOTE: currently, for multi-class, the classes would have to be encoded starting at 1, since code relies on index (R cols start at 1.)

## Full code
In the last section, we worked on improving model generalization. In addition to testing our neural net's predictions on test data, we added L1 and L2 regularization and a dropout layer to our set of functions.  
I've also cleaned up the functions a bit so things run smoother. Here's a list of the updates:   
- Overall, my changes are a move towards a new system which I think will be easy to implement and will reduce the amount of overhead needed to actually use the functions. Basically, the heart of the process will be the layer objects that get created for each hidden and output layer. The layer objects are stocked full of other objects which they can pass on to the various other functions as needed. That way, all we really have to do is pass on one layer into the next, and the only other arguments we need to specify are hyperparameters.   
- The `init_params` function will save the n_neurons in addition to all of the parameters since it was redundant to include n_neurons as an argument in both the initialization function and the `layer_dense` function. The `layer_dense` function is updated to extract the n_neurons object from the parameters object.  
- Since the init_params fns defines the number of neurons, we will need the optimized paramaters object at the end of backpropagation to also contain a n_neurons object, so that when the forward pass begins again the layer_dense function can find it. This is fairly simple since we are already passing the forward layer object to the optimizer function, and thus it can extract the number of neurons from the forward layer's output matrix - the number of columns in the output matrix corresponds to the number of neurons.  
- There was some redundancy with the backprop step, where I was specifying "dvalues" as the derivative values being passed back from the next layer into the current layer. I simplify this so that all we have to do is pass back that next layer object (slightly confusing - this is backprop layer corresponding to the forward-pass layer that is the next layer in the forward pass, so technically it is the previous layer in the backprop). I'm calling this argument `d_layer.` The idea is that you just pass back each backpropagated layer sequentially. Thus every backpropagation step takes two simple arguments: the forward pass layer object, and the backward pass layer object of the "next" layer. This works fairly well except


```{r nn-fns}
### Initialize Layer Parameters-----------------------------------------------
init_params = function(n_inputs = "# of features",
                       n_neurons = "desired # of neurons",
                       momentum = FALSE,
                       cache = FALSE){
    
    weights = matrix(data = (0.1 * rnorm(n = n_inputs*n_neurons)),
                             nrow = n_inputs, ncol = n_neurons)
           #Number of weights = the number of inputs*number of neurons,
           #since every connection between the previous neurons (from input) and
           #the current neurons have an associated weight
    biases = matrix(data = 0, nrow = 1, ncol = n_neurons)
           #Number of biases = the number
    
    #momentum initialization
    weight_momentums = matrix(data = 0, 
                            nrow = nrow(weights),
                            ncol = ncol(weights))
    bias_momentums = matrix(data = 0,
                            nrow = nrow(biases),
                            ncol = ncol(biases))
    #cache initialization
    weight_cache = matrix(data = 0,
                          nrow = nrow(weights),
                          ncol = ncol(weights))
    bias_cache = matrix(data = 0,
                        nrow = nrow(biases),
                        ncol = ncol(biases))

    #saving:
    if (momentum == TRUE & cache == FALSE){ ##momentums only
          list("weights"=weights,"biases"=biases,
               "weight_momentums"=weight_momentums,
               "bias_momentums"=bias_momentums,
               "n_neurons"=n_neurons)
    } else if (momentum == FALSE & cache == TRUE){ ##cache only
          list("weights"=weights,"biases"=biases,
               "weight_cache"=weight_cache,
               "bias_cache"=bias_cache,
               "n_neurons"=n_neurons)
    } else if (momentum == TRUE &  cache == TRUE){ ##momentums and cache
          list("weights"=weights,"biases"=biases,
               "weight_momentums"=weight_momentums,
               "bias_momentums"=bias_momentums,
               "weight_cache"=weight_cache,
               "bias_cache"=bias_cache,
               "n_neurons"=n_neurons)
    } else if (momentum == FALSE & cache == FALSE){ ##no momentums or cache
          list("weights"=weights,"biases"=biases,"n_neurons"=n_neurons)
    }
}



### Dense Layer ---------------------------------------------------------------
layer_dense = list(
## FORWARD PASS 
 forward = function(    
            inputs,      
            parameters, ## from initialize_parameters
            weight_L1 = 0, weight_L2 = 0,  ##regularization
            bias_L1 = 0, bias_L2 = 0
){
  
 if(is.matrix(inputs) == FALSE ) message("Convert inputs to matrix first")
 
 n_inputs = ncol(inputs)
 n_neurons = parameters$n_neurons
 weights = parameters$weights
 biases = parameters$biases
 #Forward Pass 
 output = inputs%*%weights + biases[col(inputs%*%weights)]
 
 #Regularization values:
 regularization = list("weight_L1" = weight_L1, "weight_L2" = weight_L2,
                       "bias_L1" = bias_L1, "bias_L2" = bias_L2)

 #SAVING:
 #then layer saves momentum only
 if (exists(x = "weight_momentums", where = parameters) & 
     !exists(x = "weight_cache", where = parameters)){
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "weight_momentums"=parameters$weight_momentums, ##for momentum
      "bias_momentums"=parameters$bias_momentums,
      "regularization" = regularization)  ##for regularization
 #if momentum==FALSE & cache==TRUE, saves cache only
 } else if (!exists(x = "weight_momentums", where = parameters) &
            exists(x = "weight_cache", where = parameters)){
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "weight_cache"=parameters$weight_cache, ##for cache
      "bias_cache"=parameters$bias_cache,
      "regularization" = regularization)

 #if momentum==TRUE & cache==TRUE, saves both
 } else if (exists(x = "weight_momentums", where = parameters) &
            exists(x = "weight_cache", where = parameters)){
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "weight_momentums"=parameters$weight_momentums, ##for momentum
      "bias_momentums"=parameters$bias_momentums,
      "weight_cache"=parameters$weight_cache, ##for cache
      "bias_cache"=parameters$bias_cache,
      "regularization" = regularization)

 #else both==FALSE, ignore momentum & cache
 } else {
   #otherwise, just save
 list("output" = output, ##for forward pass
      "inputs" = inputs, "weights"= weights, "biases" = biases, ##for backprop
      "regularization" = regularization)
      }

 },#end fwd
 
# BACKWARD
  backward = function(d_layer="layer object that occurs prior in backward pass",
                      layer="the layer object from the forward pass"){
    
    dvalues = d_layer$dinputs
    #Gradients on parameters
    dweights = t(layer$inputs)%*%dvalues
    dbiases = colSums(dvalues)
    
    #Gradients on regularization
    ##regularization hyperparams:
    layer_reg = layer$regularization ##a list of the set lambda values

    ##L1 Weights##
    if (layer_reg$weight_L1 > 0){
    dL1 = matrix(1, nrow = nrow(layer$weights), ncol = ncol(layer$weights))
      #make matrix filled with 1s
    dL1[layer$weight < 0] = -1
      #convert matrix value to -1 where weight is less than zero
    dweights = dweights + layer_reg$weight_L1 * dL1
    }
    ##L2 Weights##
    if (layer_reg$weight_L2 > 0){
    dweights = dweights + 2 * layer_reg$weight_L2 * layer$weights
    }                     #2 * lambda * weights
    
    ##L1 Biases##
    if (layer_reg$bias_L1 > 0){
    dL1 = matrix(1, nrow = nrow(layer$biases), ncol = ncol(layer$biases))
      #make matrix filled with 1s
    dL1[layer$bias < 0] = -1
      #convert matrix value to -1 where weight is less than zero
    dbiases = dbiases + layer_reg$bias_L1 * dL1
    }
    ##L2 Biases##
    if (layer_reg$bias_L2 > 0){
    dbiases = dbiases + 2 * layer_reg$bias_L2 * layer$biases
    }
    
    #Gradients on values
    dinputs = dvalues%*%t(layer$weights) 
    
    #saves:
    list("dinputs"=dinputs,
         "dweights"=dweights,
         "dbiases"=dbiases)
}#end bwd
)



### Dropout Layer -------------------------------------------------------------
layer_dropout = list(
## FORWARD PASS
  forward = function(
              input_layer, ##layer to apply dropout to
              dropout_rate ##rate of neuron deactivation
) {
  inputs = input_layer$output   ##the outputs from the previous layer
  
  #Dropout mask/filter
  dropout_filter = matrix(data = 
                           rbinom(n = nrow(inputs)*ncol(inputs),
                                  size = 1,        
                                  p = (1-dropout_rate)),
                          nrow = nrow(inputs),
                          ncol = ncol(inputs)) / 
                  (1 - dropout_rate)
  ##Creates matrix that is shape of the input layer's output (from nrow, ncol)
  ##and fills it with 1s and 0s from "Bernoulli". The length of the rbinom 
  ##output is equal to nrow*ncol so it fills the input layer's shape. 
  ##We also apply the scaling step to the filter directly since it makes the 
  ##backprop step even simpler.
  
  ##Apply mask to inputs and scale by (1 - dropout_rate)
  output = inputs * dropout_filter
  
  list("output" = output, "dropout_filter" = dropout_filter)

},
## BACKWARD PASS
  backward = function(
            d_layer = "layer object that occurs prior in backward pass",
            #derivative values being passed back from next layer
            layer_dropout = "the layer object from the forward pass"
                                    ##the forward-pass dropout layer object
){
  dvalues = d_layer$dinputs ##extract the derivative object from the layer
  dinputs = dvalues * layer_dropout$dropout_filter    
  ##Thus if the filter is 0 at the given index, the derivative is 0
  ##And if the filter is 1/(1-dropout_rate) at the given index, 
  ##the derivative is dvalues * 1/(1-dropout_rate)
  list("dinputs" = dinputs)
  }

)#end layer_dropout

### Activation Functions ------------------------------------------------------
## ReLU ##
activation_ReLU = list(
  #FORWARD PASS
  forward = function(input_layer){
    
    inputs = input_layer$output
    output = matrix(sapply(X = inputs, 
                    function(X){max(c(0, X))}
                    ), 
                  nrow = nrow(inputs), ncol = ncol(inputs))
    #ReLU function coerced into a matrix so the shape
    #is maintained (it will be equivalent to that of the input shape)
    
    #Function saves:
    list("output" = output, "inputs" = inputs)
  },
  #BACKWARD PASS
  backward = function(d_layer, layer){
    
    inputs = layer$inputs
    dinputs = d_layer$dinputs ##the dinputs from the next layer
    
    dinputs[inputs <= 0] = 0
    #save:
    list("dinputs"=dinputs)
  }
)

## SoftMax ##
activation_Softmax = list(
  forward = function(inputs){
          #scale inputs
          max_value = apply(X = inputs, MARGIN = 2,  FUN = max)
          scaled_inputs = sapply(X = 1:ncol(inputs), 
                 FUN = function(X){
                    inputs[,X] - abs(max_value[X])})

          # exponetiate
          exp_values = exp(scaled_inputs)
          # normalize
          norm_base = matrix(rowSums(exp_values),
                             nrow = nrow(inputs), ncol = 1)
          probabilities = sapply(X = 1:nrow(inputs),
                          FUN = function(X){exp_values[X,]/norm_base[X,]}) 
          return(t(probabilities))
          #(transpose probabilities)
          },
  
  backward = function(softmax_output, dvalues){
    #*INCOMPLETE SECTION - don't use*
    #flatten output array
    flat_output = as.vector(softmax_output)
    
    #calculate jacobian matrix of output
    jacobian_matrix = diag(flat_output) - flat_output%*%t(flat_output)
    
    #calculate sample-wise gradient
    dinputs = jacobian_matrix%*%flat_dvalues
    
    
  }
)

## Softmax X Cross Entropy ##
# combined softmax activation fn & cross-entropy loss for faster backprop
activation_loss_SoftmaxCrossEntropy = list(
  #FORWARD PASS
  forward = function(input_layer, y_true){
    
    inputs = input_layer$output
    #output layer's activation function
    softmax_out = activation_Softmax$forward(inputs)
    #calculate loss
    loss = Categorical_CrossEntropy$forward(softmax_out, y_true)
    #function saves:
    list("output"=softmax_out, "loss"=loss) 
  },
  #BACKWARD PASS
  backward = function(dvalues, y_true){

    #Detect number of samples
    if (is.vector(dvalues)) {      #if one sample
      samples = 1
    } else if (is.array(dvalues)) {  #else if multiple samples
      samples = nrow(dvalues)
    } else message("error checking shape of inputs")
    
    #Reverse One-Hot Encoding
    #if labels are one-hot encoded, turn them discrete values
     ##helper function 
    anti_ohe = function(y_true){
               unique_classes = ncol(y_true)
               samples = nrow(y_true)
               y_true_vec = as.vector(y_true)
                    
               class_key = rep(1:unique_classes, each = samples)
               y_true = class_key[y_true_vec==1]
                    #selects the classes that correspond to 1s in y_true vector
                    return(y_true)
                    }
     ##check & modify
    y_true = if(is.array(y_true)){ #if one-hot encoded
                    #change to sparse
                    anti_ohe(y_true)
              } else y_true
    
    #Calculate gradient
     #Copy so we can modify
     dinputs = dvalues
     #Calculate gradient
     #index the prediction array with the sample number and its
     #true value index, subtracting 1 from these values. Requires discrete,
     #not one-hot encoded, true labels (explaining the need for the above step)
     dinputs[cbind(1:samples, y_true)] = dinputs[cbind(1:samples, y_true)] - 1
     #Normalize gradient
     dinputs = dinputs/samples
    #save desired outputs
    list("dinputs" = dinputs, "samples" = samples, "y_true" = y_true)
  }
)


### Loss ----------------------------------------------------------------------
## Categorical Cross Entropy ##
Categorical_CrossEntropy = list(
  #FORWARD PASS
  forward = function(y_pred = "softmax output", y_true = "target labels"){
    
    #DETECT NUMBER OF SAMPLES
    samples = length(y_true)  

    #CLIP SAMPLES TO AVOID -Inf ERROR
    y_pred_clipped = ifelse(y_pred <= 1e-7, 1e-7, 
                        ifelse(y_pred >= (1-1e-7), (1-1e-7), y_pred))
    

    #DETERMINE IF Y_TRUE IS ONE-HOT-ENCODED AND SELECT CORRESPODNING CONFIDENCES
    confidences = ifelse(nrow(t(y_true)) == 1, 
      #if y_true is a single vector of labels (i.e, sparse), then confidences =
                    y_pred_clipped[cbind(1:samples, y_true)],
                      
                      ifelse(nrow(y_true) > 1,
                      #else, if y_true is one-hot encoded, then confidences =
                             rowSums(y_pred_clipped*y_true),
                             #else
                             "error indexing the predicted class confidences")
                  )
                    
    #CALC LOSS FOR EACH SAMPLE (ROW)
    sample_losses = -log(confidences)
    return(sample_losses)
    
  },
  #BACKWARD PASS
  backward = function(y_true, d_layer){
    dvalues = d_layer$dinputs
    
    #number of samples
    samples = length(dvalues)
    
    #number of labels
    labels = length(unique(dvalues[1,]))
    
    #if labels are sparse, turn them into one-hot encoded vector
    y_true = ifelse(#if
                    nrow(t(y_true)) ==1,
                    #one-hot-encode
                    y_true = do.call(rbind,
                                     lapply(X = y_true,
                                       function(X) as.integer(
                                          !is.na(match(unique(
                                          unlist(y_true)
                                                        ), X)
                                                ))
                                      )), 
                    #else
                    y_true)
    
    #calculate gradient
    dinputs = -y_true/dvalues
    #normalize gradient
    dinputs = dinputs/samples
    list("dinputs" = dinputs)
  }
)


## Regularization ##
regularization_loss = function(layer){
  #Regularization hyperparams:
  layer_reg = layer$regularization ##a list of the set lambda values
  
  #L1-regularization: weights
  l1_weight_apply = layer_reg$weight_L1 * sum(abs(layer$weights))
  #L1-regularization: bias
  l1_bias_apply = layer_reg$bias_L1 * sum(abs(layer$biases))
  
  #L2-regularization: weights
  l2_weight_apply = layer_reg$weight_L2 * sum(layer$weights^2)
  #L2-regularization: biases
  l2_bias_apply = layer_reg$bias_L2 * sum(layer$biases^2)
  
  #Overall regularization loss
  reg_loss = l1_weight_apply + l1_bias_apply + l2_weight_apply + l2_bias_apply
  #save:
  return(reg_loss)
}

### Optimizers----------------------------------------------------------------
## Stochastic Gradient Descent (vanilla + decay & momentum options) ##
optimizer_SGD = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 1,
                           lr_decay = 0, iteration = 1,
                           momentum_rate = 0 
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)
    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #param updates with momentum
     #If momentum == TRUE in parameter initialization, then weights_momentum
     #(and implictly, bias_momentum) will exist
    if (exists("weight_momentums", where = layer_forward)) {
      #current momentums
      weight_momentums = layer_forward$weight_momentums
      bias_momentums = layer_forward$bias_momentums
      
      #Update weights & biases with momentum:
      #Take prior updates X retainment factor (the "momentum rate"),
      #and update with current gradients
      weight_update = 
        (momentum_rate*weight_momentums) - (currnt_learn_rate*weight_gradients)
      bias_update = 
        (momentum_rate*bias_momentums) - (currnt_learn_rate*bias_gradients)
      #update params with the calculated updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
      #also update momentums
      weight_momentums = weight_update
      bias_momentums = bias_update
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
           "weight_momentums" = weight_momentums, 
           "bias_momentums" = bias_momentums,
           "n_neurons" = n_neurons)
    } else {
      
    #param updates without momentum (vanilla)
      #calculate updates
      weight_update = -currnt_learn_rate*weight_gradients
      bias_update = -currnt_learn_rate*bias_gradients
      #apply updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
           "n_neurons" = n_neurons)
    }
    
})
## AdaGrad ##
#NOTE: must initialize params with cache==TRUE
optimizer_AdaGrad = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 1,
                           lr_decay = 0, iteration = 1,
                           epsilon = 1e-7 
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)

    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #cache
    ##update cache with squared current gradients
    weight_cache = layer_forward$weight_cache + weight_gradients^2
    bias_cache = layer_forward$bias_cache + bias_gradients^2
    
    #SGD param updates with normalization 
      #calculate updates
      weight_update = -currnt_learn_rate*weight_gradients /
                        (sqrt(weight_cache) + epsilon)
      bias_update = -currnt_learn_rate*bias_gradients /
                        (sqrt(bias_cache) + epsilon)
      #apply updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
        #save:
        list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
             "weight_cache" = weight_cache, "bias_cache" = bias_cache,
             "n_neurons" = n_neurons)
} ##these are the new params to be passed on to the layers when loop restarts
)

## RMSProp ##
#NOTE: must initialize params with cache==TRUE
optimizer_RMSProp = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 0.001,
                           lr_decay = 0, iteration = 1,
                           epsilon = 1e-7,
                           rho = 0.9
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)

    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #cache
    ##update cache with squared current gradients
    weight_cache = rho * layer_forward$weight_cache + 
                  (1-rho) * weight_gradients^2
    bias_cache = rho * layer_forward$bias_cache + 
                  (1-rho) * bias_gradients^2
    
    #SGD param updates with normalization 
      #calculate updates
      weight_update = -currnt_learn_rate*weight_gradients /
                        (sqrt(weight_cache) + epsilon)
      bias_update = -currnt_learn_rate*bias_gradients /
                        (sqrt(bias_cache) + epsilon)
      #apply updates
      weights = current_weights + weight_update
      biases = current_biases + bias_update
        #save:
        list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
             "weight_cache" = weight_cache, "bias_cache" = bias_cache,
             "n_neurons" = n_neurons)
} ##these are the new params to be passed on to layers when the loop restarts
)

## Adam ##
#NOTE: must intialize paramters with both cache AND momentum
optimizer_Adam = list(
  update_params = function(layer_forward, layer_backward, #required
                           learning_rate = 0.001,
                           lr_decay = 0, iteration = 1,
                           epsilon = 1e-7,
                           beta_1 = 0.9, beta_2 = 0.999
                           ){
    #extract number of neurons from the forward layers output matrix
    n_neurons = ncol(layer_forward$output)

    #current params
    current_weights = layer_forward$weights
    current_biases = layer_forward$biases
    #gradients
    weight_gradients = layer_backward$dweights
    bias_gradients = layer_backward$dbiases
    
    #learning rate
    currnt_learn_rate = learning_rate * (1 / (1 + lr_decay*iteration)) 
    
    #momentums
    #update momentums with current gradients and bias correct
    weight_momentums = 
      (beta_1*layer_forward$weight_momentums + (1-beta_1)*weight_gradients ) /
                              (1 - (beta_1^iteration )) ##bias correction
    
    bias_momentums = 
      (beta_1*layer_forward$bias_momentums + (1-beta_1)*bias_gradients) /
                              (1 - (beta_1^iteration))
                    

    #cache
    #update cache with squared gradients and bias correct
    weight_cache = 
      (beta_2*layer_forward$weight_cache + (1-beta_2)*weight_gradients^2) /
                              (1 - (beta_2^iteration)) ##bias correction

    bias_cache = 
      (beta_2*layer_forward$bias_cache + (1-beta_2)*bias_gradients^2) /
                              (1 - (beta_2^iteration))
    
    #calculate param updates (with momentums, and normalize with cache)
    weight_update = -currnt_learn_rate*weight_momentums /
                        (sqrt(weight_cache) + epsilon)
    bias_update = -currnt_learn_rate*bias_momentums /
                        (sqrt(bias_cache) + epsilon)
    
    #apply updates
    weights = current_weights + weight_update
    biases = current_biases + bias_update
      #save:
      list("weights" = weights, "biases" = biases, "lr" = currnt_learn_rate,
           "weight_momentums" = weight_momentums, 
           "bias_momentums" = bias_momentums,
           "weight_cache" = weight_cache,
           "bias_cache" = bias_cache,
           "n_neurons" = n_neurons)

}##these are the updated params to be passed back into the layers
)

##function prints reminders for settings needed for each optimizer
optimizer_REMINDME = function(optimizer = "sgd, adagrad, rmsprop, or adam"){
  if(optimizer == "sgd"){
    message("Momentum is optional, cache is not available. If you choose to set momentum rate in SGD arguments, set momentum = TRUE in parameter initialization function")
  }
  if(optimizer == "adagrad"){
    message("Initialize parameters with cache=TRUE, momentum=FALSE. The AdaGrad optimizer implements cache - a rolling average/history of past gradients - as a form of adaptive gradients (or 'per-parameter learning rates'.")
  }
  if(optimizer == "rmsprop"){
    message("Initialize parameters with cache=TRUE, momentum=FALSE. RMSProp implements cache like AdaGrad, but exponentially decays the cache. You will also have to set the 'rho' hyperparameter - the cache memory decay rate.")
  }
  if(optimizer == "adam"){
    message("Initialize parameters with cache=TRUE & momentum=TRUE. To calculate parameter updates, Adam implements momentums in place of vanilla gradients and also applies cache to save a rolling average of the momentums.")
  }
  
}



```

And here is our function to simulate the spiral data. 
```{r sim-data-fn}
## Creating Spiral Data ----
#Source: https://cs231n.github.io/neural-networks-case-study/
sim_data_fn = function(
  N = 100, # number of points per class
  D = 2,   # "dimensionality" (number of features)
  K = 3,   # number of classes
  random_order = FALSE #T: random order of classes (harder task)
){
   X = data.frame() # data matrix (each row = single sample)
   y = data.frame() # class labels

for (j in (1:K)){
  r = seq(0.05,1,length.out = N) # radius
  t = seq((j-1)*4.7,j*4.7, length.out = N) + rnorm(N, sd = 0.3) # theta
  Xtemp = data.frame(x =r*sin(t) , y = r*cos(t)) 
  ytemp = data.frame(matrix(j, N, 1))
  X = rbind(X, Xtemp)
  y = rbind(y, ytemp)
  }
spiral_data = cbind(X,y)
colnames(spiral_data) = c(colnames(X), 'label')
spiral_data$label = spiral_data$label

# Want randomly ordered labels?
if (random_order==TRUE) {spiral_data$label = sample(1:(K), size = N*K, 
                                                    replace = TRUE)}
return(spiral_data)
}
```



# Binary Logistic Regression
We can start adding some new features to the neural net package, so that we can use it in different situations that multi-class classification.  
For binary classificiation (2 classes), logistic regression is a common method, where we apply a sigmoid activation to the output layer.    

The sigmoid function collapses its inputs into a range of outputs between 0 and 1, where 0 and 1 represent the 2 possible classes. It's outputs approach 0 and 1 exponetially fast. It will be simple to code since it is sigmoid =  1/(1 + e^-z) where e is Euler's number and z is the input value.  
The derivative of the sigmoid function also solves quite simply to sigmoid*(1 - sigmoid), which will be easy to implement.  

```{r activation-sigmoid}

activation_Sigmoid = list(
  forward = function(input_layer){
    inputs = input_layer$output
    sigmoid = 1 / (1 + exp(-inputs))
    list("output" = sigmoid)
},
  backward = function(d_layer = "the layer object being passed back",
                      layer = "the sigmoid layer obj from the forward pass"
                      ){
    dvalues = d_layer$dinputs ##the values being passed back
    output = layer$output ##the output from the forward-pass
    dinputs = dvalues * (1 - output) * output 
    list("dinputs" = dinputs)
  }
)

```



# Binary Cross-Entropy Loss
Now we will implement binary cross-entropy loss as our loss function. This will be similar to categorical cross-entropy, where we implemented -log() so that the lower the confidence of an output neuron (the lower the predicted probability for a given class), the higher the loss value will be. But instead of just calculating this on the target class, we sum the log-likelihoods of the correct & incorrect classes for each neuron. 
```
sample_losses = -(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred)
```
Where y_true is the correct class, and (1 - y_true) is the inccorect class, since the classes are simplified to 0 or 1.  

Since we can have multiple output neurons each predicting which of the 2 classes the sample corresponds to, we might have a vector of loss values (one from each neuron) and so we can calculate the mean loss for each sample.  
The derivative of this loss function simplifies to 
```
 [-(y_true / y_pred) - (1-y_true)/(1 - y_pred)] * (1/number_of_sigmoid_outputs)
```
As usual, check out the [NNFS](nnfs.io) book to see an excellent explanation of the math and intuition behind these formulas.  
```{r binary-crossentropy}
loss_BinaryCrossentropy = list(
  forward = function(y_pred, y_true){
    #clip data to prevent division by zero (both sides to keep mean unbiased)
    y_pred_clipped = ifelse(y_pred >= 1-1e-7, 1-1e-7,
                            ifelse(y_pred <= 1e-7, 1e-7, y_pred))
    
    #calculate sample-wise losse per neuron
    sample_losses = -(y_true*log(y_pred_clipped) + 
                                (1 - y_true)*log(1 - y_pred_clipped)
                        )
    #calculate total (mean) loss for each sample 
    ## (mean across neurons/vector of outputs, neurons = cols, samples = rows)
    sample_losses = rowMeans(sample_losses)
    
    #calculate mean loss across the entire batch
    data_loss = mean(sample_losses)
    
    list("sample_losses" = sample_losses, "data_loss" = data_loss)
  },
  backward = function(dvalues = "sigmoid output", y_true){
    
    #number of samples
    samples = nrow(dvalues)
    #number of output neurons
    n_outputs = ncol(dvalues)
    
    #clip data to prevent divide by zero
    clipped_dvalues = ifelse(dvalues >= 1-1e-7, 1-1e-7,
                            ifelse(dvalues <= 1e-7, 1e-7, dvalues))
    #calculate gradients
    dinputs = -(y_true / clipped_dvalues - 
                  (1 - y_true) / (1 - clipped_dvalues)
                  ) / n_outputs
    
    #normalize gradient
    dinputs = dinputs/samples
    ##We have to perform this normalization since each output returns its own
    ##derivative, and without normalization, each additional input will increase
    ##the gradients mechanically
    list("dinputs" = dinputs)
  }
)    
```

Before we test this, I've also implemented a little metrics calculation function:

```{r}
### Metrics ----
metric_calc = function(output_layer, loss_layer, y_true,
                       reg_loss = 0, loss_fn = "categorical_crossentropy,
                                                binary_crossentropy"
                       ){
  if(loss_fn == "categorical_crossentropy"){
  ##Loss
    DATA_LOSS = output_layer$loss
    
    REG_LOSS = reg_loss               ## = 0 if no regularization
    TOTAL_LOSS = DATA_LOSS + REG_LOSS ## = data loss if no regularization
      
    
 ##Predictions
    PRED = max.col(output_layer$output, ties.method = "random")

 ##Accuracy
    ACC = mean(PRED == y_true, na.rm=T)
    
    list("data_loss"=DATA_LOSS, "regularization_loss"=REG_LOSS,
         "total_loss"=TOTAL_LOSS, "accuracy"=ACC,
         "predictions"=PRED)
    
  } else if(loss_fn == "binary_crossentropy"){
    DATA_LOSS = loss_layer$data_loss
    
    REG_LOSS = reg_loss
    TOTAL_LOSS = DATA_LOSS + REG_LOSS
    
  ##Predictions
    PRED = (output_layer$output > 0.5) * 1
            ##returns TRUE (1) if true, and FALSE (0) if false
  ##Accuracy
    ACC = mean(PRED == y_true, na.rm = T)
    
    list("data_loss"=DATA_LOSS, "regularization_loss"=REG_LOSS,
         "total_loss"=TOTAL_LOSS, "accuracy" = ACC,
         "predictions" = PRED)
  }                    

  
}

```


Now I test the sigmoid activation function and binary crossentropy loss function. _Note that (for each sample) we technically just need one neuron in the output layer, since the sigmoid function will (eventually) be predicting 0 or 1. But we will still leave 3 output neurons, giving each an opportunity to predict the label._  Not sure about this. Don't know why I did or wrote this.    
First sim the data with 2 classes instead of 3.
```{r sim-the-data}
set.seed(2)
#First Create Training Data
spiral_train = sim_data_fn(N = 150, K = 2)
X_train = spiral_train[,c("x","y")] ## leave out 'label'

#Then Create Testing Data
spiral_test = sim_data_fn(N = 150, K = 2)
X_test = spiral_test[,c("x","y")] 

```

Now we run our training loop. As usual, the X features need to be in matrix form. **In addition, we need the true labels (y_true) to be a vector of 0s and 1s for our loss function to work correctly.**   
In practice, we would need to encode our classes as 0 or 1, which wouldn't be difficult.  
```{r training-loop}
## Testing binary classification ----
X_train = as.matrix(X_train)
y_true_train = spiral_train$label - 1

optimizer_REMINDME("adam") ##see parameter settings needed for adam optimizer
l1_params = init_params(n_inputs = ncol(X_train), 
                        n_neurons = 128,
                        momentum = TRUE, cache = TRUE)
l2_params = init_params(n_inputs = 128,
                        n_neurons = 64,
                        momentum = TRUE, cache = TRUE)
l3_params = init_params(n_inputs = 64,
                            n_neurons = 1,
                            momentum = TRUE, cache = TRUE)


tot_epochs = 1000L
for (epoch in 1:tot_epochs){

#forward
layer1 = layer_dense$forward(inputs = X_train,
                             parameters = l1_params,
                             weight_L2 = 5e-4, bias_L2 = 5e-4)
  layer1_ReLU = activation_ReLU$forward(layer1)
    layer1_dropout = layer_dropout$forward(layer1_ReLU, dropout_rate = 0.1)

layer2 = layer_dense$forward(inputs = layer1_dropout$output,
                             parameters = l2_params,
                             weight_L2 = 5e-4, bias_L2 = 5e-4)
  layer2_ReLU = activation_ReLU$forward(layer2)
    layer2_dropout = layer_dropout$forward(layer2_ReLU, dropout_rate = 0.01)

##CHANGES:
layer3 = layer_dense$forward(inputs = layer2_dropout$output,
                             parameters = l3_params)
  ##OUTPUT LAYER:
  layer3_Sigmoid = activation_Sigmoid$forward(layer3)
  ##LOSS:  
  loss = loss_BinaryCrossentropy$forward(y_pred = layer3_Sigmoid$output,
                                         y_true = y_true_train)
  
  
    reg_loss = regularization_loss(layer1) + regularization_loss(layer2) +
                          regularization_loss(layer3) #=0
    
    metrics = metric_calc(output_layer = layer3_Sigmoid,
                          loss_layer = loss,
                          y_true = y_true_train,
                          reg_loss = reg_loss,
                          loss_fn = "binary_crossentropy")
#backward
##CHANGES:
    ##Backprop the Binary Crossentropy fn
    loss_BCb = loss_BinaryCrossentropy$backward(dvalues =
                                                    layer3_Sigmoid$output,
                                                y_true = y_true_train)
    ##Backprop the Sigmoid function
  l3_Sig_b = activation_Sigmoid$backward(d_layer = loss_BCb,
                                          layer = layer3_Sigmoid) 
l3_b = layer_dense$backward(d_layer = l3_Sig_b, layer3)
    l2_drop_b = layer_dropout$backward(d_layer = l3_b, layer2_dropout)  
  l2_relu_b = activation_ReLU$backward(d_layer = l2_drop_b, layer2_ReLU)
l2_b = layer_dense$backward(d_layer = l2_relu_b, layer2)
    l1_drop_b = layer_dropout$backward(l2_b, layer1_dropout)
  l1_relu_b = activation_ReLU$backward(l1_drop_b, layer1_ReLU)
l1_b = layer_dense$backward(l1_relu_b, layer1)

#optimize params
l1_params = optimizer_Adam$update_params(layer_forward = layer1,
                                         layer_backward = l1_b,
                                         learning_rate = 0.001, 
                                         lr_decay = 5e-7, iteration = epoch,
                                         beta_1 = 0.5, beta_2 = 0.5)
l2_params = optimizer_Adam$update_params(layer_forward = layer2,
                                         layer_backward = l2_b,
                                         learning_rate = 0.001,
                                         lr_decay = 5e-7, iteration = epoch,
                                         beta_1 = 0.5, beta_2 = 0.5)
l3_params = optimizer_Adam$update_params(layer_forward = layer3,
                                         layer_backward = l3_b,
                                         learning_rate = 0.02,
                                         lr_decay = 5e-7, iteration = epoch,
                                         beta_1 = 0.5, beta_2 = 0.5)


#status report
  if (epoch == 1){print(c("epoch", "loss", "data_loss", "acc", "lr"))}
  if (epoch %in% seq(0,tot_epochs,by=100)){
   report =   c( epoch, 
                 metrics$total_loss,
                 metrics$data_loss,
                 metrics$accuracy,
                 l1_params$lr) 
   report = sapply(report, round, 7)
   print(report)} 


#Save Final Metrics:
  if (epoch==tot_epochs) return(metrics)

} #end loop

```


# Regression
Let's also implement some new features so that our neural net can perform regression tasks in addition to classification.  
We will need:
- a new activation function for the output layer (a "linear" activation fn)  
- a new loss function (we will implement both MSE and MAE)  
- and we'll have to implement backpropagation


## Linear Activation Function
This is quite simple - since we want to predict an actual value, as opposed to a class or label, we are not going to modify the inputs at all. For the backwards step, the derivative is also quite simple. Since the linear function is essentially y = x, the derivative is 1. Due to the chain rule, this will be 1 * dvalues, where dvalues are the derivative values being passed back from next layer.  

```{r linear-activation}

activation_Linear = list(
  forward = function(input_layer){
    output = input_layer$output
    list("output" = output)
  },
  
  backward = function(d_layer){
    dinputs = d_layer$dinputs
    list("dinputs" = dinputs)
    #derivative = 1, so 1 * dvalues = dvalues. dvalues correspond to the 
    #previously backpropagated layer's dinputs
    }
)

```

## L2 Loss: Mean Squared Error (MSE) Loss
A common loss metric used in regression tasks, to calculate MSE we square the differences between the y_true's and y_pred's, and take the mean of the values. **Squaring the difference ideally leads to a harsher penalty for bigger misses.**    
The derivative simplifies to (-2/J) * (y_true-y_pred), where J = the number of outputs. We also divide by the number of samples to normalize the gradients so that they are not dependent on the size of a batch.   

```{r MSE}
loss_MeanSquaredError = list(
  forward = function(y_pred, y_true){
    ##calculate MSE for each sample (row)
    sample_losses = rowMeans( (y_true - y_pred)^2 )
      list("sample_losses" = sample_losses, "y_true" = y_true)
  },
  backward = function(dvalues = "linear activation fn output",
                      loss_layer = "loss layer object from forward pass"){
    
    y_true = loss_layer$y_true
    #number of samples
    samples = nrow(dvalues)
    #number of output neurons
    n_outputs = ncol(dvalues)
    
    #Gradient on values (dvalues = y_pred)
    dinputs = -2 * (y_true - dvalues) / n_outputs 
    
    #Normalize
    dinputs = dinputs / samples
    
    list("dinputs"=dinputs)
  }
)
```


## L1 Loss: Mean Absolute Error (MAE) Loss
MAE is similair to MSE except we take the absolute value of the differences between the y_true's and the y_pred's (instead of squaring them), and then take the mean value.  
**Taking the absolute value of the difference ideally leads to equal penalties for all missess.**  
The derivative simplifies to (1/J) * {1, if y_true - y_pred > 0 | -1, if y_true - y_pred < 0} (this follows from the derivative of an absolute value). Again, we normalize the gradients by dividing by the number of samples.

```{r MAE}
loss_MeanAbsoluteError = list(
  forward = function(y_pred, y_true){
    ##calculate MAE for each sample (row)
    sample_losses = rowMeans( abs(y_true - y_pred) )
      list("sample_losses" = sample_losses, "y_true" = y_true)
},
  backward = function(dvalues = "linear activation fn output",
                      loss_layer = "loss layer object from forward pass"){
    y_true = loss_layer$y_true
    #number of samples
    samples = nrow(dvalues)
    #number of output neurons
    n_outputs = ncol(dvalues)
    
    #calculate gradient (sign returns 1 for values >0, -1 for values <0)
    dinputs = sign(y_true - dvalues) / n_outputs
                            ##dvalues = y_pred
    #normalize gradients
    dinputs = dinputs / samples
    
    list("dinputs"=dinputs)
  }
)
```


## Regression Problem & Training
Let's test the regression capabilities of our code. First, we'll need some new data that is fitting for a regression task. We'll copy a simple and classic regression task, which is to fit a neural net to a sine wave. 

```{r sine-wave}
##Create sine-wave datset
x  = seq(0, 2*pi, length.out = 100) 
sine_data = data.frame(x = x/max(x), ##normalized
                       y = sin(x))

##shuffle
sine_data = sine_data[sample(1:nrow(sine_data)), ]

plot(sine_data)

```


```{r}
## Testing regression ----
X = as.matrix(sine_data$x)

  optimizer_REMINDME("adam") ##see parameter settings needed for adam optimizer
  l1_params = init_params(n_inputs = ncol(X), ##just 1 input 
                          n_neurons = 64,
                          momentum = TRUE, cache = TRUE)
  l2_params = init_params(n_inputs = 64,
                          n_neurons = 64,
                          momentum = TRUE, cache = TRUE)
  l3_params = init_params(n_inputs = 64,
                          n_neurons = 1, ##just one output value per sample
                          momentum = TRUE, cache = TRUE)


tot_epochs = 5000L
for (epoch in 1:tot_epochs){
#forward
layer1 = layer_dense$forward(inputs = X, parameters = l1_params)
  layer1_ReLU = activation_ReLU$forward(layer1)
layer2 = layer_dense$forward(inputs = layer1_ReLU$output, 
                             parameters = l2_params)
  layer2_ReLU = activation_ReLU$forward(layer2)
layer3 = layer_dense$forward(inputs = layer2_ReLU$output,
                             parameters = l3_params)
  ##OUTPUT LAYER: [CHANGES]
  layer3_linear = activation_Linear$forward(layer3)

  ##LOSS:  
  loss_mse = loss_MeanSquaredError$forward(y_pred = layer3_linear$output,
                                           y_true = sine_data$y)
  ##(also report MAE, even though MSE is used for optimization)
  loss_mae = loss_MeanAbsoluteError$forward(y_pred = layer3_linear$output,
                                            y_true = sine_data$y)
    
#backward [CHANGES]
    ##Backprop the MSE loss fn
    loss_b = loss_MeanSquaredError$backward(dvalues = layer3_linear$output,
                                            loss_layer = loss_mse) 
    ##Backprop the Linear activation function
  l3_Lin_b = activation_Linear$backward(d_layer = loss_b) 
    ##Backprop the dense layers & the ReLU fns 
l3_b = layer_dense$backward(d_layer = l3_Lin_b, layer3)
  l2_relu_b = activation_ReLU$backward(d_layer = l3_b, layer2_ReLU)
l2_b = layer_dense$backward(d_layer = l2_relu_b, layer2)
  l1_relu_b = activation_ReLU$backward(l2_b, layer1_ReLU)
l1_b = layer_dense$backward(l1_relu_b, layer1)


  learn_r = 0.005
  lrdecay = 1e-3
  beta1 = 0.2
  beta2 = 0.2
#optimize params
l1_params = optimizer_Adam$update_params(layer_forward = layer1,
                                         layer_backward = l1_b,
                                         learning_rate = learn_r, 
                                         lr_decay = lrdecay, iteration = epoch,
                                         beta_1 = beta1, beta_2 = beta2)
l2_params = optimizer_Adam$update_params(layer_forward = layer2,
                                         layer_backward = l2_b,
                                         learning_rate = learn_r,
                                         lr_decay = lrdecay, iteration = epoch,
                                         beta_1 = beta1, beta_2 = beta2)
l3_params = optimizer_Adam$update_params(layer_forward = layer3,
                                         layer_backward = l3_b,
                                         learning_rate = learn_r,
                                         lr_decay = lrdecay, iteration = epoch,
                                         beta_1 = beta1, beta_2 = beta2)


#status report
  if (epoch == 1){
    print(c("epoch", "MSE loss", "MAE", "lr"))
    report_history = data.frame()
    }
  if (epoch %in% seq(0,tot_epochs,by=100)){
   report =   c( epoch, 
                 mean(loss_mse$sample_losses),
                 mean(loss_mae$sample_losses),
                 l1_params$lr) 
   report = sapply(report, round, 7)
   print(report)
   report_history = rbind(report_history, report)
   } 


#Periodically save the predicted values
  if (epoch == 1) reg_predictions = data.frame(row = 1:nrow(sine_data))
  if (epoch %in% c(10, 100, 1000, 2000)){
    i_pred = data.frame(layer3_linear$output)
    colnames(i_pred) = paste0("epoch_",epoch)
    reg_predictions = cbind(reg_predictions, i_pred)
  }  


} #end loop

```

After I tweaked with the model's hyperparameters extensively, it was able to learn the sine wave function. (Key to this process was dropping the beta_1 and beta_2 values quite low... otherwise the model was hardly updating its parameters.)  
Below we can see the progress of the model over the epochs.
```{r}
##Plot model fit history
plot_x = sine_data$x
par(mfrow = c(3,3))
  plot(sine_data$x, reg_predictions$epoch_10, 
       main = "10 Epochs", ylab="y", xlab="X")
  plot(sine_data$x, reg_predictions$epoch_100, 
       main = "100 Epochs", ylab="y", xlab="X")
  plot(sine_data$x, reg_predictions$epoch_1000, 
       main = "1000 Epochs", ylab="y", xlab="X")
  plot(sine_data$x, reg_predictions$epoch_2000, 
       main = "2000 Epochs", ylab="y", xlab="X")
  plot(sine_data$x, layer3_linear$output, 
       main = "Final: 5000 Epochs", ylab="y", xlab="X")

```

```{r}
##Plot model loss
colnames(report_history) = c("epoch", "MSE", "MAE", "lr")

plot(report_history$epoch, report_history$MSE, type = "l", col = "red")
lines(report_history$epoch, report_history$MAE, type = "l", col = "blue")
legend(8000, 0.08, legend = c("MSE","MAE"), 
       col = c("red","blue"), lty = c(1,1,1),
       ncol = 1)
```

## A Note on Weight Initializtion
A point mentioned in [NNFS](https://nnfs.io/) Chapter 17 is that the way weights are randomly initialized can actually have a significant impact on how well a model learns. As a reminder, we randomly initialize the weights as so:
```
weights = matrix(data = (0.1 * rnorm(n = n_inputs*n_neurons)),
                             nrow = n_inputs, ncol = n_neurons)
```
We multiply the weights by a fraction (I chose 0.1) to keep them small, but this number is arbitrarily chosen. In Chapter 17, the authors achieve substantial learning improvements by changing this number.  
In the popular ML library/API Keras, [the initialization of weights and biases can be customized using several built in initializer options](https://keras.io/api/layers/initializers/).  

It might be worth coding some of these up in the future.  


# A Model Object
In this section, we define a function and workflow to convert that (admittedly messy) training loop into a cleaner, automated function.

```{r build-model}
## Build Model----
initialize_model = function(){
    # Create an empty list for network objects
    
    ##if # of layers is predefined, pre-allocate the vector
    ##this is faster than the simple empty list
    # if (!is.null(n_layers)){
    #   layer_list = vector(mode = "list", length = n_layers)
    # } else layer_list = list()
    model = list()
    return(model)
}
add_dense = function(model, n_inputs, n_neurons, dropout_rate = 0,
                       weight_L1=0, weight_L2=0, bias_L1=0, bias_L2=0){
    pos = length(model) + 1 ##the layer's position in sequence of layers
    
    dense_args = list(n_inputs=n_inputs, n_neurons=n_neurons,
                        dropout_rate = dropout_rate,
                        weight_L1=weight_L1, weight_L2=weight_L2,
                        bias_L1=bias_L1, bias_L2=bias_L2,
                        class = "layer_dense", pos = pos)
    
    #layer_n = length(model) + 1 ##determine the layer's sequence number
    #layer_name = deparse(substitute(layer)) ##extract string with layer name
    
    # Add layer objects to the model
    model = c(model, "layer" = list(dense_args))
    # Give the layer a unique name corresponding to position in sequence
    names(model)[[pos]] = paste0("layer", pos)
    #names(model)[['new']] = layer_name ##rename the layer object appropriately
    return(model)
    
}
  # add_dropout = function(model, dropout_rate=0){
  #   pos = length(model) + 1
  #   base_layer = length(model) ##the dense layer that this layer is tied to
  #   dropout_args = list(dropout_rate=dropout_rate,
  #                       class = "layer_dropout", 
  #                       pos = pos, base_layer = base_layer)
  #   
  #   model = c(model, "layer" = list(dropout_args))
  #   names(model)[[pos]] = paste0("layer",pos)
  #   
  #   return(model)
  #},
add_activation =  function(model, activ_fn = c("linear", "relu", "sigmoid",
                                   "softmax_crossentropy")){
    class = activ_fn
    pos = length(model) + 1
    ##We want to tie the activation function back to the dense layer 
    ##its applied upon. This depends on if dropout was used in between
    base_layer = pos-1
    activ_args = list(class=class, pos=pos, base_layer=base_layer)
    model = c(model, "layer" = list(activ_args))
    names(model)[[pos]] = paste0("layer",pos)
    
    return(model)
}
add_loss = function(model, 
                      loss_fn=c("mse", "mae",
                                "binary_crossentropy",
                                "categorical_crossentropy")){
    
    pos = length(model) + 1
    loss_args = list(class=loss_fn, pos=pos)
    model = c(model, "layer" = list(loss_args))
    names(model)[[pos]] = paste0("layer",pos)
    
    return(model)
}
  # add_optimizer = function(model, optimizer = c("adam"),
  #                          learning_rate, lr_decay, epsilon){
  #   pos = length(model) + 1
  #   opt_args = list(class=optimizer, learning_rate=learning_rate,
  #                   lr_decay=lr_decay, epsilon=epsilon)
  #   model = c(model, "layer" = list(opt_args))
  #   names(model)[[pos]] = paste0("layer",pos)
  # }

```


```{r train_model}
## train_model----
train_model = function(model, inputs, y_true, epochs,
                       optimizer = c("adam"), 
                       learning_rate=0, lr_decay=0, epsilon=1e-7,...,
                       metric_list=NULL, print_every=100){
  
  optim_args = list(...)
  if (!is.null(metric_list)){
      ##set metrics list to store metrics
      metrics = data.frame(epoch = 1:epochs) 
  }
  
  ##How many unique "base layers" (dense layers)  
  baselayers = c()
  for (i in seq_along(model)){
    base_i = model[[i]]$base_layer
    baselayers = c(baselayers, base_i)
  }
  baselayers = unique(baselayers) ##return vector of each baselayer position
  
  # Set up the layer parameters
  layer_parameters = list()
  for (i in seq_along(baselayers)){
    ##Initialize empty list
    layer_parameters = c(layer_parameters, "layer"=list(NULL))
    names(layer_parameters)[[i]] = paste0("layer",baselayers[i])
  }
  ##the layer params list will be the length of unique baselayers and will
  ##be overwritten with the updated params at the end of the training loop
  
  ## Derive parameter settings from the selected optimizer
  if (optimizer=="adam"){
    ##requires cache and momentum
    cache_set=TRUE
    momentum_set=TRUE
    ##set default betas if not provided by user in `...`
    if (!exists(x = "beta_1", where = optim_args)){
      warning("Adam utilizes beta_1 and beta_2 hyperparameters which were not provided, and have been set to defaults of 0.9 and 0.999 respectively. Include specific 'beta_1' and 'beta_2' values in the arguments if desired.")
      optim_args$beta_1 = 0.9
      optim_args$beta_2 = 0.999
    }
    
  } else if (optimizer=="sgd"){
    ##momentum is optional, no cache
    if (exists(x = "momentum_rate", where = optim_args)){
      momentum_set=TRUE
      cache_set=FALSE
    } else {
      momentum_set=FALSE
      cache_set=FALSE
      optim_args$momentum_rate = 0
    }
  } else if (optimizer=="adagrad"){
    ##requires cache, no momentum
    cache_set=TRUE
    momentum_set=FALSE
  } else if (optimizer=="rmsprop"){
    ##requires cache, no momentum
    cache_set=TRUE
    momentum_set=FALSE
    ##requires rho hyperparameter
    if (!exists(x = "rho", where = optim_args)){
      warning("RMSProp utilizes the rho hyperparameter which was not provided, and has been set to a default of 0.9. Include a specific 'rho' value in the arguments if desired.")
      optim_args$rho = 0.9
    }
    
  } else {
    warning("Please use one of the following strings to select an optimizer: 'sgd', 'adagrad', 'rmsprop', 'adam'. Otherwise, the network will not perform as intended.")
  }
  
  # Initialize random params for every layer
  for (b in baselayers){
    current_layer = paste0("layer", b)
    n_inputs = model[[current_layer]]$n_inputs
    
    random_params = init_params(n_inputs = model[[current_layer]]$n_inputs,
                                n_neurons = model[[current_layer]]$n_neurons,
                                momentum = momentum_set, 
                                cache = cache_set)

    layer_parameters[[current_layer]] = random_params
    #names(layer_parameters)[[length(layer_parameters)]] = paste0("layer",b)
  }
  
for (epoch in 1:epochs){
  
  layers = list()
  # Forward Pass----
  ##First layer
  dense1 = layer_dense$forward(inputs = inputs,
                              parameters = layer_parameters[[1]], ##the randoms
                              weight_L1 = model[[1]]$weight_L1,
                              weight_L2 = model[[1]]$weight_L2,
                              bias_L1 = model[[1]]$bias_L1,
                              bias_L2 = model[[1]]$bias_L2
                              )
  dropout1 = layer_dropout$forward(input_layer = dense1,
                                  dropout_rate = model[[1]]$dropout_rate)
  
  ###determine activation function  
  activ_fn1 = model[[2]]$class ##second layer is the first activation layer
  
  if (activ_fn1=="linear"){
    activ1 = activation_Linear$forward(input_layer = dropout1)
  } else if (activ_fn1=="relu"){
    activ1 = activation_ReLU$forward(input_layer = dropout1)
  } else if (activ_fn1=="sigmoid"){
    activ1 = activation_Sigmoid$forward(input_layer = dropout1)
  } else if (activ_fn1=="softmax"){
    warning("Pure Softmax is broken Hans")
    activ1 = activation_Softmax$forward(inputs = dropout1$output)
  } else if (activ_fn1=="softmax_crossentropy"){
    activ1 = activation_loss_SoftmaxCrossEntropy$forward(
              input_layer = dropout1,
              y_true = y_true)
  } else message("Please enter one of the following strings in $add_activation to select an activation function: 'linear', 'relu', 'sigmoid', 'softmax_crossentropy'")
  
  ##If softmax_crossentropy, output is called slightly differently
  # if (activ_fn1=="softmax_crossentropy"){
  #   output1 = activ1$output
  # } else output1 = activ1
  output1 = activ1
  
  layer1 = list(dense=dense1,dropout=dropout1, activ=activ1, output=output1)
  layers = c(layers, "layer1" = list(layer1))

if (length(baselayers) > 1){
  ## Next Layers
  for (b in baselayers[-1]){##for all other base layers (- first element/layer)
    
    ##index of prior baselayer (the baselayer that comes next in seq, hence +1)
    prior_baselayer = baselayers[which(baselayers==b)-1] 
    ##which will serve as the input layer
    input_layer = paste0("layer",prior_baselayer)
    current_layer = paste0("layer",b)

    #prior_baselayer = baselayers[b] ##index of prior baselayer (includ. 1st)
    #input_layer = paste0("layer",prior_baselayer)
    
    dense_b = layer_dense$forward(inputs = layers[[input_layer]]$output$output,
                                parameters = layer_parameters[[current_layer]],
                                weight_L1 = model[[current_layer]]$weight_L1,
                                weight_L2 = model[[current_layer]]$weight_L2,
                                bias_L1 = model[[current_layer]]$bias_L1,
                                bias_L2 = model[[current_layer]]$bias_L2
                                )
    dropout_b = layer_dropout$forward(input_layer = dense_b,
                            dropout_rate = model[[current_layer]]$dropout_rate)
    
    activ_fn = model[[b+1]]$class
    ##current layer+1 since activation follows dense.
    
    if (activ_fn=="linear"){
      activ_b = activation_Linear$forward(input_layer = dropout_b)
    } else if (activ_fn=="relu"){
      activ_b = activation_ReLU$forward(input_layer = dropout_b)
    } else if (activ_fn=="sigmoid"){
      activ_b = activation_Sigmoid$forward(input_layer = dropout_b)
    } else if (activ_fn=="softmax"){
      warning("Pure Softmax is broken Hans")
      activ_b = activation_Softmax$forward(inputs = dropout_b$output)
    } else if (activ_fn=="softmax_crossentropy"){
      activ_b = activation_loss_SoftmaxCrossEntropy$forward(
                input_layer = dropout_b,
                y_true = y_true)
    } else message("Please enter one of the following strings in $add_activation to select an activation function: 'linear', 'relu', 'sigmoid', 'softmax_crossentropy'")
  
    ##If softmax_crossentropy, output is called slightly differently
    # if (activ_fn=="softmax_crossentropy"){
    #   output_b = activ_b$output
    # } else output_b = activ_b
    output_b = activ_b
    
    layer = list(dense=dense_b,dropout=dropout_b,activ=activ_b,output=output_b)
    layers = c(layers, "layer" = list(layer))
    names(layers)[[length(layers)]] = paste0("layer",b)
  }#end loop
}  
  
  # Calculate Loss
  ##Determine the selected loss function
  loss_fn = model[[length(model)]]$class ##last layer should be a loss layer
  
  if (loss_fn=="mse"){
    loss_layer = loss_MeanSquaredError$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = y_true
      )
  } else if (loss_fn=="mae"){
    loss_layer = loss_MeanAbsoluteError$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = y_true
    )
  } else if (loss_fn=="binary_crossentropy"){
    loss_layer = loss_BinaryCrossentropy$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = y_true
    )
  } else if (loss_fn=="categorical_crossentropy"){
    loss_layer = layers[[length(layers)]]$output
  }
  
  # Calculate Regularized Loss
  ##If regularization is not used, this code will still run but reg_loss
  ##will equal zero.
  layers_reg_loss = c()
  for (b in baselayers){
    current_layer = paste0("layer",b)
    reg_loss_b = regularization_loss(layers[[current_layer]]$dense)
    layers_reg_loss = c(layers_reg_loss, reg_loss_b)
  }
  reg_loss = sum(layers_reg_loss)
  
  
  # Calculate any other metrics
  ##Determine which metrics to calculate, then calculate and save
  for (metric in metric_list){
    
    if (metric=="mse"){
      mse = loss_MeanSquaredError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = y_true
      )
      metric_i = mean(mse$sample_losses)
    } else if (metric=="mae"){
      mae = loss_MeanAbsoluteError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = y_true
      )
      metric_i = mean(mae$sample_losses)
    } else if (metric=="accuracy"){ ##depends on if binary or cateogircal task
      if (loss_fn=="binary_crossentropy"){
        pred = (layers[[length(layers)]]$output$output > 0.5) * 1 
                  ##returns TRUE (1) if true, and FALSE (0) if false
      } else if (loss_fn=="categorical_crossentropy"){
        pred = max.col(layers[[length(layers)]]$output$output,
                                 ties.method = "random")
      }
      accuracy = mean(pred==y_true, na.rm = T)
      metric_i = accuracy
    } else if (metric=="regression_accuracy"){
      reg_pred = layers[[length(layers)]]$output$output
      accuracy_precision = sd(y_true)/max(y_true)
      reg_accuracy = mean(abs(reg_pred - y_true) < accuracy_precision)
      metric_i = reg_accuracy
    }
    
    ##add metric to the metrics dataframe (unless don't want to track them)
    if (!is.null(metric_list)){
      metrics[epoch, paste0(metric)] = metric_i
    }
  }#end metrics loop
  
  # Backward Pass ----
  layers_back = list()
  
  ## The first backprop layer (the last layer in the training sequence)
  last_layer = paste0("layer",baselayers[length(baselayers)])
  ###First backprop the loss function
  ###Determine which loss function was chosen and backprop it
  loss_fn_last = model[[length(model)]]$class
  
  if (loss_fn_last=="mse"){
    loss_backprop1 = loss_MeanSquaredError$backward(
                      dvalues = layers[[length(layers)]]$output$output,
                      loss_layer = loss_layer
    )
  } else if (loss_fn_last=="mae"){
    loss_backprop1 = loss_MeanAbsoluteError$backward(
                      dvalues = layers[[length(layers)]]$output$output,
                      loss_layer = loss_layer
    )
  } else if (loss_fn_last=="binary_crossentropy"){
    loss_backprop1 = loss_BinaryCrossentropy$backward(
                      dvalues = layers[[length(layers)]]$output$output,
                      y_true = y_true
    )
  } else if (loss_fn_last=="categorical_crossentropy"){
    loss_backprop1 = activation_loss_SoftmaxCrossEntropy$backward(
                      dvalues = layers[[length(layers)]]$output$output,
                      y_true = y_true
    )
  } else warning("Please select an available loss function when building the model.")  
  
  ###Then backprop the activation function
  activ_fn_last = model[[length(model)-1]]$class ##last layer before loss layer
  
  if (activ_fn_last=="linear"){
    activ_backprop1 = activation_Linear$backward(d_layer = loss_backprop1)
  } else if (activ_fn_last=="relu"){
    activ_backprop1 = activation_ReLU$backward(d_layer = loss_backprop1,
                                      layer = layers[[last_layer]]$activ)
  } else if (activ_fn_last=="sigmoid"){
    activ_backprop1 = activation_Sigmoid$backward(d_layer = loss_backprop1,
                                      layer = layers[[last_layer]]$activ)
  } else if (activ_fn_last=="softmax"){
    warning("pure softmax is incomplete")
  } else if (activ_fn_last=="softmax_crossentropy"){
    activ_backprop1 = loss_backprop1 ##due to combined softmax/crossent fn
  }
  ###Then backprop the dropout layer, IF dropout was used
  if (model[[baselayers[length(baselayers)]]]$dropout_rate == 0){
    #dropout1 = FALSE 
    d_layer_todense1=activ_backprop1
  } else {
    #dropout1 = TRUE
    dropout_backprop1 = layer_dropout$backward(
                        d_layer = activ_backprop1, 
                        layer_dropout = layers[[length(layers)]]$dropout
      )
    d_layer_todense1=dropout_backprop1
  }

  ###Finally, backprop the dense layer
  ###Note: the layer_dense backward fn auto-adjusts if regularization is used
  dense_backprop1 = layer_dense$backward(d_layer = d_layer_todense1,
                                        layer = layers[[length(layers)]]$dense)
  
  layer_b = list(dense_backprop=dense_backprop1)
  layers_back = c(layers_back, "layer" = list(layer_b))
  names(layers_back)[[length(layers_back)]] = ##this is the last baselayer
                          paste0("layer",baselayers[length(baselayers)])
  
  ## Next Layers
  for (b in rev(baselayers[-length(baselayers)])){
    ##for all layers but the last (rev order since moving back through layers)
    
    ##index of prior baselayer (the baselayer that comes next in seq, hence +1)
    prior_baselayer = baselayers[which(baselayers==b)+1] 
    ##which will serve as the input layer
    input_layer = paste0("layer",prior_baselayer)

    current_layer = paste0("layer", b)
    
    activ_fn = model[[b+1]]$class
    if (activ_fn=="linear"){
      activ_backprop = activation_Linear$backward(
        d_layer = layers_back[[input_layer]]$dense_backprop)
    } else if (activ_fn=="relu"){
      activ_backprop = activation_ReLU$backward(
        d_layer = layers_back[[input_layer]]$dense_backprop,
        layer = layers[[current_layer]]$activ)
    } else if (activ_fn=="sigmoid"){
      activation_Sigmoid$backward(
        d_layer = layers_back[[input_layer]]$dense_backprop,
        layer = layers[[current_layer]]$activ)
    } #else if "softmax" once pure softmax is fixed
    
    if (model[[baselayers[b]]]$dropout_rate == 0){
      #dropout_rt = FALSE 
      d_layer_todense=activ_backprop
    } else {
      #dropout_rt = TRUE
      dropout_backprop = layer_dropout$backward(
                       d_layer = activ_backprop, 
                       layer_dropout = layers[[current_layer]]$dropout
      )
      d_layer_todense=dropout_backprop
    }

    dense_backprop = layer_dense$backward(d_layer = d_layer_todense,
                                      layer = layers[[current_layer]]$dense)
    layer_b = list(dense_backprop=dense_backprop)
    layers_back = c(layers_back, "layer" = list(layer_b))
    names(layers_back)[[length(layers_back)]] = paste0("layer",b)
  }
  
  # Optimize the Parameters ----
  for (b in baselayers){
    current_layer = paste0("layer", b)
    
    if (optimizer=="sgd"){
      optimal_params_b = optimizer_SGD$update_params(
        layer_forward = layers[[current_layer]]$dense,
        layer_backward = layers_back[[current_layer]]$dense_backprop,
        learning_rate = learning_rate,
        lr_decay = lr_decay, iteration = epoch,
        momentum_rate = optim_args$momentum_rate
      )
    } else if (optimizer=="adagrad"){
      optimal_params_b = optimizer_AdaGrad$update_params(
        layer_forward = layers[[current_layer]]$dense,
        layer_backward = layers_back[[current_layer]]$dense_backprop,
        learning_rate = learning_rate,
        lr_decay = lr_decay, iteration = epoch,
        epsilon = epsilon
      )
    } else if (optimizer=="rmsprop"){
      optimal_params_b = optimizer_RMSProp$update_params(
        layer_forward = layers[[current_layer]]$dense,
        layer_backward = layers_back[[current_layer]]$dense_backprop,
        learning_rate = learning_rate,
        lr_decay = lr_decay, iteration = epoch,
        epsilon = epsilon, 
        rho = optim_args$rho
      )
    } else if (optimizer=="adam"){
      optimal_params_b = optimizer_Adam$update_params(
        layer_forward = layers[[current_layer]]$dense,
        layer_backward = layers_back[[current_layer]]$dense_backprop,
        learning_rate = learning_rate,
        lr_decay = lr_decay, iteration = epoch,
        epsilon = epsilon,
        beta_1 = optim_args$beta_1,
        beta_2 = optim_args$beta_2
      )
    }
                      
    #optim_b = list(optimal_params_b)
    layer_parameters[[current_layer]] = optimal_params_b
    #names(layer_parameters)[[length(layer_parameters)]] = paste0("layer",b)
  }
  
  
  ## Calculate final loss----
  ###If regularization is not used, it will equal zero.
  if (loss_fn=="categorical_crossentropy"){
     data_loss = loss_layer$loss
  } else {
     data_loss = mean(loss_layer$sample_losses)
  }
  
  loss = data_loss + reg_loss ##loss = data_loss if regularization not used
  
  ## Status Reports----
  if (epoch == 1){
    print(c("epoch", "loss", metric_list))
    #report_history = data.frame()
    }
  if (epoch %in% seq(0,epochs,by=print_every)){
     report = c(epoch, loss)
     for (metric in metric_list){
       value = metrics[epoch, paste0(metric)]
       report = c(report, value)
     }
   report = sapply(report, round, 7)
   print(report)
   #report_history = rbind(report_history, report)
  } 
  
  ## Send the losses to the metrics dataframe for tracking
  metrics[epoch, "loss"] = loss
  metrics[epoch, "data_loss"] = data_loss
  
  
}##end loop
  
  ##Final Output----
  final_metrics = list()
  final_metrics[["loss"]] = loss
  final_metrics[["data_loss"]] = data_loss
  for (metric in metric_list){
    final_metrics[[paste0(metric)]] = metrics[epochs, paste0(metric)]
  }
  ##Final Predictions
  raw_predictions = layers[[length(layers)]]$output$output
  if (loss_fn=="categorical_crossentropy"){
    predictions = max.col(raw_predictions, ties.method = "random")
  } else if (loss_fn=="binary_crossentropy"){
    predictions = (raw_predictions > 0.5) * 1 
  }
  else {
    if (ncol(raw_predictions) > 1){
      ##if using multiple output neurons for a regression or binary problem...
      ##this is temporary, may want to change
      predictions = rowMeans(raw_predictions)
    } else predictions = raw_predictions
  }
  
  
  ##Save final model parameters
  parameters = list()
  for (b in baselayers){
    params_b = layer_parameters[[paste0("layer",b)]]
    parameters = c(parameters, "layer"=list(params_b))    
    names(parameters)[[length(parameters)]] = paste0("layer",b)
  }
  
  ##Returns
  list(final_metrics=final_metrics, metrics=metrics, 
       predictions=predictions, raw_predictions=raw_predictions,
       parameters=parameters)
}


```

Now we can run the same training loop as above with just a few lines of code.  

```{r}
##Create sine-wave datset
x  = seq(0, 2*pi, length.out = 100) 
sine_data = data.frame(x = x/max(x), ##normalized
                       y = sin(x))
sine_data = sine_data[sample(1:nrow(sine_data)), ] ##shuffle

X = as.matrix(sine_data$x)

# Build model
model = initialize_model()
model = add_dense(model, n_inputs = 1, n_neurons = 128, 
                        dropout_rate = 0)
model = add_activation(model, activ_fn = "relu")
model = add_dense(model, n_inputs = 128, n_neurons = 64,
                        dropout_rate = 0)
model = add_activation(model, activ_fn = "relu")
model = add_dense(model, n_inputs = 64, n_neurons = 1,
                        dropout_rate = 0)
model = add_activation(model, activ_fn = "linear")
model = add_loss(model, loss_fn = "mse")


#options(error=recover); debug(compile)
fit = train_model(model, inputs = X, y_true = sine_data$y, 
            epochs = 1000, optimizer = "adam",
            learning_rate = 0.005, lr_decay = 1e-3,
            epsilon = 1e-7, beta_1 = 0.5, beta_2 = 0.5,
            metric_list = c("mse", "mae", "regression_accuracy"))

```

Using the same hyperparameters as before, we can see that the new workflow yields the exact same results.  
```{r}
plot(X, fit$predictions)
```

We can also plot the fit history over the epochs.  
```{r}
plot(fit$metrics$epoch, fit$metrics$mse, type = "l")
```


Let's try a classification task. 
First, a binary problem where we'll use a sigmoid function for the final activation function and binary crossentropy for a loss function. Note that piping makes building a model even easier. 
```{r}
spiral_binary = sim_data_fn(N = 200, K = 2)
X = spiral_binary[,c("x", "y")] ## leave out 'label'
X = as.matrix(X)
y_true = spiral_binary$label - 1

binary_model = initialize_model() |>
  add_dense(n_inputs = ncol(X), n_neurons = 64, dropout_rate = 0.2) |>
  add_activation(activ_fn = "relu") |>
  add_dense(n_inputs = 64, n_neurons = 1, dropout_rate = 0) |>
  add_activation(activ_fn = "sigmoid") |>
  add_loss(loss_fn = "binary_crossentropy")

fit_binary = train_model(binary_model, inputs = X, y_true = y_true,
                    epochs = 1000, optimizer = "adam", 
                    learning_rate = 0.001, lr_decay = 5e-7,
                    metric_list = c("accuracy"),
                    beta_1 = 0.2, beta_2 = 0.2)

fit_binary$final_metrics
```


Now let's go back to multi-class classification, where we'll use a softmax function for the final activation function and categorical crossentropy for a loss function (we'll use our combined softmax activation + categorical crossentropy function as before). I'll also add some L2 regularization to the parameters.  


```{r}
spiral_data = sim_data_fn(N=100, K=3)
X = as.matrix(spiral_data[,c("x","y")])
y_true = spiral_data$label


categ_model = initialize_model() |>
  add_dense(n_inputs = ncol(X), n_neurons = 128, dropout_rate = 0.01,
            weight_L2 = 5e-4, bias_L2 = 5e-4) |>
  add_activation("relu") |>
  add_dense(n_inputs = 128, n_neurons = 64, dropout_rate = 0.001,
            weight_L2 = 5e-4, bias_L2 = 5e-4) |>
  add_activation("relu") |>
  add_dense(n_inputs = 64, n_neurons = 3) |>
  add_activation("softmax_crossentropy") |>
  add_loss("categorical_crossentropy")


fit_categ = train_model(categ_model, inputs = X, y_true = y_true,
                        epochs = 100, optimizer = "adam", 
                        learning_rate = 0.01, lr_decay = 1e-3, 
                        beta_1 = 0.5, beta_2 = 0.5,
                        metric_list = c("accuracy"))

fit_categ$final_metrics
```


Now let's add a testing function. To make this automated, we'll want to feed the testing function our trained model and our model instructions and let it determine how many layers were used and what the weights and biases are. This is slightly redundant, since we essentially just want our model to perform one forward pass with the new data. 

```{r test_model}
# test_model()----
test_model = function(model, trained_model, 
                      X_test, y_test,
                      metric_list=NULL){
  inputs = X_test
  y_true = y_test
  ##How many baselayers were used
  baselayers = c()
  for (i in seq_along(model)){
    base_i = model[[i]]$base_layer
    baselayers = c(baselayers, base_i)
  }
  baselayers = unique(baselayers) ##return vector of each baselayer position

 # Forward Pass----
  layers = list()
  ##First layer
  dense1 = layer_dense$forward(inputs = inputs,
                parameters = trained_model$parameters[[1]],
                              weight_L1 = model[[1]]$weight_L1,
                              weight_L2 = model[[1]]$weight_L2,
                              bias_L1 = model[[1]]$bias_L1,
                              bias_L2 = model[[1]]$bias_L2
                              )
  ###determine activation function  
  activ_fn1 = model[[2]]$class ##second layer is the first activation layer
  
  if (activ_fn1=="linear"){
    activ1 = activation_Linear$forward(input_layer = dense1)
  } else if (activ_fn1=="relu"){
    activ1 = activation_ReLU$forward(input_layer = dense1)
  } else if (activ_fn1=="sigmoid"){
    activ1 = activation_Sigmoid$forward(input_layer = dense1)
  } else if (activ_fn1=="softmax"){
    warning("Pure Softmax is broken Hans")
    activ1 = activation_Softmax$forward(inputs = dense1$output)
  } else if (activ_fn1=="softmax_crossentropy"){
    activ1 = activation_loss_SoftmaxCrossEntropy$forward(
              input_layer = dense1,
              y_true = y_true)
  }
  output1 = activ1
  
  layer1 = list(dense=dense1, activ=activ1, output=output1)
  layers = c(layers, "layer1" = list(layer1))

if (length(baselayers) > 1){
  # Next Layers
  for (b in baselayers[-1]){##for all other base layers (- first element/layer)
    
    ##index of prior baselayer (the baselayer that comes next in seq, hence +1)
    prior_baselayer = baselayers[which(baselayers==b)-1] 
    ##which will serve as the input layer
    input_layer = paste0("layer",prior_baselayer)
    current_layer = paste0("layer",b)

    dense_b = layer_dense$forward(inputs = layers[[input_layer]]$output$output,
                      parameters = trained_model$parameters[[current_layer]],
                                weight_L1 = model[[current_layer]]$weight_L1,
                                weight_L2 = model[[current_layer]]$weight_L2,
                                bias_L1 = model[[current_layer]]$bias_L1,
                                bias_L2 = model[[current_layer]]$bias_L2
                                )

    activ_fn = model[[b+1]]$class
    ##current layer+1 since activation follows dense.
    
    if (activ_fn=="linear"){
      activ_b = activation_Linear$forward(input_layer = dense_b)
    } else if (activ_fn=="relu"){
      activ_b = activation_ReLU$forward(input_layer = dense_b)
    } else if (activ_fn=="sigmoid"){
      activ_b = activation_Sigmoid$forward(input_layer = dense_b)
    } else if (activ_fn=="softmax"){
      warning("Pure Softmax is broken Hans")
      activ_b = activation_Softmax$forward(inputs = dense_b$output)
    } else if (activ_fn=="softmax_crossentropy"){
      activ_b = activation_loss_SoftmaxCrossEntropy$forward(
                input_layer = dense_b,
                y_true = y_true)
    }
    output_b = activ_b
    
    layer = list(dense=dense_b,activ=activ_b,output=output_b)
    layers = c(layers, "layer" = list(layer))
    names(layers)[[length(layers)]] = paste0("layer",b)
  }#end loop
}  

    # Calculate Loss
  ##Determine the selected loss function
  loss_fn = model[[length(model)]]$class ##last layer should be a loss layer
  
  if (loss_fn=="mse"){
    loss_layer = loss_MeanSquaredError$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = y_true
      )
  } else if (loss_fn=="mae"){
    loss_layer = loss_MeanAbsoluteError$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = y_true
    )
  } else if (loss_fn=="binary_crossentropy"){
    loss_layer = loss_BinaryCrossentropy$forward(
      y_pred = layers[[length(layers)]]$output$output,
      y_true = y_true
    )
  } else if (loss_fn=="categorical_crossentropy"){
    loss_layer = layers[[length(layers)]]$output
  }
  
  # Calculate Regularized Loss
  ##If regularization is not used, this code will still run but reg_loss
  ##will equal zero.
  layers_reg_loss = c()
  for (b in baselayers){
    current_layer = paste0("layer",b)
    reg_loss_b = regularization_loss(layers[[current_layer]]$dense)
    layers_reg_loss = c(layers_reg_loss, reg_loss_b)
  }
  reg_loss = sum(layers_reg_loss)
  
  
  # Calculate any other metrics
  ##Determine which metrics to calculate, then calculate and save
  for (metric in metric_list){
    
    if (metric=="mse"){
      mse = loss_MeanSquaredError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = y_true
      )
      metric_i = mean(mse$sample_losses)
    } else if (metric=="mae"){
      mae = loss_MeanAbsoluteError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = y_true
      )
      metric_i = mean(mae$sample_losses)
    } else if (metric=="accuracy"){ ##depends on if binary or cateogircal task
      if (loss_fn=="binary_crossentropy"){
        pred = (layers[[length(layers)]]$output$output > 0.5) * 1 
                  ##returns TRUE (1) if true, and FALSE (0) if false
      } else if (loss_fn=="categorical_crossentropy"){
        pred = max.col(layers[[length(layers)]]$output$output,
                                 ties.method = "random")
      }
      accuracy = mean(pred==y_true, na.rm = T)
      metric_i = accuracy
    } else if (metric=="regression_accuracy"){
      reg_pred = layers[[length(layers)]]$output$output
      accuracy_precision = sd(y_true)/max(y_true)
      reg_accuracy = mean(abs(reg_pred - y_true) < accuracy_precision)
      metric_i = reg_accuracy
    }
  }
  
  ## Calculate final loss----
  if (loss_fn=="categorical_crossentropy"){
     data_loss = loss_layer$loss
  } else {
     data_loss = mean(loss_layer$sample_losses)
  }
  loss = data_loss + reg_loss ##loss = data_loss if regularization not used
  
  ## Calculate other metrics
  metrics = list()
  for (metric in metric_list){
    if (metric=="mse"){
      mse = loss_MeanSquaredError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = y_true
      )
      metric_i = mean(mse$sample_losses)
    } else if (metric=="mae"){
      mae = loss_MeanAbsoluteError$forward(
        y_pred = layers[[length(layers)]]$output$output,
        y_true = y_true
      )
      metric_i = mean(mae$sample_losses)
    } else if (metric=="accuracy"){ ##depends on if binary or cateogircal task
      if (loss_fn=="binary_crossentropy"){
        pred = (layers[[length(layers)]]$output$output > 0.5) * 1 
                  ##returns TRUE (1) if true, and FALSE (0) if false
      } else if (loss_fn=="categorical_crossentropy"){
        pred = max.col(layers[[length(layers)]]$output$output,
                                 ties.method = "random")
      }
      accuracy = mean(pred==y_true, na.rm = T)
      metric_i = accuracy
    } else if (metric=="regression_accuracy"){
      reg_pred = layers[[length(layers)]]$output$output
      accuracy_precision = sd(y_true)/max(y_true)
      reg_accuracy = mean(abs(reg_pred - y_true) < accuracy_precision)
      metric_i = reg_accuracy
    }
    
    ##add metric to the metrics list (unless don't want to track them)
    metrics = c(metrics, "m" = list(metric_i))
    names(metrics)[[length(metrics)]] = paste0(metric)
  
  }#end metrics loop
  
  ## Final Predictions
  raw_predictions = layers[[length(layers)]]$output$output
  if (loss_fn=="categorical_crossentropy"){
    predictions = max.col(raw_predictions, ties.method = "random")
  } else if (loss_fn=="binary_crossentropy"){
    predictions = (raw_predictions > 0.5) * 1 
  }
  else {
    if (ncol(raw_predictions) > 1){
      ##if using multiple output neurons for a regression or binary problem...
      ##this is temporary, may want to change
      predictions = rowMeans(raw_predictions)
    } else predictions = raw_predictions
  }
 
  
  ##Output:
  list(predictions=predictions, raw_predictions = raw_predictions,
       loss=loss, data_loss=data_loss,
       metrics=metrics)
  
}

```


Let's test our last categprocal model:

```{r}
# First make some testing data
test_data = sim_data_fn(200, K=3) ##600 samples, bigger than the training data!
X_test = as.matrix(test_data[,c("x","y")])
y_test = test_data$label

# Now run the test_model function
#options(error=recover); debug(test_model)
final_categ = test_model(model = categ_model,
                         trained_model = fit_categ,
                         X_test = X_test, y_test = y_test,
                         metric_list = "accuracy")


final_categ$metrics

#True test data
plot(X_test, col = y_test)

#Predicted test data
plot(X_test, col = final_categ$predictions)
```





TODO: 
- add validation_data argument to the train_model fn (see nnfs pg. 496 for inspiration)  
- add other classification metrics (specificity, sensitivity, precision, F1...)
- cross validation? or separate "evaluate_model" fn?



